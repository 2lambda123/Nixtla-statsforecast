{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05332e29-553e-48d8-b15d-b7874495b672",
   "metadata": {},
   "source": [
    "# Volatility forecasting \n",
    "\n",
    "> In this example, we'll forecast the volatility of the S&P 500 using a GARCH model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea60300-7d0f-472f-af16-a2522f0ba22c",
   "metadata": {},
   "source": [
    "::: {.callout-warning collapse=\"true\"}\n",
    "\n",
    "## Prerequesites\n",
    "\n",
    "This tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the [Quick Start](https://nixtla.github.io/statsforecast/examples/getting_started_short.html) \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7bee12-6d75-4cfe-be02-90d3f6acc027",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9f55b-f95c-4b7e-9e8b-ad3d7ffb5558",
   "metadata": {},
   "source": [
    "The Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model is used for time series that exhibit non-constant volatility over time. Here volatility refers to the conditional standard deviation. The GARCH(p,q) model is given by \n",
    "\n",
    "\\begin{equation}\n",
    "y_t = v_t \\sigma_t \n",
    "\\end{equation}\n",
    "\n",
    "where $v_t$ is independent and identically distributed with zero mean and unit variance, and $\\sigma_t$ evolves according to \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_t^2 = w + \\sum_{i=1}^p \\alpha_i y^2_{t-i} + \\sum_{j=1}^q \\beta_j \\sigma_{t-j}^2\n",
    "\\end{equation}\n",
    "\n",
    "The coefficients in the equation above must satisfy the following conditions: \n",
    "1. $w>0$, $\\alpha_i \\geq 0$ for all $i$, and $\\beta_j \\geq 0$ for all $j$ \n",
    "2. $\\sum_{k=1}^{max(p,q)} \\alpha_k + \\beta_k < 1$. Here it is assumed that $\\alpha_i=0$ for $i>p$ and $\\beta_j=0$ for $j>q$. \n",
    "\n",
    "A particular case of the GARCH model is the ARCH model, in which $q=0$. Both models are commonly used in finance to model the volatility of stock prices, exchange rates, interest rates, and other financial instruments. They're also used in risk management to estimate the probability of large variations in the price of financial assets. \n",
    "\n",
    "By the end of this tutorial, you'll have a good understanding of how to implement a GARCH or an ARCH model in [StatsForecast](https://nixtla.github.io/statsforecast/) and how they can be used to analyze and predict financial time series data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c69cb3-ffbe-44e1-9d4f-8527f690ccfd",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "\n",
    "1. Install libraries \n",
    "2. Load and explore the data\n",
    "3. Train models \n",
    "4. Perform time series cross-validation \n",
    "5. Evaluate results \n",
    "6. Forecast volatility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25559855-e8f9-4b91-8797-f0a1a94b60eb",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "You can use Colab to run this Notebook interactively <a href=\"https://colab.research.google.com/github/Nixtla/statsforecast/blob/main/nbs/examples/GARCH_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041759eb-847f-49a6-93fb-3b22a86a2c25",
   "metadata": {},
   "source": [
    "## Install libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a96763-f02b-401a-8bbd-3a8c2f9d41f0",
   "metadata": {},
   "source": [
    "We assume that you have StatsForecast already installed. If not, check this guide for instructions on [how to install StatsForecast](https://nixtla.github.io/statsforecast/examples/installation.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e255a-f2bc-4fd9-aae9-690b7c803001",
   "metadata": {},
   "source": [
    "Install the necessary packages using `pip install statsforecast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246bc11-3e95-49c5-8d42-596939cc4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install statsforecast -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086500a-89ab-49fc-a168-f0fa77ad6caf",
   "metadata": {},
   "source": [
    "## Load and explore the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bddff1-d31b-4782-b73b-ca8c43609b10",
   "metadata": {},
   "source": [
    "In this tutorial, we'll use the last 5 years of prices from the S&P 500 and several publicly traded companies. The data can be downloaded from Yahoo! Finance using [yfinance](https://github.com/ranaroussi/yfinance). To install it, use `pip install yfinance`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9182e5-296b-43df-bfa9-0a9d2e035ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install yfinance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c3169-edb0-40df-a2d6-215ab0bddd3d",
   "metadata": {},
   "source": [
    "We'll also need `pandas` to deal with the dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2895a-6087-473f-b87e-39a137261d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd \n",
    "\n",
    "tickers = ['SPY', 'MSFT', 'AAPL', 'GOOG', 'AMZN', 'TSLA', 'NVDA', 'META', 'NKE', 'NFLX'] \n",
    "df = yf.download(tickers, start = '2018-01-01', end = '2022-12-31', interval='1mo') # use monthly prices\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1741a7-0d1f-4c22-9133-e76085745ac2",
   "metadata": {},
   "source": [
    "The data downloaded includes different prices. We'll use the [adjusted closing price](https://help.yahoo.com/kb/SLN28256.html#:~:text=Adjusted%20close%20is%20the%20closing,Security%20Prices%20(CRSP)%20standards.), which is the closing price after accounting for any corporate actions like stock splits or dividend distributions. It is also the price that is used to examine historical returns.\n",
    "\n",
    "Notice that the dataframe that `yfinance` returns has a [MultiIndex](https://pandas.pydata.org/docs/user_guide/advanced.html), so we need to select both the adjusted price and the tickers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b998b3-49d9-42d5-99ea-bf1cce25981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, (['Adj Close'], tickers)]\n",
    "df.columns = df.columns.droplevel() # drop MultiIndex\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32585676-2207-4088-9e9d-dbc5ca055c17",
   "metadata": {},
   "source": [
    "The input to StatsForecast is a dataframe in [long format](https://www.theanalysisfactor.com/wide-and-long-data/) with three columns: `unique_id`, `ds` and `y`: \n",
    "\n",
    "- `unique_id`: (string, int or category) A unique identifier for the series.\n",
    "- `ds`: (datestamp or int) A datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS or an integer indexing time.\n",
    "- `y`: (numeric) The measurement we wish to forecast.\n",
    "\n",
    "Hence, we need to reshape the data. We'll do this by creating a new dataframe called `price`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae5f66-20fe-4370-9f98-380ace210056",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = df.melt(id_vars = 'Date')\n",
    "prices = prices.rename(columns={'Date': 'ds', 'variable': 'unique_id', 'value': 'y'})\n",
    "prices = prices[['unique_id', 'ds', 'y']]\n",
    "prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c70be4a-ec1b-477b-adaf-1bcbef01d1ed",
   "metadata": {},
   "source": [
    "We can plot this series using the `plot` method of the StatsForecast class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de66c7-2c8e-4a39-930b-9ef6606ff176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e283821-459c-421c-90d0-fa6a7b591e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsForecast.plot(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf476da-fe85-4c18-8ee9-1561114e0802",
   "metadata": {},
   "source": [
    "With the prices, we can compute the logarithmic returns of the S&P 500 and the publicly traded companies. This is the variable we're interested in since it's likely to work well with the GARCH framework. The logarithmic return is given by \n",
    "\n",
    "$return_t = log \\big( \\frac{price_t}{price_{t-1}} \\big)$\n",
    "\n",
    "We'll compute the returns on the price dataframe and then we'll create a return dataframe with StatsForecast's format. To do this, we'll need `numpy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cde28-d19e-417b-9433-1d4f55514fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "prices['rt'] = prices['y'].div(prices.groupby('unique_id')['y'].shift(1))\n",
    "prices['rt'] = np.log(prices['rt'])\n",
    "\n",
    "returns = prices[['unique_id', 'ds', 'rt']]\n",
    "returns = returns.rename(columns={'rt':'y'})\n",
    "returns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db7e67-922a-4746-9ed4-f5fd08a19373",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "\n",
    "If the order of the data is very small (say $<1e-5$), `scipy.optimize.minimize` might not terminate successfully. In this case, transform the data to a higher order, generate the GARCH or ARCH model, and then transform the data back. \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5814c283-0629-49d3-97ce-ff065342ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsForecast.plot(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf88371-83bd-4c55-a65c-ce70c01f23db",
   "metadata": {},
   "source": [
    "From this plot, we can see that the returns seem suited for the GARCH framework, since large shocks _tend_ to be followed by other large shocks. This doesn't mean that after every large shock we should expect another one; merely that the probability of a large variance is greater than the probability of a small one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ceb930-3b2b-4636-ad14-7c9593f9601d",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d87aa8-906a-47ae-8565-d8a80df4a3d1",
   "metadata": {},
   "source": [
    "We first need to import the [GARCH](https://nixtla.github.io/statsforecast/models.html) and the [ARCH](https://nixtla.github.io/statsforecast/models.html) models from `statsforecast.models`, and then we need to fit them by instantiating a new StatsForecast object. Notice that we'll be using different values of $p$ and $q$. In the next section, we'll determine which ones produce the most accurate model using cross-validation. We'll also import the [Naive](https://nixtla.github.io/statsforecast/models.html#naive) model since we'll use it as a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8a22c-d825-42fe-807e-e8ca4cf5d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast.models import (\n",
    "    GARCH, \n",
    "    ARCH, \n",
    "    Naive\n",
    ")\n",
    "\n",
    "models = [ARCH(1), \n",
    "          ARCH(2), \n",
    "          GARCH(1,1),\n",
    "          GARCH(1,2),\n",
    "          GARCH(2,2),\n",
    "          GARCH(2,1),\n",
    "          Naive()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea18be0-619d-4c70-9b5f-badf141f3fd0",
   "metadata": {},
   "source": [
    "To instantiate a new StatsForecast object, we need the following parameters:\n",
    "\n",
    "- `df`: The dataframe with the training data.\n",
    "- `models`: The list of models defined in the previous step.\n",
    "- `freq`: A string indicating the frequency of the data. Here we'll use **MS**, which correspond to the start of the month. You can see the list of panda's available frequencies [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases). \n",
    "- `n_jobs`: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f8584-6762-47c1-921b-7910756fe1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = StatsForecast(\n",
    "    df = returns, \n",
    "    models = models, \n",
    "    freq = 'MS',\n",
    "    n_jobs = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac6899-a451-4bce-9459-eb63c948a089",
   "metadata": {},
   "source": [
    "## Perform time series cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4054b3-d433-4156-b1ba-3f10d5ee7609",
   "metadata": {},
   "source": [
    "Time series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it. Here we'll use StatsForercast's `cross-validation` method to determine the most accurate model for the S&P 500 and the companies selected. \n",
    "\n",
    "This method takes the following arguments: \n",
    "\n",
    "- `df`: The dataframe with the training data. \n",
    "- `h` (int): represents the h steps into the future that will be forecasted.\n",
    "- `step_size` (int): step size between each window, meaning how often do you want to run the forecasting process.\n",
    "- `n_windows` (int): number of windows used for cross-validation, meaning the number of forecasting processes in the past you want to evaluate.\n",
    "\n",
    "For this particular example, we'll use 4 windows of 3 months, or all the quarters in a year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2600ea-57d6-4e3a-b17d-1efc13db0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation_df = sf.cross_validation(\n",
    "    df = returns,\n",
    "    h = 3,\n",
    "    step_size = 3,\n",
    "    n_windows = 4\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03270241-e962-44aa-b1bc-32b1cbfb5715",
   "metadata": {},
   "source": [
    "The `crossvalidation_df` object ia a dataframe with the following columns: \n",
    "\n",
    "- `unique_id`: index. \n",
    "- `ds`: datestamp or temporal index\n",
    "- `cutoff`: the last datestamp or temporal index for the `n_windows`.\n",
    "- `y`: true value\n",
    "- `\"model\"`: columns with the model’s name and fitted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb90e9-41c3-483c-87ea-fce3266a110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation_df = crossvalidation_df.reset_index()\n",
    "crossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True)\n",
    "crossvalidation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d447c8c-5a5d-4d5d-9087-c1ceef24e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsForecast.plot(returns, crossvalidation_df.drop(['cutoff', 'actual'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480aeccc-3395-45cf-bd96-927f144a0074",
   "metadata": {},
   "source": [
    "A tutorial on cross-validation can be found [here](https://nixtla.github.io/statsforecast/examples/crossvalidation.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e297167-0abe-4c78-9b7a-45ef08da8971",
   "metadata": {},
   "source": [
    "## Evaluate results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47674e91-0532-4b9a-b934-f5304cc1c73b",
   "metadata": {},
   "source": [
    "To compute the accuracy of the forecasts, we’ll use the mean average error (mae), which is the sum of the absolute errors divided by the number of forecasts. There's an implementation of MAE on [datasetsforecast](https://github.com/Nixtla/datasetsforecast/tree/main/), so we'll install it and then import the mae function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03029a-d17c-4357-b950-a402f5362bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install datasetsforecast -U "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab169de-3451-47f9-8b55-bce9d577a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetsforecast.losses import mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32138f4-5f2b-40d0-8fec-a432494731e1",
   "metadata": {},
   "source": [
    "The MAE needs to be computed for every window and then it needs to be averaged across all of them. To do this, we'll create the following function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07769cb-518b-4a43-b4c3-1503b8204ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cv_mae(crossvalidation_df):\n",
    "    \"\"\"Compute MAE for all models generated\"\"\"\n",
    "    res = {}\n",
    "    for mod in models: \n",
    "        res[mod] = mae(crossvalidation_df['actual'], crossvalidation_df[str(mod)])\n",
    "    return pd.Series(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe9575-dc7d-47c8-bacd-f6ec6c254f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_cv = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(compute_cv_mae)\n",
    "\n",
    "mae = mae_cv.groupby('unique_id').mean()\n",
    "mae.style.highlight_min(color = 'lightblue', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d21b2-a71e-4836-8ed5-b8091ca24f04",
   "metadata": {},
   "source": [
    "Hence, the most accurate model to describe the logarithmic returns of Apple's stock is a GARCH(2,1), and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fb49f-cccc-4e9c-9acd-2a63bb162b56",
   "metadata": {},
   "source": [
    "## Forecast volatility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75adaec0-6660-4d9a-8772-aebb1fba86d8",
   "metadata": {},
   "source": [
    "We can now generate a forecast for the next quarter. To do this, we'll use the `forecast` method, which requieres the following arguments:\n",
    "\n",
    "- `h`: (int) The forecasting horizon.\n",
    "- `level`: (list[float]) The confidence levels of the prediction intervals\n",
    "- `fitted` : (bool = False) Returns insample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59297f73-407c-457c-9774-3a94928536c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [80, 95] # confidence levels for the prediction intervals \n",
    "\n",
    "forecasts = sf.forecast(h=3, level=levels)\n",
    "forecasts = forecasts.reset_index()\n",
    "forecasts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3961240-086d-4f26-a44a-171654883e7e",
   "metadata": {},
   "source": [
    "With the results of the previous section, we can choose the best model for the S&P 500 and the companies selected. Some of the plots are shown below. Notice that we're using somo additional arguments in the `plot` method: \n",
    "\n",
    "- `level`: (list[int]) The confidence levels for the prediction intervals (this was already defined). \n",
    "- `unique_ids`: (list[str, int or category]) The ids to plot. \n",
    "- `models`: (list(str)). The model to plot. In this case, is the model selected by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43d5f4-258d-49e9-9e97-cdb11775d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['AAPL'], models = ['GARCH(2,1)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f7998-c2fc-4f91-816e-8887f39df7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['MSFT'], models = ['ARCH(2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c26132-5b46-461d-bdb1-913e0fe47fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['NFLX'], models = ['ARCH(1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ccfbb-22e0-4f23-a243-b586fb4aadbb",
   "metadata": {},
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ada7cc-e365-4cc2-a5d8-65db6888f538",
   "metadata": {},
   "source": [
    "- [Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.](http://www.econ.uiuc.edu/~econ508/Papers/engle82.pdf) \n",
    "\n",
    "- [Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of econometrics, 31(3), 307-327.](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=7da8bfa5295375c1141d797e80065a599153c19d)\n",
    "\n",
    "- [Hamilton, J. D. (1994). Time series analysis. Princeton university press.](https://press.princeton.edu/books/hardcover/9780691042893/time-series-analysis)\n",
    "\n",
    "- [Tsay, R. S. (2005). Analysis of financial time series. John wiley & sons.](https://www.wiley.com/en-us/Analysis+of+Financial+Time+Series%2C+3rd+Edition-p-9780470414354)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
