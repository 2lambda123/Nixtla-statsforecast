{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distributed.fugue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# FugueBackend\n",
    "\n",
    "> The computational efficiency of `StatsForecast` can be tracked to its two core components:<br>1. Its `models` written in NumBa that optimizes Python code to reach C speeds.<br>2. Its `core.StatsForecast` class that enables distributed computing.<br><br>Here we use [Fugue](https://github.com/fugue-project/fugue) which is a unified interface for `Dask` and `Spark`.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06732b96-bd80-4a4d-b9a2-4f95c7a82331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc16100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkho/Work/statsforecast/statsforecast/core.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import inspect\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fugue import transform, DataFrame, FugueWorkflow, ExecutionEngine\n",
    "from fugue.collections.yielded import Yielded\n",
    "from fugue.constants import FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT\n",
    "from statsforecast.core import _StatsForecast, ParallelBackend, make_backend\n",
    "from triad import Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711a14ea-b3e7-466c-bd38-ab94ebbd279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _cotransform(\n",
    "    df1: Any,\n",
    "    df2: Any,\n",
    "    using: Any,\n",
    "    schema: Any = None,\n",
    "    params: Any = None,\n",
    "    partition: Any = None,\n",
    "    engine: Any = None,\n",
    "    engine_conf: Any = None,\n",
    "    force_output_fugue_dataframe: bool = False,\n",
    "    as_local: bool = False,\n",
    ") -> Any:\n",
    "    dag = FugueWorkflow(compile_conf={FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT: 0})\n",
    "    \n",
    "    src = dag.create_data(df1).zip(dag.create_data(df2), partition=partition)\n",
    "    tdf = src.transform(\n",
    "        using=using,\n",
    "        schema=schema,\n",
    "        params=params,\n",
    "        pre_partition=partition,\n",
    "    )\n",
    "    tdf.yield_dataframe_as(\"result\", as_local=as_local)\n",
    "    dag.run(engine, conf=engine_conf)\n",
    "    result = dag.yields[\"result\"].result  # type:ignore\n",
    "    if force_output_fugue_dataframe or isinstance(df1, (DataFrame, Yielded)):\n",
    "        return result\n",
    "    return result.as_pandas() if result.is_local else result.native  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8d5b82-2be9-41f5-8cd0-3903d0761e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FugueBackend(ParallelBackend):\n",
    "    \"\"\"FugueBackend for Distributed Computation.\n",
    "    [Source code](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py).\n",
    "\n",
    "    This class uses [Fugue](https://github.com/fugue-project/fugue) backend capable of distributing \n",
    "    computation on Spark, Dask and Ray without any rewrites.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `engine`: fugue.ExecutionEngine, a selection between Spark, Dask, and Ray.<br>\n",
    "    `conf`: fugue.Config, engine configuration.<br>\n",
    "    `**transform_kwargs`: additional kwargs for Fugue's transform method.<br>\n",
    "\n",
    "    **Notes:**<br>\n",
    "    A short introduction to Fugue, with examples on how to scale pandas code to Spark, Dask or Ray\n",
    "     is available [here](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            engine: Any = None,\n",
    "            conf: Any = None,\n",
    "            **transform_kwargs: Any\n",
    "        ):        \n",
    "        self._engine = engine\n",
    "        self._conf = conf\n",
    "        self._transform_kwargs = dict(transform_kwargs)\n",
    "\n",
    "    def __getstate__(self) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def forecast(\n",
    "            self, \n",
    "            df,\n",
    "            models,\n",
    "            freq,\n",
    "            fallback_model = None,\n",
    "            X_df = None,\n",
    "            **kwargs: Any,\n",
    "        ) -> Any:\n",
    "        \"\"\"Memory Efficient core.StatsForecast predictions with FugueBackend.\n",
    "\n",
    "        This method uses Fugue's transform function, in combination with \n",
    "        `core.StatsForecast`'s forecast to efficiently fit a list of StatsForecast models.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `freq`: str, frequency of the data, [pandas available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "        `models`: List[typing.Any], list of instantiated objects `StatsForecast.models`.<br>\n",
    "        `fallback_model`: Any, Model to be used if a model fails.<br>\n",
    "        `X_df`: pandas.DataFrame, with [unique_id, ds] columns and dfâ€™s future exogenous.\n",
    "        `**kwargs`: Additional `core.StatsForecast` parameters. Example forecast horizon `h`.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with `models` columns for point predictions and probabilistic\n",
    "        predictions for all fitted `models`.<br>\n",
    "        \n",
    "        **References:**<br>\n",
    "        For more information check the \n",
    "        [Fugue's transform](https://fugue-tutorials.readthedocs.io/tutorials/beginner/transform.html)\n",
    "        tutorial.<br>\n",
    "        The [core.StatsForecast's forecast](https://nixtla.github.io/statsforecast/core.html#statsforecast.forecast)\n",
    "        method documentation.<br>\n",
    "        Or the list of available [StatsForecast's models](https://nixtla.github.io/statsforecast/src/core/models.html).\n",
    "        \"\"\"\n",
    "        level = kwargs.get(\"level\", [])\n",
    "        schema = \"*-y+\" + str(self._get_output_schema(models, level))\n",
    "        if X_df is None:\n",
    "            return transform(\n",
    "                df,\n",
    "                self._forecast_series,\n",
    "                params=dict(models=models, freq=freq, \n",
    "                            kwargs=kwargs, fallback_model=fallback_model),\n",
    "                schema=schema,\n",
    "                partition={\"by\": \"unique_id\"},\n",
    "                engine=self._engine,\n",
    "                engine_conf=self._conf,\n",
    "                **self._transform_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            schema = \"unique_id:str,ds:str,\" + str(self._get_output_schema(models, level))\n",
    "            return _cotransform(\n",
    "                df,\n",
    "                X_df,\n",
    "                self._forecast_series_X,\n",
    "                params=dict(models=models, freq=freq, \n",
    "                            kwargs=kwargs, fallback_model=fallback_model),\n",
    "                schema=schema,\n",
    "                partition={\"by\": \"unique_id\"},\n",
    "                engine=self._engine,\n",
    "                engine_conf=self._conf,\n",
    "                **self._transform_kwargs,\n",
    "            )\n",
    "            \n",
    "\n",
    "    def cross_validation(\n",
    "            self, \n",
    "            df,\n",
    "            models,\n",
    "            freq,\n",
    "            fallback_model=None,\n",
    "            **kwargs: Any, \n",
    "        ) -> Any:\n",
    "        \"\"\"Temporal Cross-Validation with core.StatsForecast and FugueBackend.\n",
    "\n",
    "        This method uses Fugue's transform function, in combination with \n",
    "        `core.StatsForecast`'s cross-validation to efficiently fit a list of StatsForecast \n",
    "        models through multiple training windows, in either chained or rolled manner.\n",
    "\n",
    "        `StatsForecast.models`' speed along with Fugue's distributed computation allow to \n",
    "        overcome this evaluation technique high computational costs. Temporal cross-validation \n",
    "        provides better model's generalization measurements by increasing the test's length \n",
    "        and diversity.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "        `models`: List[typing.Any], list of instantiated objects `StatsForecast.models`.<br>\n",
    "        `fallback_model`: Any, Model to be used if a model fails.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with `models` columns for point predictions and probabilistic\n",
    "        predictions for all fitted `models`.<br>\n",
    "        \n",
    "        **References:**<br>\n",
    "        The [core.StatsForecast's cross validation](https://nixtla.github.io/statsforecast/core.html#statsforecast.cross_validation)\n",
    "        method documentation.<br>\n",
    "        [Rob J. Hyndman and George Athanasopoulos (2018). \"Forecasting principles and practice, Temporal Cross-Validation\"](https://otexts.com/fpp3/tscv.html).\n",
    "        \"\"\"\n",
    "        level = kwargs.get(\"level\", [])\n",
    "        schema = \"*-y+\" + str(self._get_output_schema(models, level, mode=\"cv\"))\n",
    "        return transform(\n",
    "            df,\n",
    "            self._cv,\n",
    "            params=dict(models=models, freq=freq, \n",
    "                        kwargs=kwargs, \n",
    "                        fallback_model=fallback_model),\n",
    "            schema=schema,\n",
    "            partition={\"by\": \"unique_id\"},\n",
    "            engine=self._engine,\n",
    "            engine_conf=self._conf,\n",
    "            **self._transform_kwargs,\n",
    "        )\n",
    "\n",
    "    def _forecast_series(self, df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        return model.forecast(**kwargs).reset_index()\n",
    "    \n",
    "    # schema: unique_id:str, ds:str, *\n",
    "    def _forecast_series_X(self, df: pd.DataFrame, X_df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        if len(X_df) != kwargs['h']:\n",
    "            raise Exception(\n",
    "                'Please be sure that your exogenous variables `X_df` '\n",
    "                'have the same length than your forecast horizon `h`'\n",
    "            )\n",
    "        return model.forecast(X_df=X_df, **kwargs).reset_index()\n",
    "\n",
    "    def _cv(self, df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        return model.cross_validation(**kwargs).reset_index()\n",
    "\n",
    "    def _get_output_schema(self, models, level, mode=\"forecast\") -> Schema:\n",
    "        cols: List[Any] = []\n",
    "        for model in models:\n",
    "            has_levels = (\n",
    "                \"level\" in inspect.signature(getattr(model, \"forecast\")).parameters\n",
    "                and len(level) > 0\n",
    "            )\n",
    "            if not has_levels:\n",
    "                cols.append((repr(model), np.float32))\n",
    "            else:\n",
    "                cols.extend([(f\"{repr(model)}-lo-{l}\", np.float32) for l in reversed(level)])\n",
    "                cols.extend([(f\"{repr(model)}-hi-{l}\", np.float32) for l in level])\n",
    "        if mode == \"cv\":\n",
    "            cols = [(\"cutoff\", \"datetime\"), (\"y\", np.float32)] + cols\n",
    "        return Schema(cols)\n",
    "    \n",
    "\n",
    "@make_backend.candidate(lambda obj, *args, **kwargs: isinstance(obj, ExecutionEngine))\n",
    "def _make_fugue_backend(obj:ExecutionEngine, *args, **kwargs) -> ParallelBackend:\n",
    "    return FugueBackend(obj, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5369129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>AutoETS</th>\n",
       "      <th>AutoETS-lo-90</th>\n",
       "      <th>AutoETS-hi-90</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-10</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>2.472186</td>\n",
       "      <td>2.264802</td>\n",
       "      <td>2.029021</td>\n",
       "      <td>2.500583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-11</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>3.369775</td>\n",
       "      <td>3.207784</td>\n",
       "      <td>2.972003</td>\n",
       "      <td>3.443565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-12</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>4.245229</td>\n",
       "      <td>4.248131</td>\n",
       "      <td>4.012350</td>\n",
       "      <td>4.483912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-13</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>5.113708</td>\n",
       "      <td>5.267366</td>\n",
       "      <td>5.031586</td>\n",
       "      <td>5.503148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-14</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>6.127178</td>\n",
       "      <td>6.203136</td>\n",
       "      <td>5.967356</td>\n",
       "      <td>6.438918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds     cutoff         y   AutoETS  AutoETS-lo-90  \\\n",
       "unique_id                                                            \n",
       "0         2000-07-10 2000-07-09  2.472186  2.264802       2.029021   \n",
       "0         2000-07-11 2000-07-09  3.369775  3.207784       2.972003   \n",
       "0         2000-07-12 2000-07-09  4.245229  4.248131       4.012350   \n",
       "0         2000-07-13 2000-07-09  5.113708  5.267366       5.031586   \n",
       "0         2000-07-14 2000-07-09  6.127178  6.203136       5.967356   \n",
       "\n",
       "           AutoETS-hi-90  \n",
       "unique_id                 \n",
       "0               2.500583  \n",
       "0               3.443565  \n",
       "0               4.483912  \n",
       "0               5.503148  \n",
       "0               6.438918  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import ( \n",
    "    AutoARIMA,\n",
    "    AutoETS,\n",
    ")\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "n_series = 4\n",
    "horizon = 7\n",
    "\n",
    "series = generate_series(n_series)\n",
    "\n",
    "sf = StatsForecast(\n",
    "    models=[AutoETS(season_length=7)],\n",
    "    freq='D',\n",
    ")\n",
    "\n",
    "sf.cross_validation(df=series, h=horizon, step_size = 24,\n",
    "    n_windows = 2, level=[90]).head()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Make unique_id a column\n",
    "series = series.reset_index()\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "\n",
    "# Convert to Spark\n",
    "sdf = spark.createDataFrame(series)\n",
    "\n",
    "# Returns a Spark DataFrame\n",
    "sf.cross_validation(df=sdf, h=horizon, step_size = 24,\n",
    "    n_windows = 2, level=[90]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d84def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/06 02:15:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AutoETS' object has no attribute 'cv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m sdf \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(series)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Returns a Spark DataFrame\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_windows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:1885\u001b[0m, in \u001b[0;36mStatsForecast.cross_validation\u001b[0;34m(self, h, df, n_windows, step_size, test_size, input_size, level, fitted, refit, sort_df, prediction_intervals)\u001b[0m\n\u001b[1;32m   1883\u001b[0m engine \u001b[39m=\u001b[39m make_execution_engine(infer_by\u001b[39m=\u001b[39m[df])\n\u001b[1;32m   1884\u001b[0m backend \u001b[39m=\u001b[39m make_backend(engine)\n\u001b[0;32m-> 1885\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcross_validation(\n\u001b[1;32m   1886\u001b[0m     df\u001b[39m=\u001b[39;49mdf,\n\u001b[1;32m   1887\u001b[0m     models\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodels,\n\u001b[1;32m   1888\u001b[0m     freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfreq,\n\u001b[1;32m   1889\u001b[0m     fallback_model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfallback_model,\n\u001b[1;32m   1890\u001b[0m     h\u001b[39m=\u001b[39;49mh,\n\u001b[1;32m   1891\u001b[0m     n_windows\u001b[39m=\u001b[39;49mn_windows,\n\u001b[1;32m   1892\u001b[0m     step_size\u001b[39m=\u001b[39;49mstep_size,\n\u001b[1;32m   1893\u001b[0m     test_size\u001b[39m=\u001b[39;49mtest_size,\n\u001b[1;32m   1894\u001b[0m     input_size\u001b[39m=\u001b[39;49minput_size,\n\u001b[1;32m   1895\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   1896\u001b[0m     refit\u001b[39m=\u001b[39;49mrefit,\n\u001b[1;32m   1897\u001b[0m     fitted\u001b[39m=\u001b[39;49mfitted,\n\u001b[1;32m   1898\u001b[0m     prediction_intervals\u001b[39m=\u001b[39;49mprediction_intervals,\n\u001b[1;32m   1899\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 130\u001b[0m, in \u001b[0;36mFugueBackend.cross_validation\u001b[0;34m(self, df, models, freq, fallback_model, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Temporal Cross-Validation with core.StatsForecast and FugueBackend.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mThis method uses Fugue's transform function, in combination with \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m[Rob J. Hyndman and George Athanasopoulos (2018). \"Forecasting principles and practice, Temporal Cross-Validation\"](https://otexts.com/fpp3/tscv.html).\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m level \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 130\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*-y+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_output_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(\n\u001b[1;32m    132\u001b[0m     df,\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_kwargs,\n\u001b[1;32m    142\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 169\u001b[0m, in \u001b[0;36mFugueBackend._get_output_schema\u001b[0;34m(self, models, level, mode)\u001b[0m\n\u001b[1;32m    166\u001b[0m cols: List[Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m    168\u001b[0m     has_levels \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(level) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_levels:\n\u001b[1;32m    173\u001b[0m         cols\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mrepr\u001b[39m(model), np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoETS' object has no attribute 'cv'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Make unique_id a column\n",
    "series = series.reset_index()\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "\n",
    "# Convert to Spark\n",
    "sdf = spark.createDataFrame(series)\n",
    "\n",
    "# Returns a Spark DataFrame\n",
    "sf.cross_validation(df=sdf, h=horizon, step_size = 24,\n",
    "    n_windows = 2, level=[90]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53913de1-81b9-401c-93a2-83e42047e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(FugueBackend, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037f72d-4ace-44e8-b4d5-b8399d5e294d",
   "metadata": {},
   "source": [
    "## Dask Distributed Predictions\n",
    "\n",
    "Here we provide an example for the distribution of the `StatsForecast` predictions using `Fugue` to execute the code in a Dask cluster.\n",
    "\n",
    "To do it we instantiate the `FugueBackend` class with a `DaskExecutionEngine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df29ce-c1ac-44d9-829e-47096adf2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from fugue_dask import DaskExecutionEngine\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "# Instantiate FugueBackend with DaskExecutionEngine\n",
    "dask_client = Client()\n",
    "engine = DaskExecutionEngine(dask_client=dask_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832937cc-2b7b-4ddc-84d0-e68a650bdc12",
   "metadata": {},
   "source": [
    "We have simply create the class to the usual `StatsForecast` instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08c1f2-3b0a-460f-a1a6-1d25d982104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "sf = StatsForecast(models=[Naive()], freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a44ec-787f-4ef7-8129-71409d2dd32a",
   "metadata": {},
   "source": [
    "### Distributed Forecast\n",
    "\n",
    "For extremely fast distributed predictions we use FugueBackend as backend that operates like the original [StatsForecast.forecast](https://nixtla.github.io/statsforecast/core.html#statsforecast.forecast) method.\n",
    "\n",
    "It receives as input a pandas.DataFrame with columns [`unique_id`,`ds`,`y`] and exogenous, where the `ds` (datestamp) column should be of a format expected by Pandas. The `y` column must be numeric, and represents the measurement we wish to forecast. And the `unique_id` uniquely identifies the series in the panel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2454a-7683-40d1-8828-877dba0345fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Distributed predictions with FugueBackend.\n",
    "sf.forecast(df=df, h=12).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e7b42-8129-472a-99fd-0725ae4cb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "sf = StatsForecast(models=[Naive()], freq='D', fallback_model=Naive())\n",
    "dask_fcst = sf.forecast(df=df, h=12).compute()\n",
    "fcst_stats = sf.forecast(df=df.compute(), h=12)\n",
    "test_eq(dask_fcst.sort_values(by=['unique_id', 'ds']).reset_index(drop=True), \n",
    "        fcst_stats.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964caa9d-d580-44a7-9115-2522e9b64ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "class ReturnX:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, y, X):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, h, X):\n",
    "        mean = X\n",
    "        return X\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ReturnX'\n",
    "    \n",
    "    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n",
    "        return {'mean': X_future.flatten()}\n",
    "    \n",
    "    def new(self):\n",
    "        b = type(self).__new__(type(self))\n",
    "        b.__dict__.update(self.__dict__)\n",
    "        return b\n",
    "    \n",
    "df_w_ex = pd.DataFrame(\n",
    "    {\n",
    "        'ds': np.hstack([np.arange(10), np.arange(10)]),\n",
    "        'y': np.random.rand(20),\n",
    "        'x': np.arange(20, dtype=np.float32),\n",
    "    },\n",
    "    index=pd.Index([0] * 10 + [1] * 10, name='unique_id'),\n",
    ").reset_index()\n",
    "train_mask = df_w_ex['ds'] < 6\n",
    "train_df = dd.from_pandas(df_w_ex[train_mask], npartitions=10)\n",
    "test_df = df_w_ex[~train_mask]\n",
    "xreg = dd.from_pandas(test_df.drop(columns='y').reset_index(drop=True), npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee084bc4-7303-4dd6-99df-5a23fa7cb419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "fcst_x = StatsForecast(models=[ReturnX()], freq='D')\n",
    "res = fcst_x.forecast(df=train_df, \n",
    "                      X_df=xreg, \n",
    "                      h=4).compute()\n",
    "expected_res = xreg.rename(columns={'x': 'ReturnX'}).compute()\n",
    "# we expect strings for unique_id, and ds using exogenous\n",
    "expected_res[['unique_id', 'ds']] = expected_res[['unique_id', 'ds']].astype(str)\n",
    "pd.testing.assert_frame_equal(res.sort_values('unique_id').reset_index(drop=True), \n",
    "                              expected_res, \n",
    "                              check_dtype=False, \n",
    "                              check_index_type=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4f3de-910d-46eb-842b-579935cfbd10",
   "metadata": {},
   "source": [
    "### Distributed Cross-Validation\n",
    "\n",
    "For extremely fast distributed temporcal cross-validation we use `cross_validation` method that operates like the original [StatsForecast.cross_validation](https://nixtla.github.io/statsforecast/core.html#statsforecast) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d0a2f-8f3b-45cd-9f25-d8db281db3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Distributed cross-validation with FugueBackend.\n",
    "sf.cross_validation(df=df, h=12, n_windows=2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95e09b-cb70-4232-8ffa-b26fc8aea557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data.\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "# Distribute predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.forecast(df=df, h=12).compute().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "fcst_stats = sf.forecast(df=df.compute(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())\n",
    "\n",
    "# Distribute cross-validation predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.cross_validation(df=df, h=12).compute().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.compute(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())\n",
    "\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "    \n",
    "#cross validation fallback model\n",
    "fcst = StatsForecast(models=[FailNaive()], freq='D', fallback_model=Naive())\n",
    "fcst_fugue = fcst.cross_validation(df=df, h=12).compute().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.compute(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1084e8-c722-48d5-a038-8d7e530773bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# test ray integration\n",
    "import ray\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data.\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = ray.data.from_pandas(df).repartition(2)\n",
    "\n",
    "# Distribute predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.forecast(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "fcst_stats = sf.forecast(df=df.to_pandas(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())\n",
    "\n",
    "# Distribute cross-validation predictions.\n",
    "fcst = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = fcst.cross_validation(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.to_pandas(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())\n",
    "\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "    \n",
    "#cross validation fallback model\n",
    "sf = StatsForecast(models=[FailNaive()], freq='D', fallback_model=Naive())\n",
    "fcst_fugue = sf.cross_validation(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.to_pandas(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628146a-57b0-4b4b-8377-045b08dd2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "sf = StatsForecast(models=[ReturnX()], freq='D')\n",
    "res = sf.forecast(df=train_df, \n",
    "                  X_df=xreg, \n",
    "                  h=4).compute()\n",
    "expected_res = xreg.compute().rename(columns={'x': 'ReturnX'})\n",
    "# we expect strings for unique_id, and ds using exogenous\n",
    "expected_res[['unique_id', 'ds']] = expected_res[['unique_id', 'ds']].astype(str)\n",
    "pd.testing.assert_frame_equal(res.sort_values('unique_id').reset_index(drop=True), \n",
    "                              expected_res, \n",
    "                              check_dtype=False, \n",
    "                              check_index_type=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('nixtla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "86691f1235dfb63c9749c1b2b5c82263c6ebe5d132bc551491747a17022061dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
