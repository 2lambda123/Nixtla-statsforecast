{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distributed.fugue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# FugueBackend\n",
    "\n",
    "> The computational efficiency of `StatsForecast` can be tracked to its two core components:<br>1. Its `models` written in NumBa that optimizes Python code to reach C speeds.<br>2. Its `core.StatsForecast` class that enables distributed computing.<br><br>Here we use [Fugue](https://github.com/fugue-project/fugue) which is a unified interface for `Dask` and `Spark`.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06732b96-bd80-4a4d-b9a2-4f95c7a82331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkho/Work/statsforecast/statsforecast/core.py:24: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import inspect\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fugue import transform, DataFrame, FugueWorkflow, ExecutionEngine\n",
    "from fugue.collections.yielded import Yielded\n",
    "from fugue.constants import FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT\n",
    "from statsforecast.core import _StatsForecast, ParallelBackend, make_backend\n",
    "from triad import Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a14ea-b3e7-466c-bd38-ab94ebbd279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _cotransform(\n",
    "    df1: Any,\n",
    "    df2: Any,\n",
    "    using: Any,\n",
    "    schema: Any = None,\n",
    "    params: Any = None,\n",
    "    partition: Any = None,\n",
    "    engine: Any = None,\n",
    "    engine_conf: Any = None,\n",
    "    force_output_fugue_dataframe: bool = False,\n",
    "    as_local: bool = False,\n",
    ") -> Any:\n",
    "    dag = FugueWorkflow(compile_conf={FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT: 0})\n",
    "    \n",
    "    src = dag.create_data(df1).zip(dag.create_data(df2), partition=partition)\n",
    "    tdf = src.transform(\n",
    "        using=using,\n",
    "        schema=schema,\n",
    "        params=params,\n",
    "        pre_partition=partition,\n",
    "    )\n",
    "    tdf.yield_dataframe_as(\"result\", as_local=as_local)\n",
    "    dag.run(engine, conf=engine_conf)\n",
    "    result = dag.yields[\"result\"].result  # type:ignore\n",
    "    if force_output_fugue_dataframe or isinstance(df1, (DataFrame, Yielded)):\n",
    "        return result\n",
    "    return result.as_pandas() if result.is_local else result.native  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d5b82-2be9-41f5-8cd0-3903d0761e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FugueBackend(ParallelBackend):\n",
    "    \"\"\"FugueBackend for Distributed Computation.\n",
    "    [Source code](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py).\n",
    "\n",
    "    This class uses [Fugue](https://github.com/fugue-project/fugue) backend capable of distributing \n",
    "    computation on Spark, Dask and Ray without any rewrites.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `engine`: fugue.ExecutionEngine, a selection between Spark, Dask, and Ray.<br>\n",
    "    `conf`: fugue.Config, engine configuration.<br>\n",
    "    `**transform_kwargs`: additional kwargs for Fugue's transform method.<br>\n",
    "\n",
    "    **Notes:**<br>\n",
    "    A short introduction to Fugue, with examples on how to scale pandas code to Spark, Dask or Ray\n",
    "     is available [here](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            engine: Any = None,\n",
    "            conf: Any = None,\n",
    "            **transform_kwargs: Any\n",
    "        ):        \n",
    "        self._engine = engine\n",
    "        self._conf = conf\n",
    "        self._transform_kwargs = dict(transform_kwargs)\n",
    "\n",
    "    def __getstate__(self) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def forecast(\n",
    "            self, \n",
    "            df,\n",
    "            models,\n",
    "            freq,\n",
    "            fallback_model = None,\n",
    "            X_df = None,\n",
    "            **kwargs: Any,\n",
    "        ) -> Any:\n",
    "        \"\"\"Memory Efficient core.StatsForecast predictions with FugueBackend.\n",
    "\n",
    "        This method uses Fugue's transform function, in combination with \n",
    "        `core.StatsForecast`'s forecast to efficiently fit a list of StatsForecast models.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `freq`: str, frequency of the data, [pandas available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "        `models`: List[typing.Any], list of instantiated objects `StatsForecast.models`.<br>\n",
    "        `fallback_model`: Any, Model to be used if a model fails.<br>\n",
    "        `X_df`: pandas.DataFrame, with [unique_id, ds] columns and dfâ€™s future exogenous.\n",
    "        `**kwargs`: Additional `core.StatsForecast` parameters. Example forecast horizon `h`.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with `models` columns for point predictions and probabilistic\n",
    "        predictions for all fitted `models`.<br>\n",
    "        \n",
    "        **References:**<br>\n",
    "        For more information check the \n",
    "        [Fugue's transform](https://fugue-tutorials.readthedocs.io/tutorials/beginner/transform.html)\n",
    "        tutorial.<br>\n",
    "        The [core.StatsForecast's forecast](https://nixtla.github.io/statsforecast/core.html#statsforecast.forecast)\n",
    "        method documentation.<br>\n",
    "        Or the list of available [StatsForecast's models](https://nixtla.github.io/statsforecast/src/core/models.html).\n",
    "        \"\"\"\n",
    "        level = kwargs.get(\"level\", [])\n",
    "        schema = \"*-y+\" + str(self._get_output_schema(models, level))\n",
    "        if X_df is None:\n",
    "            return transform(\n",
    "                df,\n",
    "                self._forecast_series,\n",
    "                params=dict(models=models, freq=freq, \n",
    "                            kwargs=kwargs, fallback_model=fallback_model),\n",
    "                schema=schema,\n",
    "                partition={\"by\": \"unique_id\"},\n",
    "                engine=self._engine,\n",
    "                engine_conf=self._conf,\n",
    "                **self._transform_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            schema = \"unique_id:str,ds:str,\" + str(self._get_output_schema(models, level))\n",
    "            return _cotransform(\n",
    "                df,\n",
    "                X_df,\n",
    "                self._forecast_series_X,\n",
    "                params=dict(models=models, freq=freq, \n",
    "                            kwargs=kwargs, fallback_model=fallback_model),\n",
    "                schema=schema,\n",
    "                partition={\"by\": \"unique_id\"},\n",
    "                engine=self._engine,\n",
    "                engine_conf=self._conf,\n",
    "                **self._transform_kwargs,\n",
    "            )\n",
    "            \n",
    "\n",
    "    def cross_validation(\n",
    "            self, \n",
    "            df,\n",
    "            models,\n",
    "            freq,\n",
    "            fallback_model=None,\n",
    "            **kwargs: Any, \n",
    "        ) -> Any:\n",
    "        \"\"\"Temporal Cross-Validation with core.StatsForecast and FugueBackend.\n",
    "\n",
    "        This method uses Fugue's transform function, in combination with \n",
    "        `core.StatsForecast`'s cross-validation to efficiently fit a list of StatsForecast \n",
    "        models through multiple training windows, in either chained or rolled manner.\n",
    "\n",
    "        `StatsForecast.models`' speed along with Fugue's distributed computation allow to \n",
    "        overcome this evaluation technique high computational costs. Temporal cross-validation \n",
    "        provides better model's generalization measurements by increasing the test's length \n",
    "        and diversity.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "        `models`: List[typing.Any], list of instantiated objects `StatsForecast.models`.<br>\n",
    "        `fallback_model`: Any, Model to be used if a model fails.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with `models` columns for point predictions and probabilistic\n",
    "        predictions for all fitted `models`.<br>\n",
    "        \n",
    "        **References:**<br>\n",
    "        The [core.StatsForecast's cross validation](https://nixtla.github.io/statsforecast/core.html#statsforecast.cross_validation)\n",
    "        method documentation.<br>\n",
    "        [Rob J. Hyndman and George Athanasopoulos (2018). \"Forecasting principles and practice, Temporal Cross-Validation\"](https://otexts.com/fpp3/tscv.html).\n",
    "        \"\"\"\n",
    "        level = kwargs.get(\"level\", [])\n",
    "        schema = \"*-y+\" + str(self._get_output_schema(models, level, mode=\"cv\"))\n",
    "        return transform(\n",
    "            df,\n",
    "            self._cv,\n",
    "            params=dict(models=models, freq=freq, \n",
    "                        kwargs=kwargs, \n",
    "                        fallback_model=fallback_model),\n",
    "            schema=schema,\n",
    "            partition={\"by\": \"unique_id\"},\n",
    "            engine=self._engine,\n",
    "            engine_conf=self._conf,\n",
    "            **self._transform_kwargs,\n",
    "        )\n",
    "\n",
    "    def _forecast_series(self, df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        return model.forecast(**kwargs).reset_index()\n",
    "    \n",
    "    # schema: unique_id:str, ds:str, *\n",
    "    def _forecast_series_X(self, df: pd.DataFrame, X_df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        if len(X_df) != kwargs['h']:\n",
    "            raise Exception(\n",
    "                'Please be sure that your exogenous variables `X_df` '\n",
    "                'have the same length than your forecast horizon `h`'\n",
    "            )\n",
    "        return model.forecast(X_df=X_df, **kwargs).reset_index()\n",
    "\n",
    "    def _cv(self, df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        return model.cross_validation(**kwargs).reset_index()\n",
    "\n",
    "    def _get_output_schema(self, models, level=None, mode=\"forecast\") -> Schema:\n",
    "        cols: List[Any] = []\n",
    "        if level is None:\n",
    "            level = []\n",
    "        for model in models:\n",
    "            has_levels = (\n",
    "                \"level\" in inspect.signature(getattr(model, \"forecast\")).parameters\n",
    "                and len(level) > 0\n",
    "            )\n",
    "            cols.append((repr(model), np.float32))\n",
    "            if has_levels:\n",
    "                cols.extend([(f\"{repr(model)}-lo-{l}\", np.float32) for l in reversed(level)])\n",
    "                cols.extend([(f\"{repr(model)}-hi-{l}\", np.float32) for l in level])\n",
    "        if mode == \"cv\":\n",
    "            cols = [(\"cutoff\", \"datetime\"), (\"y\", np.float32)] + cols\n",
    "        return Schema(cols)\n",
    "    \n",
    "\n",
    "@make_backend.candidate(lambda obj, *args, **kwargs: isinstance(obj, ExecutionEngine))\n",
    "def _make_fugue_backend(obj:ExecutionEngine, *args, **kwargs) -> ParallelBackend:\n",
    "    return FugueBackend(obj, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5369129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>AutoETS</th>\n",
       "      <th>AutoETS-lo-90</th>\n",
       "      <th>AutoETS-hi-90</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-10</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>2.472186</td>\n",
       "      <td>2.264802</td>\n",
       "      <td>2.029021</td>\n",
       "      <td>2.500583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-11</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>3.369775</td>\n",
       "      <td>3.207784</td>\n",
       "      <td>2.972003</td>\n",
       "      <td>3.443565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-12</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>4.245229</td>\n",
       "      <td>4.248131</td>\n",
       "      <td>4.012350</td>\n",
       "      <td>4.483912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-13</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>5.113708</td>\n",
       "      <td>5.267366</td>\n",
       "      <td>5.031586</td>\n",
       "      <td>5.503148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-14</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>6.127178</td>\n",
       "      <td>6.203136</td>\n",
       "      <td>5.967356</td>\n",
       "      <td>6.438918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds     cutoff         y   AutoETS  AutoETS-lo-90  \\\n",
       "unique_id                                                            \n",
       "0         2000-07-10 2000-07-09  2.472186  2.264802       2.029021   \n",
       "0         2000-07-11 2000-07-09  3.369775  3.207784       2.972003   \n",
       "0         2000-07-12 2000-07-09  4.245229  4.248131       4.012350   \n",
       "0         2000-07-13 2000-07-09  5.113708  5.267366       5.031586   \n",
       "0         2000-07-14 2000-07-09  6.127178  6.203136       5.967356   \n",
       "\n",
       "           AutoETS-hi-90  \n",
       "unique_id                 \n",
       "0               2.500583  \n",
       "0               3.443565  \n",
       "0               4.483912  \n",
       "0               5.503148  \n",
       "0               6.438918  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import ( \n",
    "    AutoARIMA,\n",
    "    AutoETS,\n",
    ")\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "n_series = 4\n",
    "horizon = 7\n",
    "\n",
    "series = generate_series(n_series)\n",
    "\n",
    "sf = StatsForecast(\n",
    "    models=[AutoETS(season_length=7)],\n",
    "    freq='D',\n",
    ")\n",
    "\n",
    "sf.cross_validation(df=series, h=horizon, step_size = 24,\n",
    "    n_windows = 2, level=[90]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/08 23:54:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/08 23:54:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+-----------+----------+-------------+-------------+\n",
      "|unique_id|                 ds|             cutoff|          y|   AutoETS|AutoETS-lo-90|AutoETS-hi-90|\n",
      "+---------+-------------------+-------------------+-----------+----------+-------------+-------------+\n",
      "|        1|2000-03-07 00:00:00|2000-03-06 00:00:00|  1.1842923| 1.1227854|   0.88283557|    1.3627354|\n",
      "|        1|2000-03-08 00:00:00|2000-03-06 00:00:00|  2.0684502| 2.3335178|     2.093568|    2.5734677|\n",
      "|        1|2000-03-09 00:00:00|2000-03-06 00:00:00|   3.411059|  3.249278|    3.0093281|    3.4892278|\n",
      "|        1|2000-03-10 00:00:00|2000-03-06 00:00:00|   4.094924| 4.3513813|    4.1114316|    4.5913315|\n",
      "|        1|2000-03-11 00:00:00|2000-03-06 00:00:00|  5.2556596| 5.2070827|     4.967133|     5.447033|\n",
      "|        1|2000-03-12 00:00:00|2000-03-06 00:00:00|  6.1121583|   6.28834|      6.04839|      6.52829|\n",
      "|        1|2000-03-13 00:00:00|2000-03-06 00:00:00| 0.04892224|0.25212684|  0.012176938|   0.49207672|\n",
      "|        1|2000-03-31 00:00:00|2000-03-30 00:00:00|   4.336024| 4.3107433|    4.0474195|    4.5740666|\n",
      "|        1|2000-04-01 00:00:00|2000-03-30 00:00:00|  5.1226835| 5.2358456|    4.9725223|     5.499169|\n",
      "|        1|2000-04-02 00:00:00|2000-03-30 00:00:00|    6.21027| 6.2425833|      5.97926|    6.5059066|\n",
      "|        1|2000-04-03 00:00:00|2000-03-30 00:00:00|  0.2786844|0.22976251|  -0.03356086|    0.4930859|\n",
      "|        1|2000-04-04 00:00:00|2000-03-30 00:00:00|  1.4302756| 1.1543093|    0.8909859|    1.4176327|\n",
      "|        1|2000-04-05 00:00:00|2000-03-30 00:00:00|   2.363522| 2.2840557|    2.0207324|     2.547379|\n",
      "|        1|2000-04-06 00:00:00|2000-03-30 00:00:00|   3.135164| 3.2521222|    2.9887989|    3.5154455|\n",
      "|        3|2000-07-30 00:00:00|2000-07-29 00:00:00|  4.4213886| 4.1870832|     3.950173|     4.423993|\n",
      "|        3|2000-07-31 00:00:00|2000-07-29 00:00:00|   5.186608| 5.2659216|    5.0290112|    5.5028315|\n",
      "|        3|2000-08-01 00:00:00|2000-07-29 00:00:00|   6.111432|  6.255861|    6.0189505|    6.4927707|\n",
      "|        3|2000-08-02 00:00:00|2000-07-29 00:00:00|0.040266003| 0.2792256|  0.042315517|    0.5161357|\n",
      "|        3|2000-08-03 00:00:00|2000-07-29 00:00:00|  1.0426555| 1.2719641|     1.035054|    1.5088742|\n",
      "|        3|2000-08-04 00:00:00|2000-07-29 00:00:00|  2.1106982|  2.245433|     2.008523|    2.4823432|\n",
      "+---------+-------------------+-------------------+-----------+----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Make unique_id a column\n",
    "series = series.reset_index()\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "\n",
    "# Convert to Spark\n",
    "sdf = spark.createDataFrame(series)\n",
    "\n",
    "# Returns a Spark DataFrame\n",
    "sf.cross_validation(df=sdf, h=horizon, step_size = 24,\n",
    "    n_windows = 2, level=[90]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53913de1-81b9-401c-93a2-83e42047e471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py#L48){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FugueBackend\n",
       "\n",
       ">      FugueBackend (engine:Any=None, conf:Any=None, **transform_kwargs:Any)\n",
       "\n",
       "FugueBackend for Distributed Computation.\n",
       "[Source code](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py).\n",
       "\n",
       "This class uses [Fugue](https://github.com/fugue-project/fugue) backend capable of distributing \n",
       "computation on Spark, Dask and Ray without any rewrites.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`engine`: fugue.ExecutionEngine, a selection between Spark, Dask, and Ray.<br>\n",
       "`conf`: fugue.Config, engine configuration.<br>\n",
       "`**transform_kwargs`: additional kwargs for Fugue's transform method.<br>\n",
       "\n",
       "**Notes:**<br>\n",
       "A short introduction to Fugue, with examples on how to scale pandas code to Spark, Dask or Ray\n",
       " is available [here](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html)."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py#L48){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FugueBackend\n",
       "\n",
       ">      FugueBackend (engine:Any=None, conf:Any=None, **transform_kwargs:Any)\n",
       "\n",
       "FugueBackend for Distributed Computation.\n",
       "[Source code](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py).\n",
       "\n",
       "This class uses [Fugue](https://github.com/fugue-project/fugue) backend capable of distributing \n",
       "computation on Spark, Dask and Ray without any rewrites.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`engine`: fugue.ExecutionEngine, a selection between Spark, Dask, and Ray.<br>\n",
       "`conf`: fugue.Config, engine configuration.<br>\n",
       "`**transform_kwargs`: additional kwargs for Fugue's transform method.<br>\n",
       "\n",
       "**Notes:**<br>\n",
       "A short introduction to Fugue, with examples on how to scale pandas code to Spark, Dask or Ray\n",
       " is available [here](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html)."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FugueBackend, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037f72d-4ace-44e8-b4d5-b8399d5e294d",
   "metadata": {},
   "source": [
    "## Dask Distributed Predictions\n",
    "\n",
    "Here we provide an example for the distribution of the `StatsForecast` predictions using `Fugue` to execute the code in a Dask cluster.\n",
    "\n",
    "To do it we instantiate the `FugueBackend` class with a `DaskExecutionEngine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df29ce-c1ac-44d9-829e-47096adf2917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51914 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from fugue_dask import DaskExecutionEngine\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "# Instantiate FugueBackend with DaskExecutionEngine\n",
    "dask_client = Client()\n",
    "engine = DaskExecutionEngine(dask_client=dask_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832937cc-2b7b-4ddc-84d0-e68a650bdc12",
   "metadata": {},
   "source": [
    "We have simply create the class to the usual `StatsForecast` instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08c1f2-3b0a-460f-a1a6-1d25d982104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "sf = StatsForecast(models=[Naive()], freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a44ec-787f-4ef7-8129-71409d2dd32a",
   "metadata": {},
   "source": [
    "### Distributed Forecast\n",
    "\n",
    "For extremely fast distributed predictions we use FugueBackend as backend that operates like the original [StatsForecast.forecast](https://nixtla.github.io/statsforecast/core.html#statsforecast.forecast) method.\n",
    "\n",
    "It receives as input a pandas.DataFrame with columns [`unique_id`,`ds`,`y`] and exogenous, where the `ds` (datestamp) column should be of a format expected by Pandas. The `y` column must be numeric, and represents the measurement we wish to forecast. And the `unique_id` uniquely identifies the series in the panel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2454a-7683-40d1-8828-877dba0345fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>Naive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-30</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-31</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-09-01</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-09-02</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-09-03</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-07</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-08</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-09</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-10</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-11</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds     Naive\n",
       "0          3 2000-08-30  6.182456\n",
       "1          3 2000-08-31  6.182456\n",
       "2          3 2000-09-01  6.182456\n",
       "3          3 2000-09-02  6.182456\n",
       "4          3 2000-09-03  6.182456\n",
       "..       ...        ...       ...\n",
       "7          8 2000-03-07  0.162962\n",
       "8          8 2000-03-08  0.162962\n",
       "9          8 2000-03-09  0.162962\n",
       "10         8 2000-03-10  0.162962\n",
       "11         8 2000-03-11  0.162962\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Distributed predictions with FugueBackend.\n",
    "sf.forecast(df=df, h=12).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e7b42-8129-472a-99fd-0725ae4cb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "sf = StatsForecast(models=[Naive()], freq='D', fallback_model=Naive())\n",
    "dask_fcst = sf.forecast(df=df, h=12).compute()\n",
    "fcst_stats = sf.forecast(df=df.compute(), h=12)\n",
    "test_eq(dask_fcst.sort_values(by=['unique_id', 'ds']).reset_index(drop=True).astype({\"unique_id\": str}), \n",
    "        fcst_stats.reset_index().astype({\"unique_id\": str}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "class ReturnX:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, y, X):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, h, X):\n",
    "        mean = X\n",
    "        return X\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ReturnX'\n",
    "    \n",
    "    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n",
    "        return {'mean': X_future.flatten()}\n",
    "    \n",
    "    def new(self):\n",
    "        b = type(self).__new__(type(self))\n",
    "        b.__dict__.update(self.__dict__)\n",
    "        return b\n",
    "    \n",
    "df_w_ex = pd.DataFrame(\n",
    "    {\n",
    "        'ds': np.hstack([np.arange(10), np.arange(10)]),\n",
    "        'y': np.random.rand(20),\n",
    "        'x': np.arange(20, dtype=np.float32),\n",
    "    },\n",
    "    index=pd.Index([0] * 10 + [1] * 10, name='unique_id'),\n",
    ").reset_index()\n",
    "train_mask = df_w_ex['ds'] < 6\n",
    "train_df = dd.from_pandas(df_w_ex[train_mask], npartitions=10)\n",
    "test_df = df_w_ex[~train_mask]\n",
    "xreg = dd.from_pandas(test_df.drop(columns='y').reset_index(drop=True), npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee084bc4-7303-4dd6-99df-5a23fa7cb419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "fcst_x = StatsForecast(models=[ReturnX()], freq='D')\n",
    "res = fcst_x.forecast(df=train_df, \n",
    "                      X_df=xreg, \n",
    "                      h=4).compute()\n",
    "expected_res = xreg.rename(columns={'x': 'ReturnX'}).compute()\n",
    "# we expect strings for unique_id, and ds using exogenous\n",
    "expected_res[['unique_id', 'ds']] = expected_res[['unique_id', 'ds']].astype(str)\n",
    "pd.testing.assert_frame_equal(res.sort_values('unique_id').reset_index(drop=True), \n",
    "                              expected_res, \n",
    "                              check_dtype=False, \n",
    "                              check_index_type=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4f3de-910d-46eb-842b-579935cfbd10",
   "metadata": {},
   "source": [
    "### Distributed Cross-Validation\n",
    "\n",
    "For extremely fast distributed temporcal cross-validation we use `cross_validation` method that operates like the original [StatsForecast.cross_validation](https://nixtla.github.io/statsforecast/core.html#statsforecast) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d0a2f-8f3b-45cd-9f25-d8db281db3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>Naive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-17</td>\n",
       "      <td>2000-08-16</td>\n",
       "      <td>1.375511</td>\n",
       "      <td>0.390740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-18</td>\n",
       "      <td>2000-08-16</td>\n",
       "      <td>2.463606</td>\n",
       "      <td>0.390740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-19</td>\n",
       "      <td>2000-08-16</td>\n",
       "      <td>3.014476</td>\n",
       "      <td>0.390740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-20</td>\n",
       "      <td>2000-08-16</td>\n",
       "      <td>4.447845</td>\n",
       "      <td>0.390740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-21</td>\n",
       "      <td>2000-08-16</td>\n",
       "      <td>5.196284</td>\n",
       "      <td>0.390740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>3.267941</td>\n",
       "      <td>2.483478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>4.333817</td>\n",
       "      <td>2.483478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-02-26</td>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>5.410231</td>\n",
       "      <td>2.483478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-02-27</td>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>6.115387</td>\n",
       "      <td>2.483478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>0.162962</td>\n",
       "      <td>2.483478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds     cutoff         y     Naive\n",
       "0          3 2000-08-17 2000-08-16  1.375511  0.390740\n",
       "1          3 2000-08-18 2000-08-16  2.463606  0.390740\n",
       "2          3 2000-08-19 2000-08-16  3.014476  0.390740\n",
       "3          3 2000-08-20 2000-08-16  4.447845  0.390740\n",
       "4          3 2000-08-21 2000-08-16  5.196284  0.390740\n",
       "..       ...        ...        ...       ...       ...\n",
       "19         8 2000-02-24 2000-02-16  3.267941  2.483478\n",
       "20         8 2000-02-25 2000-02-16  4.333817  2.483478\n",
       "21         8 2000-02-26 2000-02-16  5.410231  2.483478\n",
       "22         8 2000-02-27 2000-02-16  6.115387  2.483478\n",
       "23         8 2000-02-28 2000-02-16  0.162962  2.483478\n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Distributed cross-validation with FugueBackend.\n",
    "sf.cross_validation(df=df, h=12, n_windows=2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95e09b-cb70-4232-8ffa-b26fc8aea557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data.\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "# Distribute predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.forecast(df=df, h=12).compute().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "fcst_stats = sf.forecast(df=df.compute(), h=12).reset_index().astype({\"unique_id\": str})\n",
    "test_eq(fcst_fugue, fcst_stats)\n",
    "\n",
    "# Distribute cross-validation predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.cross_validation(df=df, h=12).compute().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.compute(), h=12).reset_index().astype({\"unique_id\": str})\n",
    "test_eq(fcst_fugue, fcst_stats)\n",
    "\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "    \n",
    "#cross validation fallback model\n",
    "fcst = StatsForecast(models=[FailNaive()], freq='D', fallback_model=Naive())\n",
    "fcst_fugue = fcst.cross_validation(df=df, h=12).compute().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.compute(), h=12).reset_index().astype({\"unique_id\": str})\n",
    "test_eq(fcst_fugue, fcst_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1084e8-c722-48d5-a038-8d7e530773bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 23:56:08,907\tINFO worker.py:1625 -- Started a local Ray instance.\n",
      "2023-07-08 23:56:12,762\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:12,766\tINFO streaming_executor.py:83 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition]\n",
      "2023-07-08 23:56:12,769\tINFO streaming_executor.py:84 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\n",
      "\u001b[A\n",
      "Running: 0.0/8.0 CPU, 0.0/0.0 GPU, 0.0 MiB/512.0 MiB object_store_memory 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "                                                                                                                          \n",
      "\u001b[A2023-07-08 23:56:13,645\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:13,666\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:13,676\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition]\n",
      "2023-07-08 23:56:15,322\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(add_simple_key)]\n",
      "2023-07-08 23:56:16,652\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:16,654\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Sort] -> TaskPoolMapOperator[MapBatches(group_fn)]\n",
      "MapBatches(group_fn) 0:   6%|â–‹         | 1/16 [00:06<01:30,  6.02s/it]/opt/anaconda3/envs/nixtla/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/opt/anaconda3/envs/nixtla/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "2023-07-08 23:56:43,223\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:43,237\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:43,245\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:43,248\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition]\n",
      "2023-07-08 23:56:43,343\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(add_simple_key)]\n",
      "2023-07-08 23:56:43,427\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:43,431\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Sort] -> TaskPoolMapOperator[MapBatches(group_fn)]\n",
      "2023-07-08 23:56:44,140\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:44,247\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:44,264\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:44,267\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition]\n",
      "2023-07-08 23:56:44,526\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(add_simple_key)]\n",
      "2023-07-08 23:56:44,650\tWARNING dataset.py:4066 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-07-08 23:56:44,656\tINFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Sort] -> TaskPoolMapOperator[MapBatches(group_fn)]\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# test ray integration\n",
    "import ray\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data.\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = ray.data.from_pandas(df).repartition(2)\n",
    "\n",
    "# Distribute predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.forecast(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "fcst_stats = sf.forecast(df=df.to_pandas(), h=12).reset_index().astype({\"unique_id\": str})\n",
    "test_eq(fcst_fugue, fcst_stats)\n",
    "\n",
    "# Distribute cross-validation predictions.\n",
    "fcst = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = fcst.cross_validation(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.to_pandas(), h=12).reset_index().astype({\"unique_id\": str})\n",
    "test_eq(fcst_fugue, fcst_stats)\n",
    "\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "    \n",
    "#cross validation fallback model\n",
    "sf = StatsForecast(models=[FailNaive()], freq='D', fallback_model=Naive())\n",
    "fcst_fugue = sf.cross_validation(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.to_pandas(), h=12).reset_index().astype({\"unique_id\": str})\n",
    "test_eq(fcst_fugue, fcst_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628146a-57b0-4b4b-8377-045b08dd2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "sf = StatsForecast(models=[ReturnX()], freq='D')\n",
    "res = sf.forecast(df=train_df, \n",
    "                  X_df=xreg, \n",
    "                  h=4).compute()\n",
    "expected_res = xreg.compute().rename(columns={'x': 'ReturnX'})\n",
    "# we expect strings for unique_id, and ds using exogenous\n",
    "expected_res[['unique_id', 'ds']] = expected_res[['unique_id', 'ds']].astype(str)\n",
    "pd.testing.assert_frame_equal(res.sort_values('unique_id').reset_index(drop=True), \n",
    "                              expected_res, \n",
    "                              check_dtype=False, \n",
    "                              check_index_type=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
