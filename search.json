[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "",
    "text": "You can install StatsForecast with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\nVist our Installation Guide for further instructions.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "",
    "text": "You can install StatsForecast with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\nVist our Installation Guide for further instructions."
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Quick Start",
    "text": "Quick Start\nMinimal Example\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nsf = StatsForecast(\n    models = [AutoARIMA(season_length = 12)],\n    freq = 'M'\n)\n\nsf.fit(df)\nsf.predict(h=12, level=[95])\nGet Started with this quick guide.\nFollow this end-to-end walkthrough for best practices."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Why?",
    "text": "Why?\nCurrent Python alternatives for statistical models are slow, inaccurate and don‚Äôt scale well. So we created a library that can be used to forecast in production environments or as benchmarks. StatsForecast includes an extensive battery of models that can efficiently fit millions of time series."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Features",
    "text": "Features\n\nFastest and most accurate implementations of AutoARIMA, AutoETS, AutoCES, MSTL and Theta in Python.\nOut-of-the-box compatibility with Spark, Dask, and Ray.\nProbabilistic Forecasting and Confidence Intervals.\nSupport for exogenous Variables and static covariates.\nAnomaly Detection.\nFamiliar sklearn syntax: .fit and .predict."
  },
  {
    "objectID": "index.html#highlights",
    "href": "index.html#highlights",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Highlights",
    "text": "Highlights\n\nInclusion of exogenous variables and prediction intervals for ARIMA.\n20x faster than pmdarima.\n1.5x faster than R.\n500x faster than Prophet.\n4x faster than statsmodels.\nCompiled to high performance machine code through numba.\n1,000,000 series in 30 min with ray.\nReplace FB-Prophet in two lines of code and gain speed and accuracy. Check the experiments here.\nFit 10 benchmark models on 1,000,000 series in under 5 min.\n\nMissing something? Please open an issue or write us in"
  },
  {
    "objectID": "index.html#examples-and-guides",
    "href": "index.html#examples-and-guides",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Examples and Guides",
    "text": "Examples and Guides\nüìö End to End Walkthrough: Model training, evaluation and selection for multiple time series\nüîé Anomaly Detection: detect anomalies for time series using in-sample prediction intervals.\nüë©‚Äçüî¨ Cross Validation: robust model‚Äôs performance evaluation.\n‚ùÑÔ∏è Multiple Seasonalities: how to forecast data with multiple seasonalities using an MSTL.\nüîå Predict Demand Peaks: electricity load forecasting for detecting daily peaks and reducing electric bills.\nüìà Intermittent Demand: forecast series with very few non-zero observations.\nüå°Ô∏è Exogenous Regressors: like weather or prices"
  },
  {
    "objectID": "index.html#models",
    "href": "index.html#models",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Models",
    "text": "Models\n\nAutomatic Forecasting\nAutomatic forecasting tools search for the best parameters and select the best possible model for a group of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoETS\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoCES\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "index.html#arima-family",
    "href": "index.html#arima-family",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "ARIMA Family",
    "text": "ARIMA Family\nThese models exploit the existing autocorrelations in the time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nARIMA\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoRegressive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\nTheta Family\nFit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nOptimizedTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nDynamicTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nDynamicOptimizedTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\nMultiple Seasonalities\nSuited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nMSTL\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\nGARCH and ARCH Models\nSuited for modeling time series that exhibit non-constant volatility over time. The ARCH model is a particular case of GARCH.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nGARCH\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nARCH\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\nBaseline Models\nClassical models for establishing baseline.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nHistoricAverage\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nNaive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nRandomWalkWithDrift\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nSeasonalNaive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nWindowAverage\n‚úÖ\n\n\n\n\n\nSeasonalWindowAverage\n‚úÖ\n\n\n\n\n\n\n\n\nExponential Smoothing\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with clear trend and/or seasonality. Use the SimpleExponential family for data with no clear trend or seasonality.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nSimpleExponentialSmoothing\n‚úÖ\n\n\n\n\n\nSimpleExponentialSmoothingOptimized\n‚úÖ\n\n\n\n\n\nSeasonalExponentialSmoothing\n‚úÖ\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n‚úÖ\n\n\n\n\n\nHolt\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nHoltWinters\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\nSparse or Inttermitent\nSuited for series with very few non-zero observations\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nADIDA\n‚úÖ\n\n\n\n\n\nCrostonClassic\n‚úÖ\n\n\n\n\n\nCrostonOptimized\n‚úÖ\n\n\n\n\n\nCrostonSBA\n‚úÖ\n\n\n\n\n\nIMAPA\n‚úÖ\n\n\n\n\n\nTSB\n‚úÖ"
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "How to contribute",
    "text": "How to contribute\nSee CONTRIBUTING.md."
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Citing",
    "text": "Citing\n@misc{garza2022statsforecast,\n    author={Federico Garza, Max Mergenthaler Canseco, Cristian Chall√∫, Kin G. Olivares},\n    title = {{StatsForecast}: Lightning fast forecasting with statistical and econometric models},\n    year={2022},\n    howpublished={{PyCon} Salt Lake City, Utah, US 2022},\n    url={https://github.com/Nixtla/statsforecast}\n}"
  },
  {
    "objectID": "src/ets.html",
    "href": "src/ets.html",
    "title": "ETS Model",
    "section": "",
    "text": "source\n\n\n\n ets_target_fn (par, p_y, p_nstate, p_errortype, p_trendtype,\n                p_seasontype, p_damped, p_lower, p_upper, p_opt_crit,\n                p_nmse, p_bounds, p_m, p_optAlpha, p_optBeta, p_optGamma,\n                p_optPhi, p_givenAlpha, p_givenBeta, p_givenGamma,\n                p_givenPhi, alpha, beta, gamma, phi)\n\n\nsource\n\n\n\n\n is_constant (x)\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/ets.html#etscalc",
    "href": "src/ets.html#etscalc",
    "title": "ETS Model",
    "section": "",
    "text": "source\n\n\n\n ets_target_fn (par, p_y, p_nstate, p_errortype, p_trendtype,\n                p_seasontype, p_damped, p_lower, p_upper, p_opt_crit,\n                p_nmse, p_bounds, p_m, p_optAlpha, p_optBeta, p_optGamma,\n                p_optPhi, p_givenAlpha, p_givenBeta, p_givenGamma,\n                p_givenPhi, alpha, beta, gamma, phi)\n\n\nsource\n\n\n\n\n is_constant (x)"
  },
  {
    "objectID": "src/theta.html",
    "href": "src/theta.html",
    "title": "Theta Model",
    "section": "",
    "text": "source\n\n\n\n theta_target_fn (optimal_param, init_level, init_alpha, init_theta,\n                  opt_level, opt_alpha, opt_theta, y, modeltype, nmse)\n\n\nsource\n\n\n\n\n is_constant (x)\n\n\nis_constant(ap)\n\n\nforecast_theta(res, 12, level=[90, 80])\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/theta.html#thetacalc",
    "href": "src/theta.html#thetacalc",
    "title": "Theta Model",
    "section": "",
    "text": "source\n\n\n\n theta_target_fn (optimal_param, init_level, init_alpha, init_theta,\n                  opt_level, opt_alpha, opt_theta, y, modeltype, nmse)\n\n\nsource\n\n\n\n\n is_constant (x)\n\n\nis_constant(ap)\n\n\nforecast_theta(res, 12, level=[90, 80])"
  },
  {
    "objectID": "src/core/distributed.fugue.html",
    "href": "src/core/distributed.fugue.html",
    "title": "FugueBackend",
    "section": "",
    "text": "from statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    AutoARIMA,\n    AutoETS,\n)\nfrom statsforecast.utils import generate_series\nn_series = 4\nhorizon = 7\n\nseries = generate_series(n_series)\n\nsf = StatsForecast(\n    models=[AutoETS(season_length=7)],\n    freq='D',\n)\n\nsf.cross_validation(df=series, h=horizon, step_size = 24,\n    n_windows = 2, level=[90]).head()\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Make unique_id a column\nseries = series.reset_index()\nseries['unique_id'] = series['unique_id'].astype(str)\n\n# Convert to Spark\nsdf = spark.createDataFrame(series)\n\n# Returns a Spark DataFrame\nsf.cross_validation(df=sdf, h=horizon, step_size = 24,\n    n_windows = 2, level=[90]).show()\nsource\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/core/distributed.fugue.html#dask-distributed-predictions",
    "href": "src/core/distributed.fugue.html#dask-distributed-predictions",
    "title": "FugueBackend",
    "section": "Dask Distributed Predictions",
    "text": "Dask Distributed Predictions\nHere we provide an example for the distribution of the StatsForecast predictions using Fugue to execute the code in a Dask cluster.\nTo do it we instantiate the FugueBackend class with a DaskExecutionEngine.\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\nfrom fugue_dask import DaskExecutionEngine\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import generate_series\n\n\n# Generate Synthetic Panel Data\ndf = generate_series(10).reset_index()\ndf['unique_id'] = df['unique_id'].astype(str)\ndf = dd.from_pandas(df, npartitions=10)\n\n# Instantiate FugueBackend with DaskExecutionEngine\ndask_client = Client()\nengine = DaskExecutionEngine(dask_client=dask_client)\n\nWe have simply create the class to the usual StatsForecast instantiation.\n\nsf = StatsForecast(models=[Naive()], freq='D')\n\n\nDistributed Forecast\nFor extremely fast distributed predictions we use FugueBackend as backend that operates like the original StatsForecast.forecast method.\nIt receives as input a pandas.DataFrame with columns [unique_id,ds,y] and exogenous, where the ds (datestamp) column should be of a format expected by Pandas. The y column must be numeric, and represents the measurement we wish to forecast. And the unique_id uniquely identifies the series in the panel data.\n\n# Distributed predictions with FugueBackend.\nsf.forecast(df=df, h=12).compute()\n\n\n\nDistributed Cross-Validation\nFor extremely fast distributed temporcal cross-validation we use cross_validation method that operates like the original StatsForecast.cross_validation method.\n\n# Distributed cross-validation with FugueBackend.\nsf.cross_validation(df=df, h=12, n_windows=2).compute()"
  },
  {
    "objectID": "src/core/models.html",
    "href": "src/core/models.html",
    "title": "Models",
    "section": "",
    "text": "StatsForecast offers a wide variety of models grouped in the following categories:\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/core/models.html#autoarima",
    "href": "src/core/models.html#autoarima",
    "title": "Models",
    "section": "AutoARIMA",
    "text": "AutoARIMA\n\nsource\n\nAutoARIMA\n\n AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=False, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=False, allowmean:bool=False,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            season_length:int=1, alias:str='AutoARIMA', prediction_interva\n            ls:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nAutoARIMA model.\nAutomatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Default is Akaike Information Criterion (AICc).\nNote:\nThis implementation is a mirror of Hyndman‚Äôs forecast::auto.arima.\nReferences:\nRob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd\ntyping.Optional[int]\nNone\nOrder of first-differencing.\n\n\nD\ntyping.Optional[int]\nNone\nOrder of seasonal-differencing.\n\n\nmax_p\nint\n5\nMax autorregresives p.\n\n\nmax_q\nint\n5\nMax moving averages q.\n\n\nmax_P\nint\n2\nMax seasonal autorregresives P.\n\n\nmax_Q\nint\n2\nMax seasonal moving averages Q.\n\n\nmax_order\nint\n5\nMax p+q+P+Q value if not stepwise selection.\n\n\nmax_d\nint\n2\nMax non-seasonal differences.\n\n\nmax_D\nint\n1\nMax seasonal differences.\n\n\nstart_p\nint\n2\nStarting value of p in stepwise procedure.\n\n\nstart_q\nint\n2\nStarting value of q in stepwise procedure.\n\n\nstart_P\nint\n1\nStarting value of P in stepwise procedure.\n\n\nstart_Q\nint\n1\nStarting value of Q in stepwise procedure.\n\n\nstationary\nbool\nFalse\nIf True, restricts search to stationary models.\n\n\nseasonal\nbool\nTrue\nIf False, restricts search to non-seasonal models.\n\n\nic\nstr\naicc\nInformation criterion to be used in model selection.\n\n\nstepwise\nbool\nTrue\nIf True, will do stepwise selection (faster).\n\n\nnmodels\nint\n94\nNumber of models considered in stepwise search.\n\n\ntrace\nbool\nFalse\nIf True, the searched ARIMA models is reported.\n\n\napproximation\ntyping.Optional[bool]\nFalse\nIf True, conditional sums-of-squares estimation, final MLE.\n\n\nmethod\ntyping.Optional[str]\nNone\nFitting method between maximum likelihood or sums-of-squares.\n\n\ntruncate\ntyping.Optional[bool]\nNone\nObservations truncated series used in model selection.\n\n\ntest\nstr\nkpss\nUnit root test to use. See ndiffs for details.\n\n\ntest_kwargs\ntyping.Optional[str]\nNone\nUnit root test additional arguments.\n\n\nseasonal_test\nstr\nseas\nSelection method for seasonal differences.\n\n\nseasonal_test_kwargs\ntyping.Optional[typing.Dict]\nNone\nSeasonal unit root test arguments.\n\n\nallowdrift\nbool\nFalse\nIf True, drift models terms considered.\n\n\nallowmean\nbool\nFalse\nIf True, non-zero mean models considered.\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalias\nstr\nAutoARIMA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoARIMA.fit\n\n AutoARIMA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoARIMA model.\nFit an AutoARIMA to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoARIMA fitted model.\n\n\n\n\nsource\n\n\nAutoARIMA.predict\n\n AutoARIMA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None)\n\nPredict with fitted AutoArima.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.predict_in_sample\n\n AutoARIMA.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoArima insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.forecast\n\n AutoARIMA.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoARIMA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.forward\n\n AutoARIMA.forward (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted ARIMA model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoARIMA's usage example\n\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.utils import AirPassengers as ap\n\narima = AutoARIMA(season_length=4)\narima = arima.fit(y=ap)\ny_hat_dict = arima.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([497.95290378, 486.7806859 , 500.08214752, 494.10983682]),\n 'lo-80': 0    467.167462\n 1    442.112227\n 2    450.786736\n 3    440.963293\n Name: 80%, dtype: float64,\n 'hi-80': 0    528.738346\n 1    531.449145\n 2    549.377559\n 3    547.256381\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "src/core/models.html#autoets",
    "href": "src/core/models.html#autoets",
    "title": "Models",
    "section": "AutoETS",
    "text": "AutoETS\n\nsource\n\nAutoETS\n\n AutoETS (season_length:int=1, model:str='ZZZ',\n          damped:Optional[bool]=None, alias:str='AutoETS', prediction_inte\n          rvals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nAutomatic Exponential Smoothing model.\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative, \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: E in [\\(M, A, Z\\)], T in [\\(N, A, M, Z\\)], and S in [\\(N, A, M, Z\\)].\nFor example when model=‚ÄòANN‚Äô (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‚ÄòZ‚Äô, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nNote:\nThis implementation is a mirror of Hyndman‚Äôs forecast::ets.\nReferences:\nRob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\nHyndman, Rob, et al (2008). ‚ÄúForecasting with exponential smoothing: the state space approach‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZZZ\nControlling state-space-equations.\n\n\ndamped\ntyping.Optional[bool]\nNone\nA parameter that ‚Äòdampens‚Äô the trend.\n\n\nalias\nstr\nAutoETS\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoETS.fit\n\n AutoETS.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nAutoETS.predict\n\n AutoETS.predict (h:int, X:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.predict_in_sample\n\n AutoETS.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.forecast\n\n AutoETS.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                   X_future:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.forward\n\n AutoETS.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                  X_future:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoETS' usage example\n\nfrom statsforecast.models import AutoETS\nfrom statsforecast.utils import AirPassengers as ap\n\n# Multiplicative trend, optimal error and seasonality\nautoets = AutoETS(model='ZMZ',  \n              season_length=4)\nautoets = autoets.fit(y=ap)\ny_hat_dict = autoets.predict(h=4)\ny_hat_dict\n\n{'mean': array([416.63294737, 419.65915384, 442.66309931, 457.33314074])}\n\n\n\nsource\n\n\nETS\n\n ETS (season_length:int=1, model:str='ZZZ', damped:Optional[bool]=None,\n      alias:str='ETS', prediction_intervals:Optional[statsforecast.utils.C\n      onformalIntervals]=None)\n\nAutomatic Exponential Smoothing model.\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative, \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: E in [\\(M, A, Z\\)], T in [\\(N, A, M, Z\\)], and S in [\\(N, A, M, Z\\)].\nFor example when model=‚ÄòANN‚Äô (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‚ÄòZ‚Äô, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nNote: This implementation is a mirror of Hyndman‚Äôs forecast::ets.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\nHyndman, Rob, et al (2008). ‚ÄúForecasting with exponential smoothing: the state space approach‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZZZ\nControlling state-space-equations.\n\n\ndamped\ntyping.Optional[bool]\nNone\nA parameter that ‚Äòdampens‚Äô the trend.\n\n\nalias\nstr\nETS\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nets = ETS(model='ZMZ', season_length=4)"
  },
  {
    "objectID": "src/core/models.html#autoces",
    "href": "src/core/models.html#autoces",
    "title": "Models",
    "section": "AutoCES",
    "text": "AutoCES\n\nsource\n\nAutoCES\n\n AutoCES (season_length:int=1, model:str='Z', alias:str='CES', prediction_\n          intervals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nComplex Exponential Smoothing model.\nAutomatically selects the best Complex Exponential Smoothing model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(S\\) simple, \\(P\\) parial, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the kind of CES model: \\(N\\) for simple CES (withous seasonality), \\(S\\) for simple seasonality (lagged CES), \\(P\\) for partial seasonality (without complex part), \\(F\\) for full seasonality (lagged CES with real and complex seasonal parts).\nIf the component is selected as ‚ÄòZ‚Äô, it operates as a placeholder to ask the AutoCES model to figure out the best parameter.\nReferences:\nSvetunkov, Ivan & Kourentzes, Nikolaos. (2015). ‚ÄúComplex Exponential Smoothing‚Äù. 10.13140/RG.2.1.3757.2562..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZ\nControlling state-space-equations.\n\n\nalias\nstr\nCES\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoCES.fit\n\n AutoCES.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Complex Exponential Smoothing model.\nFit the Complex Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nComplex Exponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nAutoCES.predict\n\n AutoCES.predict (h:int, X:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.predict_in_sample\n\n AutoCES.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.forecast\n\n AutoCES.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                   X_future:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Complex Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.forward\n\n AutoCES.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                  X_future:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Complex Exponential Smoothing to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# CES' usage example\n\nfrom statsforecast.models import AutoCES\nfrom statsforecast.utils import AirPassengers as ap\n\n# Multiplicative trend, optimal error and seasonality\nces = AutoCES(model='Z',  \n              season_length=4)\nces = ces.fit(y=ap)\ny_hat_dict = ces.predict(h=4)\ny_hat_dict\n\n{'mean': array([424.30716324, 405.69589186, 442.02640533, 443.63488996])}"
  },
  {
    "objectID": "src/core/models.html#autotheta",
    "href": "src/core/models.html#autotheta",
    "title": "Models",
    "section": "AutoTheta",
    "text": "AutoTheta\n\nsource\n\nAutoTheta\n\n AutoTheta (season_length:int=1, decomposition_type:str='multiplicative',\n            model:Optional[str]=None, alias:str='AutoTheta', prediction_in\n            tervals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nAutoTheta model.\nAutomatically selects the best Theta (Standard Theta Model (‚ÄòSTM‚Äô), Optimized Theta Model (‚ÄòOTM‚Äô), Dynamic Standard Theta Model (‚ÄòDSTM‚Äô), Dynamic Optimized Theta Model (‚ÄòDOTM‚Äô)) model using mse.\nReferences:\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\n\n\nmodel\ntyping.Optional[str]\nNone\nControlling Theta Model. By default searchs the best model.\n\n\nalias\nstr\nAutoTheta\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoTheta.fit\n\n AutoTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nAutoTheta.predict\n\n AutoTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.predict_in_sample\n\n AutoTheta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.forecast\n\n AutoTheta.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.forward\n\n AutoTheta.forward (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoTheta's usage example\n\nfrom statsforecast.models import AutoTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\ntheta = AutoTheta(season_length=4)\ntheta = theta.fit(y=ap)\ny_hat_dict = theta.predict(h=4)\ny_hat_dict\n\n{'mean': array([413.86262032, 410.60532872, 429.59124482, 440.16565301])}"
  },
  {
    "objectID": "src/core/models.html#arima",
    "href": "src/core/models.html#arima",
    "title": "Models",
    "section": "ARIMA",
    "text": "ARIMA\n\nsource\n\nARIMA\n\n ARIMA (order:Tuple[int,int,int]=(0, 0, 0), season_length:int=1,\n        seasonal_order:Tuple[int,int,int]=(0, 0, 0),\n        include_mean:bool=True, include_drift:bool=False,\n        include_constant:Optional[bool]=None,\n        blambda:Optional[float]=None, biasadj:bool=False, method:str='CSS-\n        ML', fixed:Optional[dict]=None, alias:str='ARIMA', prediction_inte\n        rvals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nARIMA model.\nAutoRegressive Integrated Moving Average model.\nReferences:\nRob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norder\ntyping.Tuple[int, int, int]\n(0, 0, 0)\nA specification of the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order.\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nseasonal_order\ntyping.Tuple[int, int, int]\n(0, 0, 0)\nA specification of the seasonal part of the ARIMA model.(P, D, Q) for the AR order, the degree of differencing, the MA order.\n\n\ninclude_mean\nbool\nTrue\nShould the ARIMA model include a mean term? The default is True for undifferenced series, False for differenced ones (where a mean would not affect the fit nor predictions).\n\n\ninclude_drift\nbool\nFalse\nShould the ARIMA model include a linear drift term? (i.e., a linear regression with ARIMA errors is fitted.)\n\n\ninclude_constant\ntyping.Optional[bool]\nNone\nIf True, then includ_mean is set to be True for undifferenced series and include_drift is set to be True for differenced series. Note that if there is more than one difference taken, no constant is included regardless of the value of this argument. This is deliberate as otherwise quadratic and higher order polynomial trends would be induced.\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nmethod\nstr\nCSS-ML\nFitting method: maximum likelihood or minimize conditional sum-of-squares. The default (unless there are missing values) is to use conditional-sum-of-squares to find starting values, then maximum likelihood.\n\n\nfixed\ntyping.Optional[dict]\nNone\nDictionary containing fixed coefficients for the arima model. Example: {'ar1': 0.5, 'ma2': 0.75}.For autoregressive terms use the ar{i} keys. For its seasonal version use sar{i}.For moving average terms use the ma{i} keys. For its seasonal version use sma{i}.For intercept and drift use the intercept and drift keys.For exogenous variables use the ex_{i} keys.\n\n\nalias\nstr\nARIMA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nARIMA.fit\n\n ARIMA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nFitted model.\n\n\n\n\nsource\n\n\nARIMA.predict\n\n ARIMA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.predict_in_sample\n\n ARIMA.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.forecast\n\n ARIMA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.forward\n\n ARIMA.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# ARIMA's usage example\n\nfrom statsforecast.models import ARIMA\nfrom statsforecast.utils import AirPassengers as ap\n\n\narima = ARIMA(order=(1, 0, 0), season_length=12)\narima = arima.fit(y=ap)\ny_hat_dict = arima.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([426.54529705, 421.28383474, 416.20876726, 411.31349129]),\n 'lo-80': 0    383.228999\n 1    361.100640\n 2    343.777875\n 3    329.110028\n Name: 80%, dtype: float64,\n 'hi-80': 0    469.861595\n 1    481.467029\n 2    488.639659\n 3    493.516954\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "src/core/models.html#autoregressive",
    "href": "src/core/models.html#autoregressive",
    "title": "Models",
    "section": "AutoRegressive",
    "text": "AutoRegressive\n\nsource\n\nAutoRegressive\n\n AutoRegressive (lags:Tuple[int,List], include_mean:bool=True,\n                 include_drift:bool=False, blambda:Optional[float]=None,\n                 biasadj:bool=False, method:str='CSS-ML',\n                 fixed:Optional[dict]=None, alias:str='AutoRegressive', pr\n                 ediction_intervals:Optional[statsforecast.utils.Conformal\n                 Intervals]=None)\n\nSimple Autoregressive model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlags\ntyping.Tuple[int, typing.List]\n\nNumber of lags to include in the model. If an int is passed then all lags up to lags are considered.If a list, only the elements of the list are considered as lags.\n\n\ninclude_mean\nbool\nTrue\nShould the AutoRegressive model include a mean term? The default is True for undifferenced series, False for differenced ones (where a mean would not affect the fit nor predictions).\n\n\ninclude_drift\nbool\nFalse\nShould the AutoRegressive model include a linear drift term? (i.e., a linear regression with AutoRegressive errors is fitted.)\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nmethod\nstr\nCSS-ML\nFitting method: maximum likelihood or minimize conditional sum-of-squares. The default (unless there are missing values) is to use conditional-sum-of-squares to find starting values, then maximum likelihood.\n\n\nfixed\ntyping.Optional[dict]\nNone\nDictionary containing fixed coefficients for the AutoRegressive model. Example: {'ar1': 0.5, 'ar5': 0.75}.For autoregressive terms use the ar{i} keys.\n\n\nalias\nstr\nAutoRegressive\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoRegressive.fit\n\n AutoRegressive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nFitted model.\n\n\n\n\nsource\n\n\nAutoRegressive.predict\n\n AutoRegressive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None)\n\nPredict with fitted model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.predict_in_sample\n\n AutoRegressive.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.forecast\n\n AutoRegressive.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.forward\n\n AutoRegressive.forward (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nApply fitted model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoRegressive's usage example\n\nfrom statsforecast.models import AutoRegressive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nar = AutoRegressive(lags=[12])\nar = ar.fit(y=ap)\ny_hat_dict = ar.predict(h=4, level=[80])\ny_hat_dict\n\n/home/runner/work/statsforecast/statsforecast/statsforecast/arima.py:829: UserWarning: some AR parameters were fixed: setting transform_pars = False\n  warnings.warn(\n\n\n{'mean': array([460.01874698, 432.12629561, 462.16432016, 507.22135699]),\n 'lo-80': 0    439.539875\n 1    411.647423\n 2    441.685448\n 3    486.742485\n Name: 80%, dtype: float64,\n 'hi-80': 0    480.497619\n 1    452.605168\n 2    482.643192\n 3    527.700229\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "src/core/models.html#simplesmooth",
    "href": "src/core/models.html#simplesmooth",
    "title": "Models",
    "section": "SimpleSmooth",
    "text": "SimpleSmooth\n\nsource\n\nSimpleExponentialSmoothing\n\n SimpleExponentialSmoothing (alpha:float, alias:str='SES', prediction_inte\n                             rvals:Optional[statsforecast.utils.ConformalI\n                             ntervals]=None)\n\nSimpleExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\(\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}\\)\nThe rate \\(0 \\leq \\alpha \\leq 1\\) at which the weights decrease is called the smoothing parameter. When \\(\\alpha = 1\\), SES is equal to the naive method.\nReferences:\nCharles C Holt (1957). ‚ÄúForecasting seasonals and trends by exponentially weighted moving averages‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalpha\nfloat\n\nSmoothing parameter.\n\n\nalias\nstr\nSES\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.forecast\n\n SimpleExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                      X:Optional[numpy.ndarray]=None, X_fu\n                                      ture:Optional[numpy.ndarray]=None,\n                                      level:Optional[List[int]]=None,\n                                      fitted:bool=False)\n\nMemory Efficient SimpleExponentialSmoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.fit\n\n SimpleExponentialSmoothing.fit (y:numpy.ndarray,\n                                 X:Optional[numpy.ndarray]=None)\n\nFit the SimpleExponentialSmoothing model.\nFit an SimpleExponentialSmoothing to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSimpleExponentialSmoothing fitted model.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.predict\n\n SimpleExponentialSmoothing.predict (h:int,\n                                     X:Optional[numpy.ndarray]=None,\n                                     level:Optional[List[int]]=None)\n\nPredict with fitted SimpleExponentialSmoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.predict_in_sample\n\n SimpleExponentialSmoothing.predict_in_sample ()\n\nAccess fitted SimpleExponentialSmoothing insample predictions.\n\n# SimpleExponentialSmoothing's usage example\n\nfrom statsforecast.models import SimpleExponentialSmoothing\nfrom statsforecast.utils import AirPassengers as ap\n\n\nses = SimpleExponentialSmoothing(alpha=0.5)\nses = ses.fit(y=ap)\ny_hat_dict = ses.predict(h=4)\ny_hat_dict\n\n{'mean': array([439.256, 439.256, 439.256, 439.256], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#simplesmoothoptimized",
    "href": "src/core/models.html#simplesmoothoptimized",
    "title": "Models",
    "section": "SimpleSmoothOptimized",
    "text": "SimpleSmoothOptimized\n\nsource\n\nSimpleExponentialSmoothingOptimized\n\n SimpleExponentialSmoothingOptimized (alias:str='SESOpt', prediction_inter\n                                      vals:Optional[statsforecast.utils.Co\n                                      nformalIntervals]=None)\n\nSimpleExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\(\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}\\)\nThe smoothing parameter \\(\\alpha^*\\) is optimized by square error minimization.\nReferences:\nCharles C Holt (1957). ‚ÄúForecasting seasonals and trends by exponentially weighted moving averages‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nSESOpt\n\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.fit\n\n SimpleExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                          X:Optional[numpy.ndarray]=None)\n\nFit the SimpleExponentialSmoothingOptimized model.\nFit an SimpleExponentialSmoothingOptimized to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSimpleExponentialSmoothingOptimized fitted model.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.predict\n\n SimpleExponentialSmoothingOptimized.predict (h:int,\n                                              X:Optional[numpy.ndarray]=No\n                                              ne, level:Optional[List[int]\n                                              ]=None)\n\nPredict with fitted SimpleExponentialSmoothingOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.predict_in_sample\n\n SimpleExponentialSmoothingOptimized.predict_in_sample ()\n\nAccess fitted SimpleExponentialSmoothingOptimized insample predictions.\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.forecast\n\n SimpleExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                               X:Optional[numpy.ndarray]=N\n                                               one, X_future:Optional[nump\n                                               y.ndarray]=None, level:Opti\n                                               onal[List[int]]=None,\n                                               fitted:bool=False)\n\nMemory Efficient SimpleExponentialSmoothingOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SimpleExponentialSmoothingOptimized's usage example\n\nfrom statsforecast.models import SimpleExponentialSmoothingOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nseso = SimpleExponentialSmoothingOptimized()\nseso = seso.fit(y=ap)\ny_hat_dict = seso.predict(h=4)\ny_hat_dict\n\n{'mean': array([431.58716, 431.58716, 431.58716, 431.58716], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#seasonalsmooth",
    "href": "src/core/models.html#seasonalsmooth",
    "title": "Models",
    "section": "SeasonalSmooth",
    "text": "SeasonalSmooth\n\nplt.plot(np.concatenate([ap[6:], seas_es.forecast(ap[6:], h=12)['mean']]))\n\n\nsource\n\nSeasonalExponentialSmoothing\n\n SeasonalExponentialSmoothing (season_length:int, alpha:float,\n                               alias:str='SeasonalES', prediction_interval\n                               s:Optional[statsforecast.utils.ConformalInt\n                               ervals]=None)\n\nSeasonalExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations and season \\(s\\), the one-step forecast is given by: \\(\\hat{y}_{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1,s}\\)\nNote:\nThis method is an extremely simplified of Holt-Winter‚Äôs method where the trend and level are set to zero. And a single seasonal smoothing parameter \\(\\alpha\\) is shared across seasons.\nReferences:\nCharles. C. Holt (1957). ‚ÄúForecasting seasonals and trends by exponentially weighted moving averages‚Äù, ONR Research Memorandum, Carnegie Institute of Technology 52.. Peter R. Winters (1960). ‚ÄúForecasting sales by exponentially weighted moving averages‚Äù. Management Science.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalpha\nfloat\n\nSmoothing parameter.\n\n\nalias\nstr\nSeasonalES\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.fit\n\n SeasonalExponentialSmoothing.fit (y:numpy.ndarray,\n                                   X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalExponentialSmoothing model.\nFit an SeasonalExponentialSmoothing to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalExponentialSmoothing fitted model.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.predict\n\n SeasonalExponentialSmoothing.predict (h:int,\n                                       X:Optional[numpy.ndarray]=None,\n                                       level:Optional[List[int]]=None)\n\nPredict with fitted SeasonalExponentialSmoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.predict_in_sample\n\n SeasonalExponentialSmoothing.predict_in_sample ()\n\nAccess fitted SeasonalExponentialSmoothing insample predictions.\n\nsource\n\n\nSeasonalExponentialSmoothing.forecast\n\n SeasonalExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                        X:Optional[numpy.ndarray]=None, X_\n                                        future:Optional[numpy.ndarray]=Non\n                                        e, level:Optional[List[int]]=None,\n                                        fitted:bool=False)\n\nMemory Efficient SeasonalExponentialSmoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SeasonalExponentialSmoothing's usage example\n\nfrom statsforecast.models import SeasonalExponentialSmoothing\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalExponentialSmoothing(alpha=0.5, season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([376.28955, 354.71094, 396.02002, 412.06738], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#seasonalsmoothoptimized",
    "href": "src/core/models.html#seasonalsmoothoptimized",
    "title": "Models",
    "section": "SeasonalSmoothOptimized",
    "text": "SeasonalSmoothOptimized\n\nsource\n\nSeasonalExponentialSmoothingOptimized\n\n SeasonalExponentialSmoothingOptimized (season_length:int,\n                                        alias:str='SeasESOpt', prediction_\n                                        intervals:Optional[statsforecast.u\n                                        tils.ConformalIntervals]=None)\n\nSeasonalExponentialSmoothingOptimized model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations and season \\(s\\), the one-step forecast is given by: \\(\\hat{y}_{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1,s}\\)\nThe smoothing parameter \\(\\alpha^*\\) is optimized by square error minimization.\nNote:\nThis method is an extremely simplified of Holt-Winter‚Äôs method where the trend and level are set to zero. And a single seasonal smoothing parameter \\(\\alpha\\) is shared across seasons.\nReferences:\nCharles. C. Holt (1957). ‚ÄúForecasting seasonals and trends by exponentially weighted moving averages‚Äù, ONR Research Memorandum, Carnegie Institute of Technology 52.. Peter R. Winters (1960). ‚ÄúForecasting sales by exponentially weighted moving averages‚Äù. Management Science.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalias\nstr\nSeasESOpt\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.forecast\n\n SeasonalExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                                 X:Optional[numpy.ndarray]\n                                                 =None, X_future:Optional[\n                                                 numpy.ndarray]=None, leve\n                                                 l:Optional[List[int]]=Non\n                                                 e, fitted:bool=False)\n\nMemory Efficient SeasonalExponentialSmoothingOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.fit\n\n SeasonalExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                            X:Optional[numpy.ndarray]=None\n                                            )\n\nFit the SeasonalExponentialSmoothingOptimized model.\nFit an SeasonalExponentialSmoothingOptimized to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalExponentialSmoothingOptimized fitted model.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.predict\n\n SeasonalExponentialSmoothingOptimized.predict (h:int,\n                                                X:Optional[numpy.ndarray]=\n                                                None, level:Optional[List[\n                                                int]]=None)\n\nPredict with fitted SeasonalExponentialSmoothingOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.predict_in_sample\n\n SeasonalExponentialSmoothingOptimized.predict_in_sample ()\n\nAccess fitted SeasonalExponentialSmoothingOptimized insample predictions.\n\n# SeasonalExponentialSmoothingOptimized's usage example\n\nfrom statsforecast.models import SeasonalExponentialSmoothingOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalExponentialSmoothingOptimized(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([416.42798, 390.50757, 418.8656 , 460.3452 ], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#holts-method",
    "href": "src/core/models.html#holts-method",
    "title": "Models",
    "section": "Holt‚Äôs method",
    "text": "Holt‚Äôs method\n\nsource\n\nHolt\n\n Holt (season_length:int=1, error_type:str='A', alias:str='Holt', predicti\n       on_intervals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nHolt‚Äôs method.\nAlso known as double exponential smoothing, Holt‚Äôs method is an extension of exponential smoothing for series with a trend. This implementation returns the corresponding ETS model with additive (A) or multiplicative (M) errors (so either ‚ÄòAAN‚Äô or ‚ÄòMAN‚Äô).\nReferences:\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Methods with trend‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 12 Monthly data.\n\n\nerror_type\nstr\nA\nThe type of error of the ETS model. Can be additive (A) or multiplicative (M).\n\n\nalias\nstr\nHolt\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nHolt.forecast\n\n Holt.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.fit\n\n Holt.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nHolt.predict\n\n Holt.predict (h:int, X:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.predict_in_sample\n\n Holt.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.forward\n\n Holt.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Holt's usage example\n\nfrom statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Holt(season_length=12, error_type='A')\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([434.23881352, 436.48160242, 438.72439132, 440.96718023])}"
  },
  {
    "objectID": "src/core/models.html#holt-winters-method",
    "href": "src/core/models.html#holt-winters-method",
    "title": "Models",
    "section": "Holt-Winters‚Äô method",
    "text": "Holt-Winters‚Äô method\n\nsource\n\nHoltWinters\n\n HoltWinters (season_length:int=1, error_type:str='A',\n              alias:str='HoltWinters', prediction_intervals:Optional[stats\n              forecast.utils.ConformalIntervals]=None)\n\nHolt-Winters‚Äô method.\nAlso known as triple exponential smoothing, Holt-Winters‚Äô method is an extension of exponential smoothing for series that contain both trend and seasonality. This implementation returns the corresponding ETS model with additive (A) or multiplicative (M) errors (so either ‚ÄòAAA‚Äô or ‚ÄòMAM‚Äô).\nReferences:\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Methods with seasonality‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nseason length\n\n\nerror_type\nstr\nA\nerror type\n\n\nalias\nstr\nHoltWinters\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nHoltWinters.forecast\n\n HoltWinters.forecast (y:numpy.ndarray, h:int,\n                       X:Optional[numpy.ndarray]=None,\n                       X_future:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.fit\n\n HoltWinters.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nHoltWinters.predict\n\n HoltWinters.predict (h:int, X:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.predict_in_sample\n\n HoltWinters.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.forward\n\n HoltWinters.forward (y:numpy.ndarray, h:int,\n                      X:Optional[numpy.ndarray]=None,\n                      X_future:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Holt-Winters' usage example\n\nfrom statsforecast.models import HoltWinters\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = HoltWinters(season_length=12, error_type='A')\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([453.08875135, 427.4765464 , 460.63176307, 497.03681052])}"
  },
  {
    "objectID": "src/core/models.html#historicaverage",
    "href": "src/core/models.html#historicaverage",
    "title": "Models",
    "section": "HistoricAverage",
    "text": "HistoricAverage\n\nsource\n\nHistoricAverage\n\n HistoricAverage (alias:str='HistoricAverage', prediction_intervals:Option\n                  al[statsforecast.utils.ConformalIntervals]=None)\n\nHistoricAverage model.\nAlso known as mean method. Uses a simple average of all past observations. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\[ \\hat{y}_{t+1} = \\frac{1}{t} \\sum_{j=1}^t y_j \\]\nReferences:\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Simple Methods‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nHistoricAverage\n\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nHistoricAverage.forecast\n\n HistoricAverage.forecast (y:numpy.ndarray, h:int,\n                           X:Optional[numpy.ndarray]=None,\n                           X_future:Optional[numpy.ndarray]=None,\n                           level:Optional[List[int]]=None,\n                           fitted:bool=False)\n\nMemory Efficient HistoricAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHistoricAverage.fit\n\n HistoricAverage.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the HistoricAverage model.\nFit an HistoricAverage to a time series (numpy array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself\n\nHistoricAverage fitted model.\n\n\n\n\nsource\n\n\nHistoricAverage.predict\n\n HistoricAverage.predict (h:int, X:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None)\n\nPredict with fitted HistoricAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHistoricAverage.predict_in_sample\n\n HistoricAverage.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted HistoricAverage insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# HistoricAverage's usage example\n\nfrom statsforecast.models import HistoricAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = HistoricAverage()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([280.2986, 280.2986, 280.2986, 280.2986], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#naive",
    "href": "src/core/models.html#naive",
    "title": "Models",
    "section": "Naive",
    "text": "Naive\n\nsource\n\nNaive\n\n Naive (alias:str='Naive', prediction_intervals:Optional[statsforecast.uti\n        ls.ConformalIntervals]=None)\n\nNaive model.\nAll forecasts have the value of the last observation:\n\\(\\hat{y}_{t+1} = y_t\\) for all \\(t\\)\nReferences:\nRob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nNaive\n\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nNaive.forecast\n\n Naive.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Naive predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaive.fit\n\n Naive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Naive model.\nFit an Naive to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nNaive fitted model.\n\n\n\n\nsource\n\n\nNaive.predict\n\n Naive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted Naive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaive.predict_in_sample\n\n Naive.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Naive insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Naive's usage example\n\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Naive()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([432., 432., 432., 432.], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#randomwalkwithdrift",
    "href": "src/core/models.html#randomwalkwithdrift",
    "title": "Models",
    "section": "RandomWalkWithDrift",
    "text": "RandomWalkWithDrift\n\nsource\n\nRandomWalkWithDrift\n\n RandomWalkWithDrift (alias:str='RWD', prediction_intervals:Optional[stats\n                      forecast.utils.ConformalIntervals]=None)\n\nRandomWalkWithDrift model.\nA variation of the naive method allows the forecasts to change over time. The amout of change, called drift, is the average change seen in the historical data.\n\\[ \\hat{y}_{t+1} = y_t+\\frac{1}{t-1}\\sum_{j=1}^t (y_j-y_{j-1}) = y_t+ \\frac{y_t-y_1}{t-1} \\]\nFrom the previous equation, we can see that this is equivalent to extrapolating a line between the first and the last observation.\nReferences:\nRob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nRWD\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.forecast\n\n RandomWalkWithDrift.forecast (y:numpy.ndarray, h:int,\n                               X:Optional[numpy.ndarray]=None,\n                               X_future:Optional[numpy.ndarray]=None,\n                               level:Optional[List[int]]=None,\n                               fitted:bool=False)\n\nMemory Efficient RandomWalkWithDrift predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\nforecasts: dict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.fit\n\n RandomWalkWithDrift.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the RandomWalkWithDrift model.\nFit an RandomWalkWithDrift to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nRandomWalkWithDrift fitted model.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.predict\n\n RandomWalkWithDrift.predict (h:int, X:Optional[numpy.ndarray]=None,\n                              level:Optional[List[int]]=None)\n\nPredict with fitted RandomWalkWithDrift.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.predict_in_sample\n\n RandomWalkWithDrift.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted RandomWalkWithDrift insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# RandomWalkWithDrift's usage example\n\nfrom statsforecast.models import RandomWalkWithDrift\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = RandomWalkWithDrift()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([434.23776, 436.47552, 438.7133 , 440.95105], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#seasonalnaive",
    "href": "src/core/models.html#seasonalnaive",
    "title": "Models",
    "section": "SeasonalNaive",
    "text": "SeasonalNaive\n\nsource\n\nSeasonalNaive\n\n SeasonalNaive (season_length:int, alias:str='SeasonalNaive', prediction_i\n                ntervals:Optional[statsforecast.utils.ConformalIntervals]=\n                None)\n\nSeasonal naive model.\nA method similar to the naive, but uses the last known observation of the same period (e.g.¬†the same month of the previous year) in order to capture seasonal variations.\nReferences:\nRob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalias\nstr\nSeasonalNaive\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSeasonalNaive.forecast\n\n SeasonalNaive.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nMemory Efficient SeasonalNaive predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalNaive.fit\n\n SeasonalNaive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalNaive model.\nFit an SeasonalNaive to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nSeasonalNaive fitted model.\n\n\n\n\nsource\n\n\nSeasonalNaive.predict\n\n SeasonalNaive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None)\n\nPredict with fitted Naive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalNaive.predict_in_sample\n\n SeasonalNaive.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted SeasonalNaive insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SeasonalNaive's usage example\n\nfrom statsforecast.models import SeasonalNaive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalNaive(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([417., 391., 419., 461.], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#windowaverage",
    "href": "src/core/models.html#windowaverage",
    "title": "Models",
    "section": "WindowAverage",
    "text": "WindowAverage\n\nsource\n\nWindowAverage\n\n WindowAverage (window_size:int, alias:str='WindowAverage', prediction_int\n                ervals:Optional[statsforecast.utils.ConformalIntervals]=No\n                ne)\n\nWindowAverage model.\nUses the average of the last \\(k\\) observations, with \\(k\\) the length of the window. Wider windows will capture global trends, while narrow windows will reveal local trends. The length of the window selected should take into account the importance of past observations and how fast the series changes.\nReferences:\nRob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwindow_size\nint\n\nSize of truncated series on which average is estimated.\n\n\nalias\nstr\nWindowAverage\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nWindowAverage.forecast\n\n WindowAverage.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nMemory Efficient WindowAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nWindowAverage.fit\n\n WindowAverage.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the WindowAverage model.\nFit an WindowAverage to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nWindowAverage fitted model.\n\n\n\n\nsource\n\n\nWindowAverage.predict\n\n WindowAverage.predict (h:int, X:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None)\n\nPredict with fitted WindowAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nWindowAverage.predict_in_sample\n\n WindowAverage.predict_in_sample ()\n\nAccess fitted WindowAverage insample predictions.\n\n# WindowAverage's usage example\n\nfrom statsforecast.models import WindowAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = WindowAverage(window_size=12*4)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([413.47916, 413.47916, 413.47916, 413.47916], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#seasonalwindowaverage",
    "href": "src/core/models.html#seasonalwindowaverage",
    "title": "Models",
    "section": "SeasonalWindowAverage",
    "text": "SeasonalWindowAverage\n\nsource\n\nSeasonalWindowAverage\n\n SeasonalWindowAverage (season_length:int, window_size:int,\n                        alias:str='SeasWA', prediction_intervals:Optional[\n                        statsforecast.utils.ConformalIntervals]=None)\n\nSeasonalWindowAverage model.\nAn average of the last \\(k\\) observations of the same period, with \\(k\\) the length of the window.\nReferences:\nRob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\n\n\n\nwindow_size\nint\n\nSize of truncated series on which average is estimated.\n\n\nalias\nstr\nSeasWA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.forecast\n\n SeasonalWindowAverage.forecast (y:numpy.ndarray, h:int,\n                                 X:Optional[numpy.ndarray]=None,\n                                 X_future:Optional[numpy.ndarray]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False)\n\nMemory Efficient SeasonalWindowAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.fit\n\n SeasonalWindowAverage.fit (y:numpy.ndarray,\n                            X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalWindowAverage model.\nFit an SeasonalWindowAverage to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalWindowAverage fitted model.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.predict\n\n SeasonalWindowAverage.predict (h:int, X:Optional[numpy.ndarray]=None,\n                                level:Optional[List[int]]=None)\n\nPredict with fitted SeasonalWindowAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.predict_in_sample\n\n SeasonalWindowAverage.predict_in_sample ()\n\nAccess fitted SeasonalWindowAverage insample predictions.\n\n# SeasonalWindowAverage's usage example\n\nfrom statsforecast.models import SeasonalWindowAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalWindowAverage(season_length=12, window_size=4)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([358.  , 338.  , 385.75, 388.25], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#adida",
    "href": "src/core/models.html#adida",
    "title": "Models",
    "section": "ADIDA",
    "text": "ADIDA\n\nsource\n\nADIDA\n\n ADIDA (alias:str='ADIDA', prediction_intervals:Optional[statsforecast.uti\n        ls.ConformalIntervals]=None)\n\nADIDA model.\nAggregate-Dissagregate Intermittent Demand Approach: Uses temporal aggregation to reduce the number of zero observations. Once the data has been agregated, it uses the optimized SES to generate the forecasts at the new level. It then breaks down the forecast to the original level using equal weights.\nADIDA specializes on sparse or intermittent series are series with very few non-zero observations. They are notoriously hard to forecast, and so, different methods have been developed especifically for them.\nReferences:\nNikolopoulos, K., Syntetos, A. A., Boylan, J. E., Petropoulos, F., & Assimakopoulos, V. (2011). An aggregate‚Äìdisaggregate intermittent demand approach (ADIDA) to forecasting: an empirical proposition and analysis. Journal of the Operational Research Society, 62(3), 544-554..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nADIDA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nADIDA.forecast\n\n ADIDA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient ADIDA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nADIDA.fit\n\n ADIDA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the ADIDA model.\nFit an ADIDA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nADIDA fitted model.\n\n\n\n\nsource\n\n\nADIDA.predict\n\n ADIDA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted ADIDA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nADIDA.predict_in_sample\n\n ADIDA.predict_in_sample ()\n\nAccess fitted ADIDA insample predictions.\n\n# ADIDA's usage example\n\nfrom statsforecast.models import ADIDA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = ADIDA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#crostonclassic",
    "href": "src/core/models.html#crostonclassic",
    "title": "Models",
    "section": "CrostonClassic",
    "text": "CrostonClassic\n\nsource\n\nCrostonClassic\n\n CrostonClassic (alias:str='CrostonClassic', prediction_intervals:Optional\n                 [statsforecast.utils.ConformalIntervals]=None)\n\nCrostonClassic model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nwhere \\(\\hat{z}_t\\) and \\(\\hat{p}_t\\) are forecasted using SES. The smoothing parameter of both components is set equal to 0.1\nReferences:\nCroston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonClassic\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nCrostonClassic.forecast\n\n CrostonClassic.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory Efficient CrostonClassic predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonClassic.fit\n\n CrostonClassic.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonClassic model.\nFit an CrostonClassic to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonClassic fitted model.\n\n\n\n\nsource\n\n\nCrostonClassic.predict\n\n CrostonClassic.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None)\n\nPredict with fitted CrostonClassic.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonClassic.predict_in_sample\n\n CrostonClassic.predict_in_sample (level)\n\nAccess fitted CrostonClassic insample predictions.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nlevel\n\n\n\n\nReturns\ndict\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# CrostonClassic's usage example\n\nfrom statsforecast.models import CrostonClassic\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonClassic()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([460.30276, 460.30276, 460.30276, 460.30276], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#crostonoptimized",
    "href": "src/core/models.html#crostonoptimized",
    "title": "Models",
    "section": "CrostonOptimized",
    "text": "CrostonOptimized\n\nsource\n\nCrostonOptimized\n\n CrostonOptimized (alias:str='CrostonOptimized', prediction_intervals:Opti\n                   onal[statsforecast.utils.ConformalIntervals]=None)\n\nCrostonOptimized model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nA variation of the classic Croston‚Äôs method where the smooting paramater is optimally selected from the range \\([0.1,0.3]\\). Both the non-zero demand \\(z_t\\) and the inter-demand intervals \\(p_t\\) are smoothed separately, so their smoothing parameters can be different.\nReferences:\nCroston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonOptimized\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nCrostonOptimized.forecast\n\n CrostonOptimized.forecast (y:numpy.ndarray, h:int,\n                            X:Optional[numpy.ndarray]=None,\n                            X_future:Optional[numpy.ndarray]=None,\n                            level:Optional[List[int]]=None,\n                            fitted:bool=False)\n\nMemory Efficient CrostonOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonOptimized.fit\n\n CrostonOptimized.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonOptimized model.\nFit an CrostonOptimized to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonOptimized fitted model.\n\n\n\n\nsource\n\n\nCrostonOptimized.predict\n\n CrostonOptimized.predict (h:int, X:Optional[numpy.ndarray]=None,\n                           level:Optional[List[int]]=None)\n\nPredict with fitted CrostonOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonOptimized.predict_in_sample\n\n CrostonOptimized.predict_in_sample ()\n\nAccess fitted CrostonOptimized insample predictions.\n\n# CrostonOptimized's usage example\n\nfrom statsforecast.models import CrostonOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonOptimized()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#crostonsba",
    "href": "src/core/models.html#crostonsba",
    "title": "Models",
    "section": "CrostonSBA",
    "text": "CrostonSBA\n\nsource\n\nCrostonSBA\n\n CrostonSBA (alias:str='CrostonSBA', prediction_intervals:Optional[statsfo\n             recast.utils.ConformalIntervals]=None)\n\nCrostonSBA model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nA variation of the classic Croston‚Äôs method that uses a debiasing factor, so that the forecast is given by: \\[ \\hat{y}_t = 0.95  \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nReferences:\nCroston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonSBA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nCrostonSBA.forecast\n\n CrostonSBA.forecast (y:numpy.ndarray, h:int,\n                      X:Optional[numpy.ndarray]=None,\n                      X_future:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient CrostonSBA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonSBA.fit\n\n CrostonSBA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonSBA model.\nFit an CrostonSBA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonSBA fitted model.\n\n\n\n\nsource\n\n\nCrostonSBA.predict\n\n CrostonSBA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None)\n\nPredict with fitted CrostonSBA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonSBA.predict_in_sample\n\n CrostonSBA.predict_in_sample ()\n\nAccess fitted CrostonSBA insample predictions.\n\n# CrostonSBA's usage example\n\nfrom statsforecast.models import CrostonSBA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonSBA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([437.28763, 437.28763, 437.28763, 437.28763], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#imapa",
    "href": "src/core/models.html#imapa",
    "title": "Models",
    "section": "IMAPA",
    "text": "IMAPA\n\nsource\n\nIMAPA\n\n IMAPA (alias:str='IMAPA', prediction_intervals:Optional[statsforecast.uti\n        ls.ConformalIntervals]=None)\n\nIMAPA model.\nIntermittent Multiple Aggregation Prediction Algorithm: Similar to ADIDA, but instead of using a single aggregation level, it considers multiple in order to capture different dynamics of the data. Uses the optimized SES to generate the forecasts at the new levels and then combines them using a simple average.\nReferences:\nSyntetos, A. A., & Boylan, J. E. (2021). Intermittent demand forecasting: Context, methods and applications. John Wiley & Sons..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nIMAPA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nIMAPA.forecast\n\n IMAPA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient IMAPA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nIMAPA.fit\n\n IMAPA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the IMAPA model.\nFit an IMAPA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nIMAPA fitted model.\n\n\n\n\nsource\n\n\nIMAPA.predict\n\n IMAPA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted IMAPA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nIMAPA.predict_in_sample\n\n IMAPA.predict_in_sample ()\n\nAccess fitted IMAPA insample predictions.\n\n# IMAPA's usage example\n\nfrom statsforecast.models import IMAPA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = IMAPA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#tsb",
    "href": "src/core/models.html#tsb",
    "title": "Models",
    "section": "TSB",
    "text": "TSB\n\nsource\n\nTSB\n\n TSB (alpha_d:float, alpha_p:float, alias:str='TSB', prediction_intervals:\n      Optional[statsforecast.utils.ConformalIntervals]=None)\n\nTSB model.\nTeunter-Syntetos-Babai: A modification of Croston‚Äôs method that replaces the inter-demand intervals with the demand probability \\(d_t\\), which is defined as follows.\n\\[\nd_t = \\begin{cases}\n    1  & \\text{if demand occurs at time t} \\\\\n    0  & \\text{otherwise.}\n\\end{cases}\n\\]\nHence, the forecast is given by\n\\[\\hat{y}_t= \\hat{d}_t\\hat{z_t}\\]\nBoth \\(d_t\\) and \\(z_t\\) are forecasted using SES. The smooting paramaters of each may differ, like in the optimized Croston‚Äôs method.\nReferences:\nTeunter, R. H., Syntetos, A. A., & Babai, M. Z. (2011). Intermittent demand: Linking forecasting to inventory obsolescence. European Journal of Operational Research, 214(3), 606-615.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalpha_d\nfloat\n\nSmoothing parameter for demand.\n\n\nalpha_p\nfloat\n\nSmoothing parameter for probability.\n\n\nalias\nstr\nTSB\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nTSB.forecast\n\n TSB.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient TSB predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTSB.fit\n\n TSB.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the TSB model.\nFit an TSB to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nTSB fitted model.\n\n\n\n\nsource\n\n\nTSB.predict\n\n TSB.predict (h:int, X:Optional[numpy.ndarray]=None,\n              level:Optional[List[int]]=None)\n\nPredict with fitted TSB.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTSB.predict_in_sample\n\n TSB.predict_in_sample ()\n\nAccess fitted TSB insample predictions.\n\n# TSB's usage example\n\nfrom statsforecast.models import TSB\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = TSB(alpha_d=0.5, alpha_p=0.5)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([439.256, 439.256, 439.256, 439.256], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#mstl",
    "href": "src/core/models.html#mstl",
    "title": "Models",
    "section": "MSTL",
    "text": "MSTL\n\nsource\n\nMSTL\n\n MSTL (season_length:Union[int,List[int]], trend_forecaster=AutoETS,\n       stl_kwargs:Optional[Dict]=None, alias:str='MSTL', prediction_interv\n       als:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nMSTL model.\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) decomposes the time series in multiple seasonalities using LOESS. Then forecasts the trend using a custom non-seaonal model and each seasonality using a SeasonalNaive model.\nReferences:\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). ‚ÄúMSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns‚Äù..\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\ntyping.Union[int, typing.List[int]]\n\nNumber of observations per unit of time. For multiple seasonalities use a list.\n\n\ntrend_forecaster\nAutoETS\nAutoETS\nStatsForecast model used to forecast the trend component.\n\n\nstl_kwargs\ntyping.Optional[typing.Dict]\nNone\nExtra arguments to pass to statsmodels.tsa.seasonal.STL.The period and seasonal arguments are reserved.\n\n\nalias\nstr\nMSTL\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nMSTL.fit\n\n MSTL.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the MSTL model.\nFit MSTL to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nMSTL fitted model.\n\n\n\n\nsource\n\n\nMSTL.predict\n\n MSTL.predict (h:int, X:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None)\n\nPredict with fitted MSTL.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nMSTL.predict_in_sample\n\n MSTL.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted MSTL insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nMSTL.forecast\n\n MSTL.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient MSTL predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nMSTL.forward\n\n MSTL.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted MSTL model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# MSTL's usage example\n\nfrom statsforecast.models import MSTL\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmstl_model = MSTL(season_length=[3, 12], trend_forecaster=AutoARIMA(prediction_intervals=ConformalIntervals(h=4, n_windows=2)))\nmstl_model = mstl_model.fit(y=ap)\ny_hat_dict = mstl_model.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([449.64812085, 411.68927237, 467.03003869, 472.74771836]),\n 'lo-80': array([439.9824947 , 400.44724213, 462.79116282, 463.15713004]),\n 'hi-80': array([459.31374699, 422.93130261, 471.26891456, 482.33830668])}"
  },
  {
    "objectID": "src/core/models.html#standard-theta-method",
    "href": "src/core/models.html#standard-theta-method",
    "title": "Models",
    "section": "Standard Theta Method",
    "text": "Standard Theta Method\n\nsource\n\nTheta\n\n Theta (season_length:int=1, decomposition_type:str='multiplicative',\n        alias:str='Theta', prediction_intervals:Optional[statsforecast.uti\n        ls.ConformalIntervals]=None)\n\nStandard Theta Method.\nReferences:\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\n\n\nalias\nstr\nTheta\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nTheta.forecast\n\n Theta.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.fit\n\n Theta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nTheta.predict\n\n Theta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.predict_in_sample\n\n Theta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.forward\n\n Theta.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Theta's usage example\n\nfrom statsforecast.models import Theta\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Theta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.96918204, 429.24926382, 490.69323393, 476.65998055])}"
  },
  {
    "objectID": "src/core/models.html#optimized-theta-method",
    "href": "src/core/models.html#optimized-theta-method",
    "title": "Models",
    "section": "Optimized Theta Method",
    "text": "Optimized Theta Method\n\nsource\n\nOptimizedTheta\n\n OptimizedTheta (season_length:int=1,\n                 decomposition_type:str='multiplicative',\n                 alias:str='OptimizedTheta', prediction_intervals:Optional\n                 [statsforecast.utils.ConformalIntervals]=None)\n\nOptimized Theta Method.\nReferences:\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\n\n\nalias\nstr\nOptimizedTheta\nCustom name of the model. Default OptimizedTheta.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nOptimizedTheta.forecast\n\n OptimizedTheta.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.fit\n\n OptimizedTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nOptimizedTheta.predict\n\n OptimizedTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.predict_in_sample\n\n OptimizedTheta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.forward\n\n OptimizedTheta.forward (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# OptimzedThetA's usage example\n\nfrom statsforecast.models import OptimizedTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = OptimizedTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([442.94078295, 432.22936898, 495.30609727, 482.30625563])}"
  },
  {
    "objectID": "src/core/models.html#dynamic-standard-theta-method",
    "href": "src/core/models.html#dynamic-standard-theta-method",
    "title": "Models",
    "section": "Dynamic Standard Theta Method",
    "text": "Dynamic Standard Theta Method\n\nsource\n\nDynamicTheta\n\n DynamicTheta (season_length:int=1,\n               decomposition_type:str='multiplicative',\n               alias:str='DynamicTheta', prediction_intervals:Optional[sta\n               tsforecast.utils.ConformalIntervals]=None)\n\nDynamic Standard Theta Method.\nReferences:\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\n\n\nalias\nstr\nDynamicTheta\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nDynamicTheta.forecast\n\n DynamicTheta.forecast (y:numpy.ndarray, h:int,\n                        X:Optional[numpy.ndarray]=None,\n                        X_future:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.fit\n\n DynamicTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nDynamicTheta.predict\n\n DynamicTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.predict_in_sample\n\n DynamicTheta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.forward\n\n DynamicTheta.forward (y:numpy.ndarray, h:int,\n                       X:Optional[numpy.ndarray]=None,\n                       X_future:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# DynStandardThetaMethod's usage example\n\nfrom statsforecast.models import DynamicTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = DynamicTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.77412474, 429.06190332, 490.48332496, 476.46133269])}"
  },
  {
    "objectID": "src/core/models.html#dynamic-optimized-theta-method",
    "href": "src/core/models.html#dynamic-optimized-theta-method",
    "title": "Models",
    "section": "Dynamic Optimized Theta Method",
    "text": "Dynamic Optimized Theta Method\n\nsource\n\nDynamicOptimizedTheta\n\n DynamicOptimizedTheta (season_length:int=1,\n                        decomposition_type:str='multiplicative',\n                        alias:str='DynamicOptimizedTheta', prediction_inte\n                        rvals:Optional[statsforecast.utils.ConformalInterv\n                        als]=None)\n\nDynamic Optimized Theta Method.\nReferences:\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\n\n\nalias\nstr\nDynamicOptimizedTheta\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.forecast\n\n DynamicOptimizedTheta.forecast (y:numpy.ndarray, h:int,\n                                 X:Optional[numpy.ndarray]=None,\n                                 X_future:Optional[numpy.ndarray]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.fit\n\n DynamicOptimizedTheta.fit (y:numpy.ndarray,\n                            X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.predict\n\n DynamicOptimizedTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                                level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.predict_in_sample\n\n DynamicOptimizedTheta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.forward\n\n DynamicOptimizedTheta.forward (y:numpy.ndarray, h:int,\n                                X:Optional[numpy.ndarray]=None,\n                                X_future:Optional[numpy.ndarray]=None,\n                                level:Optional[List[int]]=None,\n                                fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# OptimzedThetaMethod's usage example\n\nfrom statsforecast.models import DynamicOptimizedTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = DynamicOptimizedTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([442.94256075, 432.31255941, 495.49774527, 482.58930649])}"
  },
  {
    "objectID": "src/core/models.html#garch-model",
    "href": "src/core/models.html#garch-model",
    "title": "Models",
    "section": "Garch model",
    "text": "Garch model\n\nsource\n\nGARCH\n\n GARCH (p:int=1, q:int=1, alias:str='GARCH', prediction_intervals:Optional\n        [statsforecast.utils.ConformalIntervals]=None)\n\nGeneralized Autoregressive Conditional Heteroskedasticity (GARCH) model.\nA method for modeling time series that exhibit non-constant volatility over time. The GARCH model assumes that at time \\(t\\), \\(y_t\\) is given by:\n\\[ y_t = v_t \\sigma_t\\]\nwith\n\\[ \\sigma_t^2 = w + \\sum_{i=1}^p a_i y_{t-i}^2 + \\sum_{j=1}^q b_j \\sigma_{t-j}^2\\].\nHere {\\(v_t\\)} is a sequence of iid random variables with zero mean and unit variance. The coefficients \\(w\\), \\(a_i\\), \\(i=1,...,p\\), and \\(b_j\\), \\(j=1,...,q\\) must satisfy the following conditions:\n\n\\(w &gt; 0\\) and \\(a_i, b_j \\geq 0\\) for all \\(i\\) and \\(j\\).\n\\(\\sum_{k=1}^{max(p,q)} a_k + b_k &lt; 1\\). Here it is assumed that \\(a_i=0\\) for \\(i&gt;p\\) and \\(b_j=0\\) for \\(j&gt;q\\).\n\nThe ARCH model is a particular case of the GARCH model when \\(q=0\\).\nReferences:\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.\nBollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of econometrics, 31(3), 307-327.\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nint\n1\nNumber of lagged versions of the series.\n\n\nq\nint\n1\n\n\n\nalias\nstr\nGARCH\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nGARCH.fit\n\n GARCH.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit GARCH model.\nFit GARCH model to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nGARCH model.\n\n\n\n\nsource\n\n\nGARCH.predict\n\n GARCH.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted GARCH model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nGARCH.predict_in_sample\n\n GARCH.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted GARCH model predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nGARCH.forecast\n\n GARCH.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient GARCH model.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions."
  },
  {
    "objectID": "src/core/models.html#arch-model",
    "href": "src/core/models.html#arch-model",
    "title": "Models",
    "section": "ARCH model",
    "text": "ARCH model\n\nsource\n\nARCH\n\n ARCH (p:int=1, alias:str='ARCH', prediction_intervals:Optional[statsforec\n       ast.utils.ConformalIntervals]=None)\n\nAutoregressive Conditional Heteroskedasticity (ARCH) model.\nA particular case of the GARCH(p,q) model where \\(q=0\\). It assumes that at time \\(t\\), \\(y_t\\) is given by:\n\\[ y_t = \\epsilon_t \\sigma_t\\]\nwith\n\\[ \\sigma_t^2 = w0 + \\sum_{i=1}^p a_i y_{t-i}^2\\].\nHere \\(\\epsilon_t\\) is a sequence of iid random variables with zero mean and unit variance. The coefficients \\(w\\) and \\(a_i\\), \\(i=1,...,p\\) must be nonnegative and \\(\\sum_{k=1}^p a_k &lt; 1\\).\nReferences:\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nint\n1\nNumber of lagged versions of the series.\n\n\nalias\nstr\nARCH\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nARCH.fit\n\n ARCH.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit GARCH model.\nFit GARCH model to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nGARCH model.\n\n\n\n\nsource\n\n\nARCH.predict\n\n ARCH.predict (h:int, X:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None)\n\nPredict with fitted GARCH model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARCH.predict_in_sample\n\n ARCH.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted GARCH model predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARCH.forecast\n\n ARCH.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient GARCH model.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions."
  },
  {
    "objectID": "src/core/models.html#constantmodel",
    "href": "src/core/models.html#constantmodel",
    "title": "Models",
    "section": "ConstantModel",
    "text": "ConstantModel\n\nsource\n\nConstantModel\n\n ConstantModel (constant:float, alias:str='ConstantModel')\n\nConstant Model.\nReturns Constant values.\n\nsource\n\n\nConstantModel.forecast\n\n ConstantModel.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nMemory Efficient Constant Model predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nConstantModel.fit\n\n ConstantModel.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Constant model.\nFit an Constant Model to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nConstant fitted model.\n\n\n\n\nsource\n\n\nConstantModel.predict\n\n ConstantModel.predict (h:int, X:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None)\n\nPredict with fitted ConstantModel.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nConstantModel.predict_in_sample\n\n ConstantModel.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Constant Model insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nConstantModel.forward\n\n ConstantModel.forward (y:numpy.ndarray, h:int,\n                        X:Optional[numpy.ndarray]=None,\n                        X_future:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None, fitted:bool=False)\n\nApply Constant model predictions to a new/updated time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries constant for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# ConstantModel's usage example\n\nfrom statsforecast.models import ConstantModel\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = ConstantModel(1)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([1., 1., 1., 1.], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#zeromodel",
    "href": "src/core/models.html#zeromodel",
    "title": "Models",
    "section": "ZeroModel",
    "text": "ZeroModel\n\nsource\n\nZeroModel\n\n ZeroModel (alias:str='ZeroModel')\n\nReturns Zero forecasts.\nReturns Zero values.\n\nsource\n\n\nZeroModel.forecast\n\n ZeroModel.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Constant Model predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nZeroModel.fit\n\n ZeroModel.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Constant model.\nFit an Constant Model to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nConstant fitted model.\n\n\n\n\nsource\n\n\nZeroModel.predict\n\n ZeroModel.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None)\n\nPredict with fitted ConstantModel.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nZeroModel.predict_in_sample\n\n ZeroModel.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Constant Model insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nZeroModel.forward\n\n ZeroModel.forward (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nApply Constant model predictions to a new/updated time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries constant for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# NanModel's usage example\n\nfrom statsforecast.models import ZeroModel\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = ZeroModel()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([0., 0., 0., 0.], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#nanmodel",
    "href": "src/core/models.html#nanmodel",
    "title": "Models",
    "section": "NaNModel",
    "text": "NaNModel\n\nsource\n\nNaNModel\n\n NaNModel (alias:str='NaNModel')\n\nNaN Model.\nReturns NaN values.\n\nsource\n\n\nNaNModel.forecast\n\n NaNModel.forecast (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Constant Model predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaNModel.fit\n\n NaNModel.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Constant model.\nFit an Constant Model to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nConstant fitted model.\n\n\n\n\nsource\n\n\nNaNModel.predict\n\n NaNModel.predict (h:int, X:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None)\n\nPredict with fitted ConstantModel.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaNModel.predict_in_sample\n\n NaNModel.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Constant Model insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# NanModel's usage example\n\nfrom statsforecast.models import NaNModel\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = NaNModel()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([nan, nan, nan, nan], dtype=float32)}"
  },
  {
    "objectID": "src/adapters.prophet.html",
    "href": "src/adapters.prophet.html",
    "title": "Replace FB-Prophet",
    "section": "",
    "text": "source\n\n\n\n AutoARIMAProphet (growth='linear', changepoints=None, n_changepoints=25,\n                   changepoint_range=0.8, yearly_seasonality='auto',\n                   weekly_seasonality='auto', daily_seasonality='auto',\n                   holidays=None, seasonality_mode='additive',\n                   seasonality_prior_scale=10.0,\n                   holidays_prior_scale=10.0,\n                   changepoint_prior_scale=0.05, mcmc_samples=0,\n                   interval_width=0.8, uncertainty_samples=1000,\n                   stan_backend=None, d=None, D=None, max_p=5, max_q=5,\n                   max_P=2, max_Q=2, max_order=5, max_d=2, max_D=1,\n                   start_p=2, start_q=2, start_P=1, start_Q=1,\n                   stationary=False, seasonal=True, ic='aicc',\n                   stepwise=True, nmodels=94, trace=False,\n                   approximation=False, method=None, truncate=None,\n                   test='kpss', test_kwargs=None, seasonal_test='seas',\n                   seasonal_test_kwargs=None, allowdrift=False,\n                   allowmean=False, blambda=None, biasadj=False, period=1)\n\nAutoARIMAProphet adapter.\nReturns best ARIMA model using external variables created by the Prophet interface. This class receives as parameters the same as prophet.Prophet and uses a models.AutoARIMA backend.\nIf your forecasting pipeline uses Prophet the AutoARIMAProphet adapter helps to easily substitute Prophet with an AutoARIMA.\nParameters: growth: String ‚Äòlinear‚Äô, ‚Äòlogistic‚Äô or ‚Äòflat‚Äô to specify a linear, logistic or flat trend. changepoints: List of dates of potential changepoints. Otherwise selected automatically. n_changepoints: Number of potential changepoints to include. changepoint_range: Proportion of history in which trend changepoints will be estimated. yearly_seasonality: Fit yearly seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. weekly_seasonality: Fit weekly seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. daily_seasonality: Fit daily seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. holidays: pandas.DataFrame with columns holiday (string) and ds (date type). interval_width: float, uncertainty forecast intervals width. StatsForecast‚Äôs level \nNotes: You can create automated exogenous variables from the Prophet data processing pipeline these exogenous will be included into AutoARIMA‚Äôs exogenous features. Parameters like seasonality_mode, seasonality_prior_scale, holidays_prior_scale, changepoint_prior_scale, mcmc_samples, uncertainty_samples, stan_backend are Prophet exclusive.\nReferences: Sean J. Taylor, Benjamin Letham (2017). ‚ÄúProphet Forecasting at Scale‚Äù\nOskar Triebe, Hansika Hewamalage, Polina Pilyugina, Nikolay Laptev, Christoph Bergmeir, Ram Rajagopal (2021). ‚ÄúNeuralProphet: Explainable Forecasting at Scale‚Äù.\nRob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\n\nsource\n\n\n\n\n AutoARIMAProphet.fit (df, disable_seasonal_features=True, **kwargs)\n\nFit the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series. disable_seasonal_features: bool, Wheter disable Prophet‚Äôs seasonal features. kwargs: Additional arguments.\nReturns: self: AutoARIMAProphet adapter object with AutoARIMA fitted model.\n\nsource\n\n\n\n\n AutoARIMAProphet.predict (df=None)\n\nPredict using the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series.\nReturns: fcsts_df: A pandas.DataFrame with the forecast components.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/adapters.prophet.html#autoarimaprophet",
    "href": "src/adapters.prophet.html#autoarimaprophet",
    "title": "Replace FB-Prophet",
    "section": "",
    "text": "source\n\n\n\n AutoARIMAProphet (growth='linear', changepoints=None, n_changepoints=25,\n                   changepoint_range=0.8, yearly_seasonality='auto',\n                   weekly_seasonality='auto', daily_seasonality='auto',\n                   holidays=None, seasonality_mode='additive',\n                   seasonality_prior_scale=10.0,\n                   holidays_prior_scale=10.0,\n                   changepoint_prior_scale=0.05, mcmc_samples=0,\n                   interval_width=0.8, uncertainty_samples=1000,\n                   stan_backend=None, d=None, D=None, max_p=5, max_q=5,\n                   max_P=2, max_Q=2, max_order=5, max_d=2, max_D=1,\n                   start_p=2, start_q=2, start_P=1, start_Q=1,\n                   stationary=False, seasonal=True, ic='aicc',\n                   stepwise=True, nmodels=94, trace=False,\n                   approximation=False, method=None, truncate=None,\n                   test='kpss', test_kwargs=None, seasonal_test='seas',\n                   seasonal_test_kwargs=None, allowdrift=False,\n                   allowmean=False, blambda=None, biasadj=False, period=1)\n\nAutoARIMAProphet adapter.\nReturns best ARIMA model using external variables created by the Prophet interface. This class receives as parameters the same as prophet.Prophet and uses a models.AutoARIMA backend.\nIf your forecasting pipeline uses Prophet the AutoARIMAProphet adapter helps to easily substitute Prophet with an AutoARIMA.\nParameters: growth: String ‚Äòlinear‚Äô, ‚Äòlogistic‚Äô or ‚Äòflat‚Äô to specify a linear, logistic or flat trend. changepoints: List of dates of potential changepoints. Otherwise selected automatically. n_changepoints: Number of potential changepoints to include. changepoint_range: Proportion of history in which trend changepoints will be estimated. yearly_seasonality: Fit yearly seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. weekly_seasonality: Fit weekly seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. daily_seasonality: Fit daily seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. holidays: pandas.DataFrame with columns holiday (string) and ds (date type). interval_width: float, uncertainty forecast intervals width. StatsForecast‚Äôs level \nNotes: You can create automated exogenous variables from the Prophet data processing pipeline these exogenous will be included into AutoARIMA‚Äôs exogenous features. Parameters like seasonality_mode, seasonality_prior_scale, holidays_prior_scale, changepoint_prior_scale, mcmc_samples, uncertainty_samples, stan_backend are Prophet exclusive.\nReferences: Sean J. Taylor, Benjamin Letham (2017). ‚ÄúProphet Forecasting at Scale‚Äù\nOskar Triebe, Hansika Hewamalage, Polina Pilyugina, Nikolay Laptev, Christoph Bergmeir, Ram Rajagopal (2021). ‚ÄúNeuralProphet: Explainable Forecasting at Scale‚Äù.\nRob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\n\nsource\n\n\n\n\n AutoARIMAProphet.fit (df, disable_seasonal_features=True, **kwargs)\n\nFit the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series. disable_seasonal_features: bool, Wheter disable Prophet‚Äôs seasonal features. kwargs: Additional arguments.\nReturns: self: AutoARIMAProphet adapter object with AutoARIMA fitted model.\n\nsource\n\n\n\n\n AutoARIMAProphet.predict (df=None)\n\nPredict using the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series.\nReturns: fcsts_df: A pandas.DataFrame with the forecast components."
  },
  {
    "objectID": "src/adapters.prophet.html#univariate-prophet",
    "href": "src/adapters.prophet.html#univariate-prophet",
    "title": "Replace FB-Prophet",
    "section": "2.1 Univariate Prophet",
    "text": "2.1 Univariate Prophet\nHere we forecast with Prophet without external regressors. We first instantiate a new Prophet object, and define its forecasting procedure into its constructor. After that a classic sklearn fit and predict is used to obtain the predictions.\n\nm = Prophet(daily_seasonality=False)\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nHere we forecast with AutoARIMAProphet adapter without external regressors. It inherits the Prophet constructor as well as its fit and predict methods.\nWith the class AutoARIMAProphet you can simply substitute Prophet and you‚Äôll be training an AutoARIMA model without changing anything in your forecasting pipeline.\n\nm = AutoARIMAProphet(daily_seasonality=False)\nm.fit(df)\n# m.fit(df, disable_seasonal_features=False) # Uncomment for better AutoARIMA predictions\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  },
  {
    "objectID": "src/adapters.prophet.html#holiday-prophet",
    "href": "src/adapters.prophet.html#holiday-prophet",
    "title": "Replace FB-Prophet",
    "section": "2.2 Holiday Prophet",
    "text": "2.2 Holiday Prophet\nUsually Prophet pipelines include the usage of external regressors such as holidays.\nSuppose you want to include holidays or other recurring calendar events, you can create a pandas.DataFrame for them. The DataFrame needs two columns [holiday, ds] and a row for each holiday. It requires all the occurrences of the holiday (as far as the historical data allows) and the future events of the holiday. If the future does not have the holidays registered, they will be modeled but not included in the forecast.\nYou can also include into the events DataFrame, lower_window and upper_window that extends the effect of the holidays through dates to [lower_window, upper_window] days around the date. For example if you wanted to account for Christmas Eve in addition to Christmas you‚Äôd include lower_window=-1,upper_window=0, or Black Friday in addition to Thanksgiving, you‚Äôd include lower_window=0,upper_window=1.\nHere we Peyton Manning‚Äôs playoff appearances dates:\n\nplayoffs = pd.DataFrame({\n  'holiday': 'playoff',\n  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n                        '2010-01-24', '2010-02-07', '2011-01-08',\n                        '2013-01-12', '2014-01-12', '2014-01-19',\n                        '2014-02-02', '2015-01-11', '2016-01-17',\n                        '2016-01-24', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nsuperbowls = pd.DataFrame({\n  'holiday': 'superbowl',\n  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nholidays = pd.concat((playoffs, superbowls))\n\n\nm = Prophet(daily_seasonality=False, holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nThe class AutoARIMAProphet adapter allows to handle these scenarios to fit an AutoARIMA model with exogenous variables.\nYou can enjoy your Prophet pipelines with the improved performance of a classic ARIMA.\n\nm = AutoARIMAProphet(daily_seasonality=False,\n                     holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\n# m.fit(df, disable_seasonal_features=False) # Uncomment for better AutoARIMA predictions\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  },
  {
    "objectID": "src/distributed.core.html",
    "href": "src/distributed.core.html",
    "title": "Core",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/utils.html",
    "href": "src/utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n generate_series (n_series:int, freq:str='D', min_length:int=50,\n                  max_length:int=500, n_static_features:int=0,\n                  equal_ends:bool=False, engine:str='pandas', seed:int=0)\n\nGenerate Synthetic Panel Series.\nGenerates n_series of frequency freq of different lengths in the interval [min_length, max_length]. If n_static_features &gt; 0, then each series gets static features with random values. If equal_ends == True then all series end at the same date.\nParameters: n_series: int, number of series for synthetic panel. min_length: int, minimal length of synthetic panel‚Äôs series. max_length: int, minimal length of synthetic panel‚Äôs series. n_static_features: int, default=0, number of static exogenous variables for synthetic panel‚Äôs series. equal_ends: bool, if True, series finish in the same date stamp ds. freq: str, frequency of the data, panda‚Äôs available frequencies. engine: str, engine to be used in DataFrame construction; NOTE: index does not exist in polars DataFrame\nReturns: freq: pandas.DataFrame | polars.DataFrame, synthetic panel with columns [unique_id, ds, y] and exogenous.\n\nsynthetic_panel = generate_series(n_series=2)\nsynthetic_panel.groupby('unique_id').head(4)\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/utils.html#model-utils",
    "href": "src/utils.html#model-utils",
    "title": "Utils",
    "section": "Model utils",
    "text": "Model utils"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "StatsForecast Blog",
    "section": "",
    "text": "Scalable Time Series Modeling with open-source projects\n\n\n\n\n\nHow to Forecast 1M Time Series in 15 Minutes with Spark, Fugue and Nixtla‚Äôs Statsforecast.\n\n\n\n\n\n\nOct 5, 2022\n\n\nFugue , Nixtla\n\n\n\n\n\n\nNo matching items\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/autoets.html",
    "href": "docs/models/autoets.html",
    "title": "AutoETS Model",
    "section": "",
    "text": "Introduction\nETS Models\nETS Estimation\nModel Selection\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoETS with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/autoets.html#table-of-contents",
    "href": "docs/models/autoets.html#table-of-contents",
    "title": "AutoETS Model",
    "section": "",
    "text": "Introduction\nETS Models\nETS Estimation\nModel Selection\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoETS with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/autoets.html#introduction",
    "href": "docs/models/autoets.html#introduction",
    "title": "AutoETS Model",
    "section": "Introduction ",
    "text": "Introduction \nAutomatic forecasts of large numbers of univariate time series are often needed in business. It is common to have over one thousand product lines that need forecasting at least monthly. Even when a smaller number of forecasts are required, there may be nobody suitably trained in the use of time series models to produce them. In these circumstances, an automatic forecasting algorithm is an essential tool. Automatic forecasting algorithms must determine an appropriate time series model, estimate the parameters and compute the forecasts. They must be robust to unusual time series patterns, and applicable to large numbers of series without user intervention. The most popular automatic forecasting algorithms are based on either exponential smoothing or ARIMA models."
  },
  {
    "objectID": "docs/models/autoets.html#estimating-ets-models",
    "href": "docs/models/autoets.html#estimating-ets-models",
    "title": "AutoETS Model",
    "section": "Estimating ETS models ",
    "text": "Estimating ETS models \nAn alternative to estimating the parameters by minimising the sum of squared errors is to maximise the ‚Äúlikelihood‚Äù. The likelihood is the probability of the data arising from the specified model. Thus, a large likelihood is associated with a good model. For an additive error model, maximising the likelihood (assuming normally distributed errors) gives the same results as minimising the sum of squared errors. However, different results will be obtained for multiplicative error models. In this section, we will estimate the smoothing parameters \\(\\alpha, \\beta, \\gamma\\) and \\(\\phi\\) and the initial states \\(\\ell_0, b_0, s_0,s_{-1},\\dots,s_{-m+1}\\), by maximising the likelihood.\nThe possible values that the smoothing parameters can take are restricted. Traditionally, the parameters have been constrained to lie between 0 and 1 so that the equations can be interpreted as weighted averages. That is, \\(0&lt; \\alpha,\\beta^*,\\gamma^*,\\phi&lt;1\\). For the state space models, we have set \\(\\beta=\\alpha\\beta^*\\) and \\(\\gamma=(1-\\alpha)\\gamma^*\\). Therefore, the traditional restrictions translate to \\(0&lt; \\alpha &lt;1, 0 &lt; \\beta &lt; \\alpha\\) and \\(0&lt; \\gamma &lt; 1-\\alpha\\). In practice, the damping parameter \\(\\phi\\) is usually constrained further to prevent numerical difficulties in estimating the model.\nAnother way to view the parameters is through a consideration of the mathematical properties of the state space models. The parameters are constrained in order to prevent observations in the distant past having a continuing effect on current forecasts. This leads to some admissibility constraints on the parameters, which are usually (but not always) less restrictive than the traditional constraints region (Hyndman et al., 2008, pp. 149‚Äì161). For example, for the ETS(A,N,N) model, the traditional parameter region is \\(0&lt; \\alpha &lt;1\\) but the admissible region is \\(0&lt; \\alpha &lt;2\\). For the ETS(A,A,N) model, the traditional parameter region is \\(0&lt;\\alpha&lt;1\\) and \\(0&lt;\\beta&lt;\\alpha\\) but the admissible region is \\(0&lt;\\alpha&lt;2\\) and \\(0&lt;\\beta&lt;4-2\\alpha\\)."
  },
  {
    "objectID": "docs/models/autoets.html#model-selection",
    "href": "docs/models/autoets.html#model-selection",
    "title": "AutoETS Model",
    "section": "Model selection ",
    "text": "Model selection \nA great advantage of the ETS statistical framework is that information criteria can be used for model selection. The AIC, AIC_c and BIC, can be used here to determine which of the ETS models is most appropriate for a given time series.\nFor ETS models, Akaike‚Äôs Information Criterion (AIC) is defined as\n\\[\\text{AIC} = -2\\log(L) + 2k,\\]\nwhere \\(L\\) is the likelihood of the model and \\(k\\) is the total number of parameters and initial states that have been estimated (including the residual variance).\nThe AIC corrected for small sample bias (AIC_c) is defined as\n\\[AIC_c = AIC + \\frac{2k(k+1)}{T-k-1}\\]\nand the Bayesian Information Criterion (BIC) is\n\\[\\text{BIC} = \\text{AIC} + k[\\log(T)-2]\\]\nThree of the combinations of (Error, Trend, Seasonal) can lead to numerical difficulties. Specifically, the models that can cause such instabilities are ETS(A,N,M), ETS(A,A,M), and ETS(A,Ad,M), due to division by values potentially close to zero in the state equations. We normally do not consider these particular combinations when selecting a model.\nModels with multiplicative errors are useful when the data are strictly positive, but are not numerically stable when the data contain zeros or negative values. Therefore, multiplicative error models will not be considered if the time series is not strictly positive. In that case, only the six fully additive models will be applied."
  },
  {
    "objectID": "docs/models/autoets.html#loading-libraries-and-data",
    "href": "docs/models/autoets.html#loading-libraries-and-data",
    "title": "AutoETS Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/Esperanza_vida.csv\", usecols=[1,2])\ndf.head()\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n1960-01-01\n69.123902\n\n\n1\n1961-01-01\n69.760244\n\n\n2\n1962-01-01\n69.149756\n\n\n3\n1963-01-01\n69.248049\n\n\n4\n1964-01-01\n70.311707\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1960-01-01\n69.123902\n1\n\n\n1\n1961-01-01\n69.760244\n1\n\n\n2\n1962-01-01\n69.149756\n1\n\n\n3\n1963-01-01\n69.248049\n1\n\n\n4\n1964-01-01\n70.311707\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds            object\ny            float64\nunique_id     object\ndtype: object\n\n\nWe need to convert the ds from object type to datetime.\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/autoets.html#explore-data-with-the-plot-method",
    "href": "docs/models/autoets.html#explore-data-with-the-plot-method",
    "title": "AutoETS Model",
    "section": "Explore data with the plot method ",
    "text": "Explore data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)"
  },
  {
    "objectID": "docs/models/autoets.html#autocorrelation-plots",
    "href": "docs/models/autoets.html#autocorrelation-plots",
    "title": "AutoETS Model",
    "section": "Autocorrelation plots",
    "text": "Autocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=20, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\n# Plot\nplot_pacf(df[\"y\"],  lags=20, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"add\", period=1)\na.plot();\n\n\n\n\nBreaking down a time series into its components helps us to identify the behavior of the time series we are analyzing. In addition, it helps us to know what type of models we can apply, for our example of the Life expectancy data set, we can observe that our time series shows an increasing trend throughout the year, on the other hand, it can be observed also that the time series has no seasonality.\nBy looking at the previous graph and knowing each of the components, we can get an idea of which model we can apply: * We have trend * There is no seasonality"
  },
  {
    "objectID": "docs/models/autoets.html#split-the-data-into-training-and-testing",
    "href": "docs/models/autoets.html#split-the-data-into-training-and-testing",
    "title": "AutoETS Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our model.\nData to test our model.\n\nFor the test data we will use the last 6 years to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2013-01-01'] \ntest = df[df.ds&gt;'2013-01-01']\n\n\ntrain.shape, test.shape\n\n((54, 3), (6, 3))\n\n\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.show()"
  },
  {
    "objectID": "docs/models/autoets.html#implementation-of-autoets-with-statsforecast",
    "href": "docs/models/autoets.html#implementation-of-autoets-with-statsforecast",
    "title": "AutoETS Model",
    "section": "Implementation of AutoETS with StatsForecast",
    "text": "Implementation of AutoETS with StatsForecast\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: \\(E\\) in \\([M,A,Z]\\), \\(T\\) in \\([N,A,M,Z]\\), and \\(S\\) in \\([N,A,M,Z]\\).\nFor example when model=‚ÄòANN‚Äô (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‚ÄòZ‚Äô, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nTo also know more about the parameters of the functions of the AutoETS Model, they are listed below. For more information, visit the documentation\nmodel : str\n    Controlling state-space-equations.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\ndamped : bool\n    A parameter that 'dampens' the trend.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals],\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nfrom statsforecast.models import AutoETS\n\n\nInstantiate Model\n\nautoets = AutoETS(model=[\"A\",\"Z\",\"N\"],  alias=\"AutoETS\", season_length=1)\n\n\n\nFit the Model\n\nautoets = autoets.fit(df[\"y\"].values)\nautoets\n\nAutoETS\n\n\n\n\nModel Prediction\n\ny_hat_dict = autoets.predict(h=6)\ny_hat_dict\n\n{'mean': array([83.56937105, 83.65696041, 83.74454977, 83.83213913, 83.91972848,\n        84.00731784])}\n\n\nYou can see that the result that has been extracted in the predictions or in any other method that we use from now on with the Arima model is a dictionary. To extract that result we can use the .get() function, which will help us to be able to extract the result of each part of the dictionary of each of the methods that we use.\n\nforecast=pd.Series(pd.date_range(\"2014-01-01\", freq=\"ys\", periods=6))\nforecast=pd.DataFrame(forecast)\nforecast.columns=[\"ds\"]\nforecast[\"hat\"]=y_hat_dict.get(\"mean\")\nforecast[\"unique_id\"]=\"1\"\nforecast\n\n\n\n\n\n\n\n\nds\nhat\nunique_id\n\n\n\n\n0\n2014-01-01\n83.569371\n1\n\n\n1\n2015-01-01\n83.656960\n1\n\n\n2\n2016-01-01\n83.744550\n1\n\n\n3\n2017-01-01\n83.832139\n1\n\n\n4\n2018-01-01\n83.919728\n1\n\n\n5\n2019-01-01\n84.007318\n1\n\n\n\n\n\n\n\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nsns.lineplot(forecast,x=\"ds\", y=\"hat\", label=\"Forecast\",)\nplt.show()\n\n\n\n\nLet‚Äôs add a confidence interval to our forecast.\n\ny_hat_dict = autoets.predict(h=6, level=[80,90,95])\ny_hat_dict\n\n{'mean': array([83.56937105, 83.65696041, 83.74454977, 83.83213913, 83.91972848,\n        84.00731784]),\n 'lo-95': array([83.09409059, 83.17958519, 83.25889648, 83.32836493, 83.3852606 ,\n        83.42814393]),\n 'lo-90': array([83.17050311, 83.2563345 , 83.33697668, 83.40935849, 83.4711889 ,\n        83.52125977]),\n 'lo-80': array([83.25860186, 83.34482153, 83.42699815, 83.50273888, 83.57025872,\n        83.62861638]),\n 'hi-80': array([83.88014025, 83.96909929, 84.06210139, 84.16153937, 84.26919825,\n        84.38601931]),\n 'hi-90': array([83.96823899, 84.05758633, 84.15212286, 84.25491976, 84.36826807,\n        84.49337591]),\n 'hi-95': array([84.04465152, 84.13433563, 84.23020306, 84.33591332, 84.45419637,\n        84.58649176])}\n\n\n\nforecast[\"hat\"]=y_hat_dict.get(\"mean\")\n\nforecast[\"lo-80\"]=y_hat_dict.get(\"lo-80\")\nforecast[\"hi-80\"]=y_hat_dict.get(\"hi-80\")\n\nforecast[\"lo-90\"]=y_hat_dict.get(\"lo-80\")\nforecast[\"hi-90\"]=y_hat_dict.get(\"hi-80\")\n\nforecast[\"lo-95\"]=y_hat_dict.get(\"lo-95\")\nforecast[\"hi-95\"]=y_hat_dict.get(\"hi-95\")\nforecast\n\n\n\n\n\n\n\n\nds\nhat\nunique_id\nlo-80\nhi-80\nlo-90\nhi-90\nlo-95\nhi-95\n\n\n\n\n0\n2014-01-01\n83.569371\n1\n83.258602\n83.880140\n83.258602\n83.880140\n83.094091\n84.044652\n\n\n1\n2015-01-01\n83.656960\n1\n83.344822\n83.969099\n83.344822\n83.969099\n83.179585\n84.134336\n\n\n2\n2016-01-01\n83.744550\n1\n83.426998\n84.062101\n83.426998\n84.062101\n83.258896\n84.230203\n\n\n3\n2017-01-01\n83.832139\n1\n83.502739\n84.161539\n83.502739\n84.161539\n83.328365\n84.335913\n\n\n4\n2018-01-01\n83.919728\n1\n83.570259\n84.269198\n83.570259\n84.269198\n83.385261\n84.454196\n\n\n5\n2019-01-01\n84.007318\n1\n83.628616\n84.386019\n83.628616\n84.386019\n83.428144\n84.586492\n\n\n\n\n\n\n\n\ndf=df.set_index(\"ds\")\nforecast=forecast.set_index(\"ds\")\n\n\ndf['unique_id'] = df['unique_id'].astype(object)\ndf_plot=df.merge(forecast, how='left', on=['unique_id', 'ds'])\n\n\nfig, ax = plt.subplots()\nplt.plot_date(df_plot.index, df_plot[\"y\"],label=\"Actual\", linestyle=\"-\")\nplt.plot_date(df_plot.index, df_plot[\"hat\"],label=\"Forecas\", linestyle=\"-\")\nax.fill_between(df_plot.index, \n                df_plot['lo-80'], \n                df_plot['hi-80'],\n                alpha=.35,\n                color='orange',\n                label='AutoETS-level-95')\nax.set_title('', fontsize=22)\nax.set_ylabel('', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=12)\nplt.legend(fontsize=12)\nax.grid(True)\n\nplt.show()\n\n\n\n\n\n\nAutoETS.predict_in_sample method\nAccess fitted Exponential Smoothing insample predictions.\n\nautoets.predict_in_sample()\n\n{'fitted': array([69.11047128, 69.33779313, 69.60481978, 69.82902341, 69.99867608,\n        70.19782129, 70.39447403, 70.6410976 , 70.91730954, 71.18057737,\n        71.40921444, 71.65195408, 71.90923238, 72.18210689, 72.44032072,\n        72.72619326, 73.00461648, 73.28185885, 73.56688287, 73.86376672,\n        74.17369207, 74.4619337 , 74.74004938, 75.02518845, 75.27413727,\n        75.53397698, 75.78785822, 76.0401374 , 76.30927822, 76.58417332,\n        76.88118063, 77.18657633, 77.47625869, 77.76062769, 78.04136835,\n        78.31088961, 78.56725183, 78.81937322, 79.07197167, 79.31551239,\n        79.56929831, 79.84269161, 80.14276586, 80.45093604, 80.71510722,\n        80.98548038, 81.23680745, 81.49249391, 81.74269063, 81.96870832,\n        82.16354077, 82.34648092, 82.51452241, 82.65668891, 82.80204272,\n        82.97448067, 83.1064133 , 83.2513209 , 83.36754658, 83.4818162 ])}\n\n\n\n\nAutoETS.forecast method\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\nautoets.forecast(y=train[\"y\"].values, h=6, fitted=True)\n\n{'mean': array([82.95334067, 83.14710962, 83.34087858, 83.53464753, 83.72841648,\n        83.92218543]),\n 'fitted': array([69.00631079, 69.23838078, 69.49672275, 69.73753749, 69.95373373,\n        70.18800832, 70.42142612, 70.68026343, 70.95296729, 71.21693197,\n        71.46051702, 71.70909162, 71.96257898, 72.22173709, 72.47104279,\n        72.73363157, 72.99184677, 73.25007587, 73.5140747 , 73.78708229,\n        74.07093074, 74.34832297, 74.62600899, 74.91319459, 75.18661412,\n        75.47027994, 75.75394823, 76.03846177, 76.33209228, 76.62765077,\n        76.93286853, 77.2399741 , 77.53597227, 77.82612695, 78.11104644,\n        78.38645253, 78.6510127 , 78.90909424, 79.16292255, 79.40732528,\n        79.65260623, 79.90420341, 80.16700064, 80.43291173, 80.67615301,\n        80.92469414, 81.16608469, 81.41337419, 81.66169821, 81.90113913,\n        82.12727338, 82.34886657, 82.56235691, 82.75957866])}\n\n\n\nautoets.forecast(y=train[\"y\"].values, h=6, fitted=True, level=[95])\n\n{'mean': array([82.95334067, 83.14710962, 83.34087858, 83.53464753, 83.72841648,\n        83.92218543]),\n 'fitted': array([69.00631079, 69.23838078, 69.49672275, 69.73753749, 69.95373373,\n        70.18800832, 70.42142612, 70.68026343, 70.95296729, 71.21693197,\n        71.46051702, 71.70909162, 71.96257898, 72.22173709, 72.47104279,\n        72.73363157, 72.99184677, 73.25007587, 73.5140747 , 73.78708229,\n        74.07093074, 74.34832297, 74.62600899, 74.91319459, 75.18661412,\n        75.47027994, 75.75394823, 76.03846177, 76.33209228, 76.62765077,\n        76.93286853, 77.2399741 , 77.53597227, 77.82612695, 78.11104644,\n        78.38645253, 78.6510127 , 78.90909424, 79.16292255, 79.40732528,\n        79.65260623, 79.90420341, 80.16700064, 80.43291173, 80.67615301,\n        80.92469414, 81.16608469, 81.41337419, 81.66169821, 81.90113913,\n        82.12727338, 82.34886657, 82.56235691, 82.75957866]),\n 'lo-95': array([82.50120336, 82.69439921, 82.88588753, 83.07456973, 83.2594347 ,\n        83.43962264]),\n 'hi-95': array([83.40547799, 83.59982004, 83.79586962, 83.99472532, 84.19739825,\n        84.40474821]),\n 'fitted-lo-95': array([68.5588109 , 68.79088089, 69.04922287, 69.2900376 , 69.50623384,\n        69.74050844, 69.97392623, 70.23276354, 70.5054674 , 70.76943209,\n        71.01301713, 71.26159173, 71.51507909, 71.7742372 , 72.0235429 ,\n        72.28613168, 72.54434688, 72.80257598, 73.06657481, 73.3395824 ,\n        73.62343085, 73.90082308, 74.1785091 , 74.4656947 , 74.73911423,\n        75.02278005, 75.30644834, 75.59096188, 75.88459239, 76.18015088,\n        76.48536864, 76.79247421, 77.08847238, 77.37862706, 77.66354655,\n        77.93895264, 78.20351282, 78.46159435, 78.71542266, 78.9598254 ,\n        79.20510634, 79.45670352, 79.71950075, 79.98541184, 80.22865312,\n        80.47719425, 80.7185848 , 80.96587431, 81.21419832, 81.45363924,\n        81.67977349, 81.90136668, 82.11485702, 82.31207877]),\n 'fitted-hi-95': array([69.45381068, 69.68588067, 69.94422264, 70.18503738, 70.40123361,\n        70.63550821, 70.86892601, 71.12776332, 71.40046717, 71.66443186,\n        71.90801691, 72.15659151, 72.41007887, 72.66923698, 72.91854267,\n        73.18113146, 73.43934666, 73.69757575, 73.96157459, 74.23458218,\n        74.51843063, 74.79582286, 75.07350887, 75.36069448, 75.634114  ,\n        75.91777983, 76.20144811, 76.48596166, 76.77959217, 77.07515066,\n        77.38036842, 77.68747399, 77.98347215, 78.27362683, 78.55854633,\n        78.83395242, 79.09851259, 79.35659413, 79.61042244, 79.85482517,\n        80.10010612, 80.35170329, 80.61450053, 80.88041162, 81.1236529 ,\n        81.37219402, 81.61358458, 81.86087408, 82.1091981 , 82.34863902,\n        82.57477327, 82.79636645, 83.0098568 , 83.20707854])}\n\n\n\n\nAutoETS.forward method\n\nautoets.forward(train[\"y\"].values, h=6)\n\n{'mean': array([82.80204272, 82.94739245, 83.09274218, 83.23809191, 83.38344164,\n        83.52879137])}"
  },
  {
    "objectID": "docs/models/autoets.html#model-evaluation",
    "href": "docs/models/autoets.html#model-evaluation",
    "title": "AutoETS Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nThe commonly used accuracy metrics to judge forecasts are:\n\nMean Absolute Percentage Error (MAPE)\nMean Error (ME)\nMean Absolute Error (MAE)\nMean Percentage Error (MPE)\nRoot Mean Squared Error (RMSE)\nCorrelation between the Actual and the Forecast (corr)\n\n\nfrom sklearn import metrics\n\n\ndef model_evaluation(y_true, y_pred, model):\n    \n    def mean_absolute_percentage_error(y_true, y_pred): \n        y_true, y_pred = np.array(y_true), np.array(y_pred)\n        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n    print (f'Model Evaluation: {model}')\n    print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}')\n    print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}')\n    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')\n    print(f'MAPE is : {mean_absolute_percentage_error(y_true, y_pred)}')\n    print(f'R2 is : {metrics.r2_score(y_true, y_pred)}')\n    print(f'corr is : {np.corrcoef(y_true, y_pred)[0,1]}',end='\\n\\n')\n\n\nmodel_evaluation(test[\"y\"], forecast[\"hat\"], \"AutoETS\")\n\nModel Evaluation: AutoETS\nMSE is : 0.5813708312500836\nMAE is : 0.7269623343638779\nRMSE is : 0.7624767742364902\nMAPE is : 0.87594467113361\nR2 is : -7.407128931524218\ncorr is : 0.4910418089228463"
  },
  {
    "objectID": "docs/models/autoets.html#acknowledgements",
    "href": "docs/models/autoets.html#acknowledgements",
    "title": "AutoETS Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/autoets.html#references",
    "href": "docs/models/autoets.html#references",
    "title": "AutoETS Model",
    "section": "References ",
    "text": "References \n\nNixtla Automatic Forecasting\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù."
  },
  {
    "objectID": "docs/models/tsb.html",
    "href": "docs/models/tsb.html",
    "title": "TSB Model",
    "section": "",
    "text": "Introduction\nTSB\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of TSB with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/tsb.html#table-of-contents",
    "href": "docs/models/tsb.html#table-of-contents",
    "title": "TSB Model",
    "section": "",
    "text": "Introduction\nTSB\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of TSB with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/tsb.html#introduction",
    "href": "docs/models/tsb.html#introduction",
    "title": "TSB Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Teunter-Syntetos-Babai (TSB) model is a model used in the field of inventory management and demand forecasting in time series. It was proposed by Teunter, Syntetos, and Babai in 2001 as an extension of Croston‚Äôs demand forecasting model.\nThe TSB model is specifically used to forecast demand for products with intermittent demand characteristics, that is, products that experience periods of demand followed by periods of non-demand. It is designed to handle time series data with many zeros and variability in the intervals between non-null observations.\nThe TSB model is based on two main components: the level model and the interval model. The level model estimates the level of demand when it occurs, while the interval model estimates the interval between demand occurrences. These two components combine to generate accurate forecasts of future demand.\nThe TSB model has proven to be effective in intermittent demand forecasting and has been widely used in various industrial sectors. However, it is important to note that there are other models and approaches available for demand forecasting, and the choice of the appropriate model will depend on the specific characteristics of the data and the context in which it is applied."
  },
  {
    "objectID": "docs/models/tsb.html#tsb-model",
    "href": "docs/models/tsb.html#tsb-model",
    "title": "TSB Model",
    "section": "TSB Model",
    "text": "TSB Model\nTSB (Teunter, Syntetos and Babai) is a new method proposed in 2011, the method replace the demand interval by demand probability which is updated every period. The reason for this is the Croston‚Äôs method only update demand when it occur, however in real life there are plenty of cases with many zero demands, therefore, the result of forecast will be unsuitable for estimating the risk of obsolescence because of the outdated information.\nIn TSB method, the \\(D_t\\) represent the demand occurrence indicator for period \\(t\\), so :\nIf \\(D_t=0\\), then\n\\[Z'_t=Z'_{t-1}\\]\n\\[D_t=D'_{t-1}+\\beta (0- D'_{t-1})\\]\nOtherwise \\[Z'_t=Z'_{t-1}+\\alpha(Z_t - Z'_{t-2})\\]\n\\[D'_t=D'_{t-1}+\\beta(1-D'_{t-1})\\]\nHence, the forecast is given by\n\\[Y'_t=D'_t \\cdot Z'_t\\]\nWhere\n\n\\(Y'_t:\\) Average demand per period\n\\(Z_t:\\) Actual demand at period \\(t\\)\n\\(Z'_t:\\) Time between two positive demand\n\\(D'_t:\\) Estimate probability of a demand occurrence at the end of period \\(t\\)\n\\(\\alpha, \\beta:\\) Smoothing Constant, \\(0 \\leq \\alpha, \\beta \\leq 1\\)\n\n\nTSB General Properties\nThe Teunter-Syntetos-Babai (TSB) model for time series has the following properties:\n\nIntermittent Demand Modelling: The TSB model is specifically designed to forecast intermittent demand, which is characterized by periods of non-demand followed by periods of demand. The model efficiently addresses this characteristic of demand.\nLevel and interval components: The TSB model is based on two main components: the level model and the interval model. The level model estimates the level of demand when it occurs, while the interval model estimates the interval between demand occurrences.\nHandling data with many zeros: The TSB model can efficiently handle time series data with many zeros, which are common in intermittent demand. The model properly considers these zeros in the forecasting process.\nExponential Smoothing: The TSB model uses exponential smoothing methods to estimate demand levels and intervals between occurrences. Exponential smoothing is a widely used technique in time series forecasting.\nConfidence interval estimation: The TSB model provides confidence interval estimates for the generated forecasts. This allows having a measure of the uncertainty associated with forecasts and facilitates decision making.\nSimplicity and ease of implementation: The TSB model is relatively simple and easy to implement compared to other more complex approaches. It does not require sophisticated assumptions about the distribution of demand and can be applied in a practical way.\n\nThose are some of the fundamental properties of the Teunter-Syntetos-Babai model in the context of time series and intermittent demand forecasting."
  },
  {
    "objectID": "docs/models/tsb.html#loading-libraries-and-data",
    "href": "docs/models/tsb.html#loading-libraries-and-data",
    "title": "TSB Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nimport pandas as pd\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/intermittend_demand2\")\ndf.head()\n\n\n\n\n\n\n\n\ndate\nsales\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n\n\n1\n2022-01-01 01:00:00\n10\n\n\n2\n2022-01-01 02:00:00\n0\n\n\n3\n2022-01-01 03:00:00\n0\n\n\n4\n2022-01-01 04:00:00\n100\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n1\n\n\n1\n2022-01-01 01:00:00\n10\n1\n\n\n2\n2022-01-01 02:00:00\n0\n1\n\n\n3\n2022-01-01 03:00:00\n0\n1\n\n\n4\n2022-01-01 04:00:00\n100\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/tsb.html#explore-data-with-the-plot-method",
    "href": "docs/models/tsb.html#explore-data-with-the-plot-method",
    "title": "TSB Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\nAutocorrelation (ACF) and partial autocorrelation (PACF) plots are statistical tools used to analyze time series. ACF charts show the correlation between the values of a time series and their lagged values, while PACF charts show the correlation between the values of a time series and their lagged values, after the effect of previous lagged values has been removed.\nACF and PACF charts can be used to identify the structure of a time series, which can be helpful in choosing a suitable model for the time series. For example, if the ACF chart shows a repeating peak and valley pattern, this indicates that the time series is stationary, meaning that it has the same statistical properties over time. If the PACF chart shows a pattern of rapidly decreasing spikes, this indicates that the time series is invertible, meaning it can be reversed to get a stationary time series.\nThe importance of the ACF and PACF charts is that they can help analysts better understand the structure of a time series. This understanding can be helpful in choosing a suitable model for the time series, which can improve the ability to predict future values of the time series.\nTo analyze ACF and PACF charts:\n\nLook for patterns in charts. Common patterns include repeating peaks and valleys, sawtooth patterns, and plateau patterns.\nCompare ACF and PACF charts. The PACF chart generally has fewer spikes than the ACF chart.\nConsider the length of the time series. ACF and PACF charts for longer time series will have more spikes.\nUse a confidence interval. The ACF and PACF plots also show confidence intervals for the autocorrelation values. If an autocorrelation value is outside the confidence interval, it is likely to be significant.\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom plotly.subplots import make_subplots\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=24,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "docs/models/tsb.html#split-the-data-into-training-and-testing",
    "href": "docs/models/tsb.html#split-the-data-into-training-and-testing",
    "title": "TSB Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our TSB Model. 2. Data to test our model\nFor the test data we will use the last 500 Hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2023-01-31 19:00:00'] \ntest = df[df.ds&gt;'2023-01-31 19:00:00']\n\n\ntrain.shape, test.shape\n\n((9500, 3), (500, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\",linewidth=2)\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\", linewidth=2, color=\"yellow\")\nplt.title(\"Store visit\");\nplt.xlabel(\"Hours\")\nplt.show()"
  },
  {
    "objectID": "docs/models/tsb.html#implementation-of-tsb-model-with-statsforecast",
    "href": "docs/models/tsb.html#implementation-of-tsb-model-with-statsforecast",
    "title": "TSB Model",
    "section": "Implementation of TSB Model with StatsForecast",
    "text": "Implementation of TSB Model with StatsForecast\nTo also know more about the parameters of the functions of the TSB Model you can see it here.\nalpha_d : float\n    Smoothing parameter for demand.\nalpha_p : float\n    Smoothing parameter for probability.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import TSB\n\n\n\nBuilding Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\nmodels = [TSB(alpha_d=0.8, alpha_p=0.9)]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[TSB])\n\n\nLet‚Äôs see the results of our TSB Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([22.443005], dtype=float32)}\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon)\nY_hat\n\n\n\n\n\n\n\n\nds\nTSB\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n22.443005\n\n\n1\n2023-02-21 17:00:00\n22.443005\n\n\n1\n2023-02-21 18:00:00\n22.443005\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n22.443005\n\n\n1\n2023-03-14 10:00:00\n22.443005\n\n\n1\n2023-03-14 11:00:00\n22.443005\n\n\n\n\n500 rows √ó 2 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nTSB\n\n\n\n\n0\n1\n2023-02-21 16:00:00\n22.443005\n\n\n1\n1\n2023-02-21 17:00:00\n22.443005\n\n\n2\n1\n2023-02-21 18:00:00\n22.443005\n\n\n...\n...\n...\n...\n\n\n497\n1\n2023-03-14 09:00:00\n22.443005\n\n\n498\n1\n2023-03-14 10:00:00\n22.443005\n\n\n499\n1\n2023-03-14 11:00:00\n22.443005\n\n\n\n\n500 rows √ó 3 columns\n\n\n\n\n# Concat the forecasts with the true values\nY_hat1 = pd.concat([df,Y_hat])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nTSB\n\n\n\n\n0\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n1\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n497\n2023-03-14 09:00:00\nNaN\n1\n22.443005\n\n\n498\n2023-03-14 10:00:00\nNaN\n1\n22.443005\n\n\n499\n2023-03-14 11:00:00\nNaN\n1\n22.443005\n\n\n\n\n10500 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df, Y_hat1]).set_index('ds')\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[\"TSB\"].plot(ax=ax, linewidth=2, color=\"yellow\")\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel(\"Store visit (Hourly data)\", fontsize=20)\nax.set_xlabel('Hours', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \n\nforecast_df\n\n\n\n\n\n\n\n\nds\nTSB\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n22.443005\n\n\n1\n2023-02-21 17:00:00\n22.443005\n\n\n1\n2023-02-21 18:00:00\n22.443005\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n22.443005\n\n\n1\n2023-03-14 10:00:00\n22.443005\n\n\n1\n2023-03-14 11:00:00\n22.443005\n\n\n\n\n500 rows √ó 2 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nTSB\n\n\nds\n\n\n\n\n\n\n\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n22.443005\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n22.443005\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n22.443005\n\n\n\n\n10500 rows √ó 3 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds').tail(5000)\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nTSB\n\n\nds\n\n\n\n\n\n\n\n2022-08-18 04:00:00\n0.0\n1\nNaN\n\n\n2022-08-18 05:00:00\n80.0\n1\nNaN\n\n\n2022-08-18 06:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n22.443005\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n22.443005\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n22.443005\n\n\n\n\n5000 rows √ó 3 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series.\n\nplt.plot(df_plot['y'],label=\"Actual\", linewidth=2.5)\nplt.plot(df_plot['TSB'], label=\"TSB\", color=\"yellow\") # '-', '--', '-.', ':',\n\nplt.title(\"Store visit (Hourly data)\");\nplt.xlabel(\"Hourly\")\nplt.ylabel(\"Store visit\")\nplt.legend()\nplt.show();\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/tsb.html#cross-validation",
    "href": "docs/models/tsb.html#cross-validation",
    "title": "TSB Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=50). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 500 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=50,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nTSB\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2023-01-23 12:00:00\n2023-01-23 11:00:00\n0.0\n0.000005\n\n\n1\n2023-01-23 13:00:00\n2023-01-23 11:00:00\n0.0\n0.000005\n\n\n1\n2023-01-23 14:00:00\n2023-01-23 11:00:00\n0.0\n0.000005\n\n\n...\n...\n...\n...\n...\n\n\n1\n2023-02-21 13:00:00\n2023-01-31 19:00:00\n60.0\n65.586456\n\n\n1\n2023-02-21 14:00:00\n2023-01-31 19:00:00\n20.0\n65.586456\n\n\n1\n2023-02-21 15:00:00\n2023-01-31 19:00:00\n20.0\n65.586456\n\n\n\n\n2500 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/tsb.html#evaluate-model",
    "href": "docs/models/tsb.html#evaluate-model",
    "title": "TSB Model",
    "section": "Evaluate Model ",
    "text": "Evaluate Model \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, TSB Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"TSB\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  91.88035"
  },
  {
    "objectID": "docs/models/tsb.html#acknowledgements",
    "href": "docs/models/tsb.html#acknowledgements",
    "title": "TSB Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/tsb.html#references",
    "href": "docs/models/tsb.html#references",
    "title": "TSB Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/garch.html",
    "href": "docs/models/garch.html",
    "title": "GARCH Model",
    "section": "",
    "text": "Introduction\nGARCH Models\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of GARCH with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/garch.html#table-of-contents",
    "href": "docs/models/garch.html#table-of-contents",
    "title": "GARCH Model",
    "section": "",
    "text": "Introduction\nGARCH Models\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of GARCH with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/garch.html#introduction",
    "href": "docs/models/garch.html#introduction",
    "title": "GARCH Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model is a statistical technique used to model and predict volatility in financial and economic time series. It was developed by Robert Engle in 1982 as an extension of the Autoregressive Conditional Heteroskedasticity (ARCH) model proposed by Andrew Lo and Craig MacKinlay in 1988.\nThe GARCH model allows capturing the presence of conditional heteroscedasticity in time series data, that is, the presence of fluctuations in the variance of a time series as a function of time. This is especially useful in financial data analysis, where volatility can be an important measure of risk.\nThe GARCH model has become a fundamental tool in the analysis of financial time series and has been used in a wide variety of applications, from risk management to forecasting prices of shares and other financial values."
  },
  {
    "objectID": "docs/models/garch.html#definition-of-garch-models",
    "href": "docs/models/garch.html#definition-of-garch-models",
    "title": "GARCH Model",
    "section": "Definition of GARCH Models ",
    "text": "Definition of GARCH Models \nDefinition 1. A \\(\\text{GARCH}(p,q)\\) model with order \\((p‚â•1,q‚â•0)\\) is of the form\n\\[\\begin{equation}\n    \\left\\{\n        \\begin{array}{ll}\n         X_t =\\sigma_t \\varepsilon_t      \\\\\n         \\sigma_{t}^2 =\\omega+ \\sum_{i=1}^{p} \\alpha_i X_{t-i}^2 + \\sum_{j=1}^{q} \\beta_j \\sigma_{t-j}^2 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nwhere \\(\\omega ‚â•0,\\alpha_i ‚â•0,\\beta_j ‚â•0,\\alpha_p &gt;0\\) ,and \\(\\beta_q &gt;0\\) are constants,\\(\\varepsilon_t \\sim iid(0,1)\\), and \\(\\varepsilon_t\\) is independent of \\(\\{X_k;k ‚â§ t ‚àí 1 \\}\\). A stochastic process \\(X_t\\) is called a \\(\\text{GARCH}(p, q )\\) process if it satisfies Eq. (1).\nIn practice, it has been found that for some time series, the \\(\\text{ARCH}(p)\\) model defined by (1) will provide an adequate fit only if the order \\(p\\) is large. By allowing past volatilities to affect the present volatility in (1), a more parsimonious model may result. That is why we need GARCH models. Besides, note the condition that the order \\(p ‚â• 1\\). The GARCH model in Definition 1 has the properties as follows.\nProposition 1. If \\(X_t\\) is a \\(\\text{GARCH}(p, q)\\) process defined in (1) and \\(\\sum_{i=1}^{p} \\alpha_{i} + \\sum_{j=1}^{q} \\beta_j &lt;1\\),then the following propositions hold.\n\n\\(X_{t}^2\\) follows the \\(\\text{ARMA}(m, q )\\) model\n\n\\[X_{t}^2=\\omega +\\sum_{i=1}^{m} (\\alpha_i + \\beta_i )X_{t-i}^2 + \\eta_t ‚àí \\sum_{j=1}^q \\beta_j \\eta_{t-j} \\]\nwhere \\(\\alpha_i =0\\) for \\(i &gt;p,Œ≤j =0\\) for \\(j &gt;q,m=max(p,q)\\), and \\(\\eta_t =\\sigma_{t}^2 (\\varepsilon_{t}^2 ‚àí1)\\).\n\n\\(X_t\\) is a white noise with\n\n\\[E(X)=0, E(X_{t+h} X_t )=0 \\ \\ \\text{for} \\ any \\ \\ h \\neq 0, Var(X_t)= \\frac{\\omega}{1-\\sum_{i=1}^{m} (\\alpha_i + \\beta_i )} \\]\n\n\\(\\sigma_{t}^2\\) is the conditional variance of \\(X_t\\) , that is, we have\n\n\\[E(X_t|\\mathscr{F}_{t‚àí1}) = 0, \\sigma_{t}^2 = Var(X_{t}^2|\\mathscr{F}_{t‚àí1}).\\]\n\nModel (1) reflects the fat tails and volatility clustering.\n\nAlthough an asset return series can usually be seen as a white noise, there exists such a return series so that it may be autocorrelated. What is more, a given original time series is not necessarily a return series, and at the same time, its values may be negative. If a time series is autocorrelated, we must first build an adequate model (e.g., an ARMA model) for the series in order to remove any autocorrelation in it. Then check whether the residual series has an ARCH effect, and if yes then we further model the residuals. In other words, if a time series \\(Y_t\\) is autocorrelated and has ARCH effect, then a GARCH model that can capture the features of \\(Y_t\\)t should be of the form\n\\[\\begin{equation}\n    \\left\\{\n        \\begin{array}{ll}\n         Y_t =f(Z_t,Y_{t‚àí1},X_{t‚àí1},Y_{t‚àí2},X_{t‚àí2},\\cdots )+X_t   \\tag 2,  \\\\\n         X_t = \\sigma_t \\varepsilon_t, \\\\\n         \\sigma_{t}^2 =\\omega+ \\sum_{i=1}^{p} \\alpha_i X_{t-i}^2 + \\sum_{j=1}^{q} \\beta_j \\sigma_{t-j}^2   \\ \\  \\ \\   \\   \\  (3)  \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nwhere Eq. (2) is referred to as the mean equation (model) and Eq. (3) is known as the volatility (variance) equation (model), and \\(Z_t\\) is a representative of exogenous regressors. If \\(Y_t\\) is a return series, then typically \\(Y_t = r + X_t\\) where \\(r\\) is a constant that means the expected returns is fixed.\n\nAdvantages and disadvantages of the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Model\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\n1. 1. Flexible model: The GARCH model is flexible and can fit different types of time series data with different volatility patterns.\n1. Requires a large amount of data: The GARCH model requires a large amount of data to accurately estimate the model parameters.\n\n\n2. Ability to model volatility: The GARCH model is capable of modeling the volatility and heteroscedasticity of a time series, which can improve the accuracy of forecasts.\n2. Sensitive to the model specification: The GARCH model is sensitive to the model specification and can be difficult to estimate if incorrectly specified.\n\n\n3. It incorporates past information: The GARCH model incorporates past information on the volatility of the time series, which makes it useful for predicting future volatility.\n3. It can be computationally expensive: The GARCH model can be computationally expensive, especially if more complex models are used.\n\n\n4. Allows the inclusion of exogenous variables: The GARCH model can be extended to include exogenous variables, which can improve the accuracy of the predictions.\n4. It does not consider extreme events: The GARCH model does not consider extreme or unexpected events in the time series, which can affect the accuracy of the predictions in situations of high volatility.\n\n\n5. The GARCH model makes it possible to model conditional heteroscedasticity, that is, the variation of the variance of a time series as a function of time and of the previous values of the time series itself.\n5. The GARCH model assumes that the time series errors are normally distributed, which may not be true in practice. If the errors are not normally distributed, the model may produce inaccurate estimates of volatility.\n\n\n6. The GARCH model can be used to estimate the value at risk (VaR) and the conditional value at risk (CVaR) of an investment portfolio.\n\n\n\n\n\n\nThe Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model can be applied in several fields\nThe Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model can be applied in a wide variety of areas where time series volatility is required to be modeled and predicted. Some of the areas in which the GARCH model can be applied are:\n\nFinancial markets: the GARCH model is widely used to model the volatility (risk) of returns on financial assets such as stocks, bonds, currencies, etc. It allows you to capture the changing nature of volatility.\nCommodity prices: the prices of raw materials such as oil, gold, grains, etc. they exhibit conditional volatility that can be modeled with GARCH.\nCredit risk: the risk of non-payment of loans and bonds also presents volatility over time that suits GARCH well.\nEconomic time series: macroeconomic indicators such as inflation, GDP, unemployment, etc. they have conditional volatility modelable with GARCH.\nImplicit volatility: the GARCH model allows estimating the implicit volatility in financial options.\nForecasts: GARCH allows conditional volatility forecasts to be made in any time series.\nRisk analysis: GARCH is useful for measuring and managing the risk of investment portfolios and assets.\nFinance: The GARCH model is widely used in finance to model the price volatility of financial assets, such as stocks, bonds, and currencies.\nEconomics: The GARCH model is used in economics to model the volatility of the prices of goods and services, inflation, and other economic indicators.\nEnvironmental sciences: The GARCH model is applied in environmental sciences to model the volatility of variables such as temperature, precipitation, and air quality.\nSocial sciences: The GARCH model is used in the social sciences to model the volatility of variables such as crime, migration, and employment.\nEngineering: The GARCH model is applied in engineering to model the volatility of variables such as the demand for electrical energy, industrial production, and vehicular traffic.\nHealth sciences: The GARCH model is used in health sciences to model the volatility of variables such as the number of cases of infectious diseases and the prices of medicines.\n\nThe GARCH Model is applicable in any context where it is required to model and forecast heterogeneous conditional volatility in time series, especially in finance and economics."
  },
  {
    "objectID": "docs/models/garch.html#loading-libraries-and-data",
    "href": "docs/models/garch.html#loading-libraries-and-data",
    "title": "GARCH Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\nLet‚Äôs pull the S&P500 stock data from the Yahoo Finance site.\n\nimport pandas as pd\nimport time\nfrom datetime import datetime\n\nticker = '^GSPC'\nperiod1 = int(time.mktime(datetime(2015, 1, 1, 23, 59).timetuple()))\nperiod2 = int(time.mktime(datetime.now().timetuple()))\ninterval = '1d' # 1d, 1m\n\nquery_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n\nSP_500 = pd.read_csv(query_string)\nSP_500.head()\n\n\n\n\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\n\n\n0\n2015-01-02\n2058.899902\n2072.360107\n2046.040039\n2058.199951\n2058.199951\n2708700000\n\n\n1\n2015-01-05\n2054.439941\n2054.439941\n2017.339966\n2020.579956\n2020.579956\n3799120000\n\n\n2\n2015-01-06\n2022.150024\n2030.250000\n1992.439941\n2002.609985\n2002.609985\n4460110000\n\n\n3\n2015-01-07\n2005.550049\n2029.609985\n2005.550049\n2025.900024\n2025.900024\n3805480000\n\n\n4\n2015-01-08\n2030.609985\n2064.080078\n2030.609985\n2062.139893\n2062.139893\n3934010000\n\n\n\n\n\n\n\n\ndf=SP_500[[\"Date\",\"Close\"]]\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2015-01-02\n2058.199951\n1\n\n\n1\n2015-01-05\n2020.579956\n1\n\n\n2\n2015-01-06\n2002.609985\n1\n\n\n3\n2015-01-07\n2025.900024\n1\n\n\n4\n2015-01-08\n2062.139893\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds            object\ny            float64\nunique_id     object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/garch.html#explore-data-with-the-plot-method",
    "href": "docs/models/garch.html#explore-data-with-the-plot-method",
    "title": "GARCH Model",
    "section": "Explore data with the plot method ",
    "text": "Explore data with the plot method \nPlot a series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nThe Augmented Dickey-Fuller Test\nAn Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\nNull Hypothesis: Time Series is non-stationary. It gives a time-dependent trend. Alternate Hypothesis: Time Series is stationary. In another term, the series doesn‚Äôt depend on time.\nADF or t Statistic &lt; critical values: Reject the null hypothesis, time series is stationary. ADF or t Statistic &gt; critical values: Failed to reject the null hypothesis, time series is non-stationary.\nLet‚Äôs check if our series that we are analyzing is a stationary series. Let‚Äôs create a function to check, using the Dickey Fuller test\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],'S&P500')\n\nDickey-Fuller test results for columns: S&P500\nTest Statistic          -0.814971\np-value                  0.814685\nNo Lags Used            10.000000\n                          ...    \nCritical Value (1%)     -3.433341\nCritical Value (5%)     -2.862861\nCritical Value (10%)    -2.567473\nLength: 7, dtype: float64\nConclusion:====&gt;\nThe null hypothesis cannot be rejected\nThe data is not stationary\n\n\nIn the previous result we can see that the Augmented_Dickey_Fuller test gives us a p-value of 0.864700, which tells us that the null hypothesis cannot be rejected, and on the other hand the data of our series are not stationary.\nWe need to differentiate our time series, in order to convert the data to stationary.\n\n\nReturn Series\nSince the 1970s, the financial industry has been very prosperous with advancement of computer and Internet technology. Trade of financial products (including various derivatives) generates a huge amount of data which form financial time series. For finance, the return on a financial product is most interesting, and so our attention focuses on the return series. If {Pt } is the closing price at time t for a certain financial product, then the return on this product is\n\\[X_t = \\frac{(P_t ‚àí P_{t‚àí1})}{P_{t‚àí1}} ‚âà log(P_t ) ‚àí log(P_{t‚àí1}).\\]\nIt is return series \\(\\{X_t \\}\\) that have been much independently studied. And important stylized features which are common across many instruments, markets, and time periods have been summarized. Note that if you purchase the financial product, then it becomes your asset, and its returns become your asset returns. Now let us look at the following examples.\nWe can estimate the series of returns using the pandas, DataFrame.pct_change() function. The pct_change() function has a periods parameter whose default value is 1. If you want to calculate a 30-day return, you must change the value to 30.\n\ndf['return'] = 100 * df[\"y\"].pct_change()\ndf.dropna(inplace=True, how='any')\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nreturn\n\n\n\n\n1\n2015-01-05\n2020.579956\n1\n-1.827811\n\n\n2\n2015-01-06\n2002.609985\n1\n-0.889347\n\n\n3\n2015-01-07\n2025.900024\n1\n1.162984\n\n\n4\n2015-01-08\n2062.139893\n1\n1.788828\n\n\n5\n2015-01-09\n2044.810059\n1\n-0.840381\n\n\n\n\n\n\n\n\nimport plotly.express as px\nfig = px.line(df, x=df[\"ds\"], y=\"return\",title=\"SP500 Return Chart\",template = \"plotly_dark\")\nfig.show()\n\n\n                                                \n\n\n\n\nCreating Squared Returns\n\ndf['sq_return'] = df[\"return\"].mul(df[\"return\"])\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nreturn\nsq_return\n\n\n\n\n1\n2015-01-05\n2020.579956\n1\n-1.827811\n3.340891\n\n\n2\n2015-01-06\n2002.609985\n1\n-0.889347\n0.790938\n\n\n3\n2015-01-07\n2025.900024\n1\n1.162984\n1.352532\n\n\n4\n2015-01-08\n2062.139893\n1\n1.788828\n3.199906\n\n\n5\n2015-01-09\n2044.810059\n1\n-0.840381\n0.706240\n\n\n\n\n\n\n\n\n\nReturns vs Squared Returns\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(go.Scatter(x=df[\"ds\"], y=df[\"return\"],\n                         mode='lines',\n                         name='return'),\nrow=1, col=1\n)\n\n\nfig.add_trace(go.Scatter(x=df[\"ds\"], y=df[\"sq_return\"],\n                         mode='lines',\n                         name='sq_return'), \n    row=1, col=2\n)\n\nfig.update_layout(height=600, width=800, title_text=\"Returns vs Squared Returns\", template = \"plotly_dark\")\nfig.show()\n\n\n                                                \n\n\n\nfrom scipy.stats import probplot, moment\nfrom statsmodels.tsa.stattools import adfuller, q_stat, acf\nimport numpy as np\nimport seaborn as sns\n\ndef plot_correlogram(x, lags=None, title=None):    \n    lags = min(10, int(len(x)/5)) if lags is None else lags\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 8))\n    x.plot(ax=axes[0][0], title='Return')\n    x.rolling(21).mean().plot(ax=axes[0][0], c='k', lw=1)\n    q_p = np.max(q_stat(acf(x, nlags=lags), len(x))[1])\n    stats = f'Q-Stat: {np.max(q_p):&gt;8.2f}\\nADF: {adfuller(x)[1]:&gt;11.2f}'\n    axes[0][0].text(x=.02, y=.85, s=stats, transform=axes[0][0].transAxes)\n    probplot(x, plot=axes[0][1])\n    mean, var, skew, kurtosis = moment(x, moment=[1, 2, 3, 4])\n    s = f'Mean: {mean:&gt;12.2f}\\nSD: {np.sqrt(var):&gt;16.2f}\\nSkew: {skew:12.2f}\\nKurtosis:{kurtosis:9.2f}'\n    axes[0][1].text(x=.02, y=.75, s=s, transform=axes[0][1].transAxes)\n    plot_acf(x=x, lags=lags, zero=False, ax=axes[1][0])\n    plot_pacf(x, lags=lags, zero=False, ax=axes[1][1])\n    axes[1][0].set_xlabel('Lag')\n    axes[1][1].set_xlabel('Lag')\n    fig.suptitle(title+ f'Dickey-Fuller: {adfuller(x)[1]:&gt;11.2f}', fontsize=14)\n    sns.despine()\n    fig.tight_layout()\n    fig.subplots_adjust(top=.9)\n\n\nplot_correlogram(df[\"return\"], lags=30, title=\"Time Series Analysis plot \\n\")\n\n\n\n\n\n\nLjung-Box Test\nLjung-Box is a test for autocorrelation that we can use in tandem with our ACF and PACF plots. The Ljung-Box test takes our data, optionally either lag values to test, or the largest lag value to consider, and whether to compute the Box-Pierce statistic. Ljung-Box and Box-Pierce are two similar test statisitcs, Q , that are compared against a chi-squared distribution to determine if the series is white noise. We might use the Ljung-Box test on the residuals of our model to look for autocorrelation, ideally our residuals would be white noise.\n\nHo : The data are independently distributed, no autocorrelation.\nHa : The data are not independently distributed; they exhibit serial correlation.\n\nThe Ljung-Box with the Box-Pierce option will return, for each lag, the Ljung-Box test statistic, Ljung-Box p-values, Box-Pierce test statistic, and Box-Pierce p-values.\nIf \\(p&lt;\\alpha (0.05)\\) we reject the null hypothesis.\n\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\nljung_res = acorr_ljungbox(df[\"return\"], lags= 40, boxpierce=True)\n\nljung_res.head()\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\nbp_stat\nbp_pvalue\n\n\n\n\n1\n49.222273\n2.285409e-12\n49.155183\n2.364927e-12\n\n\n2\n62.991348\n2.097020e-14\n62.899234\n2.195861e-14\n\n\n3\n63.944944\n8.433622e-14\n63.850663\n8.834380e-14\n\n\n4\n74.343652\n2.742989e-15\n74.221024\n2.911751e-15\n\n\n5\n80.234862\n7.494100e-16\n80.093498\n8.022242e-16"
  },
  {
    "objectID": "docs/models/garch.html#split-the-data-into-training-and-testing",
    "href": "docs/models/garch.html#split-the-data-into-training-and-testing",
    "title": "GARCH Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our GARCH model 2. Data to test our model\nFor the test data we will use the last 30 day to test and evaluate the performance of our model.\n\ndf=df[[\"ds\",\"unique_id\",\"return\"]]\ndf.columns=[\"ds\", \"unique_id\", \"y\"]\n\n\ntrain = df[df.ds&lt;='2023-05-31'] # Let's forecast the last 30 days\ntest = df[df.ds&gt;'2023-05-31']\n\n\ntrain.shape, test.shape\n\n((2116, 3), (83, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.show()"
  },
  {
    "objectID": "docs/models/garch.html#implementation-of-garch-with-statsforecast",
    "href": "docs/models/garch.html#implementation-of-garch-with-statsforecast",
    "title": "GARCH Model",
    "section": "Implementation of GARCH with StatsForecast ",
    "text": "Implementation of GARCH with StatsForecast \nTo also know more about the parameters of the functions of the GARCH Model, they are listed below. For more information, visit the documentation\np : int\n    Number of lagged versions of the series.\nq: int\n    Number of lagged versions of the volatility.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast \nfrom statsforecast.models import GARCH\n\n\n\nInstantiating Models\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful.season_length.\n\nseason_length = 7 # Dayly data \nhorizon = len(test) # number of predictions biasadj=True, include_drift=True,\n\nmodels = [GARCH(1,1),\n          GARCH(1,2),\n          GARCH(2,2),\n          GARCH(2,1),\n          GARCH(3,1),\n          GARCH(3,2),\n          GARCH(3,3), \n          GARCH(1,3), \n          GARCH(2,3)]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='C', # custom business day frequency\n                   n_jobs=-1)"
  },
  {
    "objectID": "docs/models/garch.html#cross-validation",
    "href": "docs/models/garch.html#cross-validation",
    "title": "GARCH Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWe have built different GARCH models, so we need to determine which is the best model to then be able to train it and thus be able to make the predictions. To know which is the best model we go to the Cross Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=6,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nGARCH(1,1)\nGARCH(1,2)\nGARCH(2,2)\nGARCH(2,1)\nGARCH(3,1)\nGARCH(3,2)\nGARCH(3,3)\nGARCH(1,3)\nGARCH(2,3)\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2023-01-03\n2023-01-02\n-0.404962\n1.906913\n1.886413\n1.908357\n1.930915\n1.913208\n1.904380\n1.903494\n1.885897\n1.873233\n\n\n1\n2023-01-04\n2023-01-02\n-1.202064\n-0.824837\n-0.838750\n-0.827394\n-0.824686\n-0.835927\n-0.822887\n-0.841358\n-0.833009\n-0.828155\n\n\n1\n2023-01-05\n2023-01-02\n1.746133\n-0.665798\n-0.653746\n-0.666613\n-0.680718\n-0.663415\n-0.677103\n-0.649123\n-0.661879\n-0.665392\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2023-05-29\n2023-02-03\n1.304909\n-0.389434\n-0.350367\n-0.313831\n-0.191451\n-0.192649\n-0.186990\n-0.184408\n-0.188771\n-0.185092\n\n\n1\n2023-05-30\n2023-02-03\n0.001660\n-0.340987\n-0.296400\n-0.278776\n-0.158889\n-0.157296\n-0.160854\n-0.154074\n-0.156617\n-0.158255\n\n\n1\n2023-05-31\n2023-02-03\n-0.610862\n0.316639\n0.274853\n0.248995\n0.135092\n0.134911\n0.132532\n0.135533\n0.134844\n0.133779\n\n\n\n\n415 rows √ó 12 columns\n\n\n\n\nfrom datasetsforecast.losses import rmse\n\ndef compute_cv_rmse(crossvalidation_df):\n    \"\"\"Compute MAE for all models generated\"\"\"\n    res = {}\n    for mod in models: \n        res[mod] = rmse(crossvalidation_df['actual'], crossvalidation_df[str(mod)])\n    return pd.Series(res)\n\n\ncrossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True) # rename actual values \nrmse_cv = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(compute_cv_rmse)\n\nmae = rmse_cv.groupby('unique_id').mean()\nmae.style.highlight_min(color = 'red', axis = 1)\n\n\n\n\n\n\n¬†\nGARCH(1,1)\nGARCH(1,2)\nGARCH(2,2)\nGARCH(2,1)\nGARCH(3,1)\nGARCH(3,2)\nGARCH(3,3)\nGARCH(1,3)\nGARCH(2,3)\n\n\nunique_id\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\n1\n1.598613\n1.721620\n1.547210\n1.375646\n1.442002\n1.419195\n1.373299\n1.377253\n1.374233\n\n\n\n\n\nNote: This result can vary depending on the data and period you use to train and test the model, and the models you want to test. This is an example, where the objective is to be able to teach a methodology for the use of StatsForecast, and in particular the GARCH model and the parameters used in Cross Validation to determine the best model for this example.\nIn the previous result it can be seen that the best model is the model \\(\\text{GARCH}(1,1)\\)\nWith this result found using Cross Validation to determine which is the best model, we are going to continue training our model, to then make the predictions.\n\n\nFit the Model\n\nseason_length = 7 # Dayly data \nhorizon = len(test) # number of predictions biasadj=True, include_drift=True,\n\nmodels = [GARCH(1,1)]\n\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='C', # custom business day frequency\n                   n_jobs=-1)\n\n\nsf.fit()\n\nStatsForecast(models=[GARCH(1,1)])\n\n\nLet‚Äôs see the results of our Theta model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'p': 1,\n 'q': 1,\n 'coeff': array([0.11715665, 0.261296  , 0.64151786]),\n 'message': 'Optimization terminated successfully',\n 'y_vals': array([-0.61086244]),\n 'sigma2_vals': array([0.81241364]),\n 'fitted': array([        nan,  2.22688619, -0.75658963, ..., -0.20975516,\n         0.83589047,  0.1360351 ]),\n 'actual_residuals': array([        nan, -3.11623338,  1.91957389, ...,  1.51466384,\n        -0.83423013, -0.74689754])}\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"actual_residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\nNaN\n\n\n1\n-3.116233\n\n\n2\n1.919574\n\n\n...\n...\n\n\n2113\n1.514664\n\n\n2114\n-0.834230\n\n\n2115\n-0.746898\n\n\n\n\n2116 rows √ó 1 columns\n\n\n\n\nfrom scipy import stats\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\n# plot[1,1]\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\n# plot\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\n# plot\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\n# plot\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\nY_hat = sf.forecast(horizon, fitted=True)\n\nY_hat.head()\n\n\n\n\n\n\n\n\nds\nGARCH(1,1)\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-06-01\n1.393380\n\n\n1\n2023-06-02\n-0.640599\n\n\n1\n2023-06-05\n-0.508753\n\n\n1\n2023-06-06\n-0.947621\n\n\n1\n2023-06-07\n0.798889\n\n\n\n\n\n\n\n\nY_hat = sf.forecast(horizon, fitted=True, level=[95])\n\nY_hat.head()\n\n\n\n\n\n\n\n\nds\nGARCH(1,1)\nGARCH(1,1)-lo-95\nGARCH(1,1)-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2023-06-01\n1.393380\n-0.048836\n2.835595\n\n\n1\n2023-06-02\n-0.640599\n-2.789734\n1.508536\n\n\n1\n2023-06-05\n-0.508753\n-2.327246\n1.309740\n\n\n1\n2023-06-06\n-0.947621\n-2.476394\n0.581153\n\n\n1\n2023-06-07\n0.798889\n-0.871354\n2.469133\n\n\n\n\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nGARCH(1,1)\nGARCH(1,1)-lo-95\nGARCH(1,1)-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n1\n2015-01-05\n-1.827811\nNaN\nNaN\nNaN\n\n\n1\n2015-01-06\n-0.889347\n2.226886\n-0.858147\n5.311919\n\n\n1\n2015-01-07\n1.162984\n-0.756590\n-3.841623\n2.328444\n\n\n1\n2015-01-08\n1.788828\n-0.636398\n-3.721431\n2.448635\n\n\n1\n2015-01-09\n-0.840381\n-1.472993\n-4.558026\n1.612040\n\n\n\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nGARCH(1,1)\nGARCH(1,1)-lo-95\nGARCH(1,1)-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2023-06-01\n1.393380\n-0.048836\n2.835595\n\n\n1\n2023-06-02\n-0.640599\n-2.789734\n1.508536\n\n\n1\n2023-06-05\n-0.508753\n-2.327246\n1.309740\n\n\n...\n...\n...\n...\n...\n\n\n1\n2023-09-21\n-0.199650\n-1.780158\n1.380858\n\n\n1\n2023-09-22\n-0.161219\n-1.425179\n1.102742\n\n\n1\n2023-09-25\n0.136796\n-0.916991\n1.190583\n\n\n\n\n83 rows √ó 4 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nGARCH(1,1)\nGARCH(1,1)-lo-95\nGARCH(1,1)-hi-95\n\n\n\n\n0\n1\n2023-06-01\n1.393380\n-0.048836\n2.835595\n\n\n1\n1\n2023-06-02\n-0.640599\n-2.789734\n1.508536\n\n\n2\n1\n2023-06-05\n-0.508753\n-2.327246\n1.309740\n\n\n3\n1\n2023-06-06\n-0.947621\n-2.476394\n0.581153\n\n\n4\n1\n2023-06-07\n0.798889\n-0.871354\n2.469133\n\n\n\n\n\n\n\n\n# Merge the forecasts with the true values\ntest['unique_id'] = test['unique_id'].astype(int)\nY_hat1 = test.merge(Y_hat, how='left', on=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\nds\nunique_id\ny\nGARCH(1,1)\nGARCH(1,1)-lo-95\nGARCH(1,1)-hi-95\n\n\n\n\n0\n2023-06-01\n1\n0.985445\n1.393380\n-0.048836\n2.835595\n\n\n1\n2023-06-02\n1\n1.453442\n-0.640599\n-2.789734\n1.508536\n\n\n2\n2023-06-05\n1\n-0.200358\n-0.508753\n-2.327246\n1.309740\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n80\n2023-09-26\n1\n-1.473453\nNaN\nNaN\nNaN\n\n\n81\n2023-09-27\n1\n0.022931\nNaN\nNaN\nNaN\n\n\n82\n2023-09-28\n1\n0.589317\nNaN\nNaN\nNaN\n\n\n\n\n83 rows √ó 6 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds').tail(200)\nplot_df[['y', \"GARCH(1,1)\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Year ', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\nplt.show()\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 dayly ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nGARCH(1,1)\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-06-01\n1.393380\n\n\n1\n2023-06-02\n-0.640599\n\n\n1\n2023-06-05\n-0.508753\n\n\n...\n...\n...\n\n\n1\n2023-09-21\n-0.199650\n\n\n1\n2023-09-22\n-0.161219\n\n\n1\n2023-09-25\n0.136796\n\n\n\n\n83 rows √ó 2 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \n\nforecast_df.head(10)\n\n\n\n\n\n\n\n\nds\nGARCH(1,1)\nGARCH(1,1)-lo-95\nGARCH(1,1)-lo-80\nGARCH(1,1)-hi-80\nGARCH(1,1)-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n1\n2023-06-01\n1.393380\n-0.048836\n0.450365\n2.336394\n2.835595\n\n\n1\n2023-06-02\n-0.640599\n-2.789734\n-2.045843\n0.764645\n1.508536\n\n\n1\n2023-06-05\n-0.508753\n-2.327246\n-1.697802\n0.680296\n1.309740\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2023-06-12\n-1.251548\n-6.549860\n-4.715928\n2.212832\n4.046764\n\n\n1\n2023-06-13\n0.479689\n-3.951083\n-2.417437\n3.376815\n4.910460\n\n\n1\n2023-06-14\n-0.318133\n-3.508017\n-2.403886\n1.767620\n2.871751\n\n\n\n\n10 rows √ó 6 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\ndf_plot=pd.concat([df, forecast_df]).set_index('ds').tail(220)\ndf_plot\n\n\n\n\n\n\n\n\nunique_id\ny\nGARCH(1,1)\nGARCH(1,1)-lo-95\nGARCH(1,1)-lo-80\nGARCH(1,1)-hi-80\nGARCH(1,1)-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n2023-03-15\n1\n-0.698088\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2023-03-16\n1\n1.756201\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2023-03-17\n1\n-1.101946\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-09-21\nNaN\nNaN\n-0.199650\n-1.780158\n-1.233088\n0.833789\n1.380858\n\n\n2023-09-22\nNaN\nNaN\n-0.161219\n-1.425179\n-0.987678\n0.665241\n1.102742\n\n\n2023-09-25\nNaN\nNaN\n0.136796\n-0.916991\n-0.552238\n0.825831\n1.190583\n\n\n\n\n220 rows √ó 7 columns\n\n\n\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2 )\n    colors = ['green']\n  # Specify graph features:\n    ax.fill_between(df_plot.index, \n                df_plot['GARCH(1,1)-lo-80'], \n                df_plot['GARCH(1,1)-hi-80'],\n                alpha=.20,\n                color='lime',\n                label='GARCH(1,1)_level_80')\n    ax.fill_between(df_plot.index, \n                df_plot['GARCH(1,1)-lo-95'], \n                df_plot['GARCH(1,1)-hi-95'],\n                alpha=.2,\n                color='white',\n                label='GARCH(1,1)_level_95')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel(\"Return\", fontsize=20)\n    ax.set_xlabel('Month-Days', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid(True)\n    plt.show()\n\n\nplot_forecasts(train, test, forecast_df, models=[\"GARCH(1,1)\" ])"
  },
  {
    "objectID": "docs/models/garch.html#model-evaluation",
    "href": "docs/models/garch.html#model-evaluation",
    "title": "GARCH Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, GARCH.\n\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import mae, mape, mase, rmse, smape\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, model):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name =='mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=7)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, test, Y_hat, model=\"GARCH(1,1)\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nGARCH(1,1)\n0.886834\n372.562147\nNaN\n1.108626\n136.528289"
  },
  {
    "objectID": "docs/models/garch.html#acknowledgements",
    "href": "docs/models/garch.html#acknowledgements",
    "title": "GARCH Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/garch.html#references",
    "href": "docs/models/garch.html#references",
    "title": "GARCH Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nBollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of econometrics, 31(3), 307-327.\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007..\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/arima.html",
    "href": "docs/models/arima.html",
    "title": "ARIMA Model",
    "section": "",
    "text": "Introduction\nARIMA Models\nThe meaning of p, d and q in ARIMA model\nAR and MA models\nARIMA model\nHow to find the order of differencing (d) in ARIMA model\nLoading libraries and data\nExplore data with the plot method\nHow to find the order of the AR term (p)\nHow to find the order of the MA term (q)\nHow to handle if a time series is slightly under or over differenced\nImplementation of ARIMA with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/arima.html#table-of-contents",
    "href": "docs/models/arima.html#table-of-contents",
    "title": "ARIMA Model",
    "section": "",
    "text": "Introduction\nARIMA Models\nThe meaning of p, d and q in ARIMA model\nAR and MA models\nARIMA model\nHow to find the order of differencing (d) in ARIMA model\nLoading libraries and data\nExplore data with the plot method\nHow to find the order of the AR term (p)\nHow to find the order of the MA term (q)\nHow to handle if a time series is slightly under or over differenced\nImplementation of ARIMA with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/arima.html#introduction",
    "href": "docs/models/arima.html#introduction",
    "title": "ARIMA Model",
    "section": "Introduction ",
    "text": "Introduction \n\nA Time Series is defined as a series of data points recorded at different time intervals. The time order can be daily, monthly, or even yearly.\nTime Series forecasting is the process of using a statistical model to predict future values of a time series based on past results.\nWe have discussed various aspects of Time Series Forecasting in the previous notebook.\nForecasting is the step where we want to predict the future values the series is going to take. Forecasting a time series is often of tremendous commercial value.\n\nForecasting a time series can be broadly divided into two types.\n\nIf we use only the previous values of the time series to predict its future values, it is called Univariate Time Series Forecasting.\nIf we use predictors other than the series (like exogenous variables) to forecast it is called Multi Variate Time Series Forecasting.\nThis notebook focuses on a particular type of forecasting method called ARIMA modeling."
  },
  {
    "objectID": "docs/models/arima.html#introduction-to-arima-models",
    "href": "docs/models/arima.html#introduction-to-arima-models",
    "title": "ARIMA Model",
    "section": "Introduction to ARIMA Models ",
    "text": "Introduction to ARIMA Models \n\nARIMA stands for Autoregressive Integrated Moving Average Model. It belongs to a class of models that explains a given time series based on its own past values -i.e.- its own lags and the lagged forecast errors. The equation can be used to forecast future values. Any ‚Äònon-seasonal‚Äô time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models.\nSo, ARIMA, short for AutoRegressive Integrated Moving Average, is a forecasting algorithm based on the idea that the information in the past values of the time series can alone be used to predict the future values.\nARIMA Models are specified by three order parameters: (p, d, q),\nwhere,\n\np is the order of the AR term\nq is the order of the MA term\nd is the number of differencing required to make the time series stationary\n\nAR(p) Autoregression ‚Äì a regression model that utilizes the dependent relationship between a current observation and observations over a previous period. An auto regressive (AR(p)) component refers to the use of past values in the regression equation for the time series.\nI(d) Integration ‚Äì uses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values d number of times.\nMA(q) Moving Average ‚Äì a model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. A moving average component depicts the error of the model as a combination of previous error terms. The order q represents the number of terms to be included in the model.\n\n\nTypes of ARIMA Model\n\nARIMA : Non-seasonal Autoregressive Integrated Moving Averages\nSARIMA : Seasonal ARIMA\nSARIMAX : Seasonal ARIMA with exogenous variables\n\nIf a time series, has seasonal patterns, then we need to add seasonal terms and it becomes SARIMA, short for Seasonal ARIMA."
  },
  {
    "objectID": "docs/models/arima.html#the-meaning-of-p-d-and-q-in-arima-model",
    "href": "docs/models/arima.html#the-meaning-of-p-d-and-q-in-arima-model",
    "title": "ARIMA Model",
    "section": "The meaning of p, d and q in ARIMA model ",
    "text": "The meaning of p, d and q in ARIMA model \n\nThe meaning of p\n\np is the order of the Auto Regressive (AR) term. It refers to the number of lags of Y to be used as predictors.\n\n\n\nThe meaning of d\n\nThe term Auto Regressive‚Äô in ARIMA means it is a linear regression model that uses its own lags as predictors. Linear regression models, as we know, work best when the predictors are not correlated and are independent of each other. So we need to make the time series stationary.\nThe most common approach to make the series stationary is to difference it. That is, subtract the previous value from the current value. Sometimes, depending on the complexity of the series, more than one differencing may be needed.\nThe value of d, therefore, is the minimum number of differencing needed to make the series stationary. If the time series is already stationary, then d = 0.\n\n\n\nThe meaning of q\n\nq is the order of the Moving Average (MA) term. It refers to the number of lagged forecast errors that should go into the ARIMA Model."
  },
  {
    "objectID": "docs/models/arima.html#ar-and-ma-models",
    "href": "docs/models/arima.html#ar-and-ma-models",
    "title": "ARIMA Model",
    "section": "AR and MA models ",
    "text": "AR and MA models \n\nAR model\nIn an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself.\nThus, an autoregressive model of order p can be written as\n\\[y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t}, \\tag{1}\\]\nwhere \\(\\epsilon_t\\) is white noise. This is like a multiple regression but with lagged values of \\(y_t\\) as predictors. We refer to this as an AR( p) model, an autoregressive model of order p .\n\n\nMA model\nRather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model,\n\\[y_{t} = c + \\varepsilon_t + \\theta_{1}\\varepsilon_{t-1} + \\theta_{2}\\varepsilon_{t-2} + \\dots + \\theta_{q}\\varepsilon_{t-q}, \\tag{2}\\]\nwhere \\(\\epsilon_t\\) is white noise. We refer to this as an MA(q) model, a moving average model of order q. Of course, we do not observe the values of\n\\(\\epsilon_t\\) , so it is not really a regression in the usual sense.\nNotice that each value of yt can be thought of as a weighted moving average of the past few forecast errors (although the coefficients will not normally sum to one). However, moving average models should not be confused with the moving average smoothing . A moving average model is used for forecasting future values, while moving average smoothing is used for estimating the trend-cycle of past values.\nThus, we have discussed AR and MA Models respectively."
  },
  {
    "objectID": "docs/models/arima.html#arima-model",
    "href": "docs/models/arima.html#arima-model",
    "title": "ARIMA Model",
    "section": "ARIMA model ",
    "text": "ARIMA model \nIf we combine differencing with autoregression and a moving average model, we obtain a non-seasonal ARIMA model. ARIMA is an acronym for AutoRegressive Integrated Moving Average (in this context, ‚Äúintegration‚Äù is the reverse of differencing). The full model can be written as\n\\[\\begin{equation}\n  y'_{t} = c + \\phi_{1}y'_{t-1} + \\cdots + \\phi_{p}y'_{t-p}\n     + \\theta_{1}\\varepsilon_{t-1} + \\cdots + \\theta_{q}\\varepsilon_{t-q} + \\varepsilon_{t},   \\tag{3}\n\\end{equation}\\]\nwhere \\(y'_{t}\\) is the differenced series (it may have been differenced more than once). The ‚Äúpredictors‚Äù on the right hand side include both lagged values of \\(y_t\\) and lagged errors. We call this an ARIMA(p,d,q) model, where\n\n\n\np\norder of the autoregressive part\n\n\nd\ndegree of first differencing involved\n\n\nq\norder of the moving average part\n\n\n\nThe same stationarity and invertibility conditions that are used for autoregressive and moving average models also apply to an ARIMA model.\nMany of the models we have already discussed are special cases of the ARIMA model, as shown in Table\n\n\n\n\n\n\n\n\n\nModel\np d q\nDifferenced\nMethod\n\n\n\n\nArima(0,0,0)\n0 0 0\n\\(y_t=Y_t\\)\nWhite noise\n\n\nARIMA (0,1,0)\n0 1 0\n\\(y_t = Y_t - Y_{t-1}\\)\nRandom walk\n\n\nARIMA (0,2,0)\n0 2 0\n\\(y_t = Y_t - 2Y_{t-1} + Y_{t-2}\\)\nConstant\n\n\nARIMA (1,0,0)\n1 0 0\n$Y_t = + 1 Y{t-1} + \\(| AR(1): AR(1): First-order regression model| |ARIMA (2, 0, 0)|2 0 0 |\\)Y_t = 0 + 1 Y{t-1} + 2 Y{t-2} + \\(| AR(2): Second-order regression model| |ARIMA (1, 1, 0)|1 1 0 |\\)Y_t = + Y{t-1} + 1 (Y{t-1}- Y_{t-2})$\nDifferenced first-order\n\n\nautoregressive model\n\n\n\n\n\nARIMA (0, 1, 1)\n0 1 1\n\\(\\hat Y_t = Y_{t-1} - \\Phi_1 e^{t-1}\\)\nSimple exponential\n\n\nsmoothing\n\n\n\n\n\nARIMA (0, 0, 1)\n0 0 1\n\\(\\hat Y_t = \\mu_0+ \\epsilon_t ‚Äì \\omega_1 \\epsilon_{t-1}\\)\nMA(1): First-order\n\n\nregression model\n\n\n\n\n\nARIMA (0, 0, 2)\n0 0 2\n\\(\\hat Y_t = \\mu_0+ \\epsilon_t ‚Äì \\omega_1 \\epsilon_{t-1} ‚Äì \\omega_2 \\epsilon_{t-2}\\)\nMA(2): Second-order\n\n\nregression model\n\n\n\n\n\nARIMA (1, 0, 1)\n1 0 1\n\\(\\hat Y_t = \\Phi_0 + \\Phi_1 Y_{t-1}+ \\epsilon_t ‚Äì \\omega_1 \\epsilon_{t-1}\\)\nARMA model\n\n\nARIMA (1, 1, 1)\n1 1 1\n\\(\\Delta Y_t = \\Phi_1 Y_{t-1} + \\epsilon_t - \\omega_1 \\epsilon_{t-1}\\)\nARIMA model\n\n\nARIMA (1, 1, 2)\n1 1 2\n\\(\\hat Y_t = Y_{t-1} + \\Phi_1 (Y_{t-1} - Y_{t-2} )- \\Theta_1 e_{t-1} - \\Theta_1 e_{t-1}\\) Damped-trend linear Exponential smoothing\n\n\n\nARIMA (0, 2, 1) OR (0,2,2)\n0 2 1\n\\(\\hat Y_t = 2 Y_{t-1} - Y_{t-2} - \\Theta_1 e_{t-1} - \\Theta_2 e_{t-2}\\)\nLinear exponential smoothing\n\n\n\nOnce we start combining components in this way to form more complicated models, it is much easier to work with the backshift notation. For example, the above equation can be written in backshift notation as\n\\[\\begin{equation}\n\\tag{4}\n  \\begin{array}{c c c c}\n    (1-\\phi_1B - \\cdots - \\phi_p B^p) & (1-B)^d y_{t} &= &c + (1 + \\theta_1 B + \\cdots + \\theta_q B^q)\\varepsilon_t\\\\\n    {\\uparrow} & {\\uparrow} & &{\\uparrow}\\\\\n    \\text{AR($p$)} & \\text{$d$ differences} & & \\text{MA($q$)}\\\\\n  \\end{array}\n\\end{equation}\\]\n\nARIMA model in words\nPredicted Yt = Constant + Linear combination Lags of Y (upto p lags) + Linear Combination of Lagged forecast errors (upto q lags)"
  },
  {
    "objectID": "docs/models/arima.html#how-to-find-the-order-of-differencing-d-in-arima-model",
    "href": "docs/models/arima.html#how-to-find-the-order-of-differencing-d-in-arima-model",
    "title": "ARIMA Model",
    "section": "How to find the order of differencing (d) in ARIMA model ",
    "text": "How to find the order of differencing (d) in ARIMA model \n\nAs stated earlier, the purpose of differencing is to make the time series stationary. But we should be careful to not over-difference the series. An over differenced series may still be stationary, which in turn will affect the model parameters.\nSo we should determine the right order of differencing. The right order of differencing is the minimum differencing required to get a near-stationary series which roams around a defined mean and the ACF plot reaches to zero fairly quick.\nIf the autocorrelations are positive for many number of lags (10 or more), then the series needs further differencing. On the other hand, if the lag 1 autocorrelation itself is too negative, then the series is probably over-differenced.\nIf we can‚Äôt really decide between two orders of differencing, then we go with the order that gives the least standard deviation in the differenced series.\nNow, we will explain these concepts with the help of an example as follows:\n\nFirst, I will check if the series is stationary using the Augmented Dickey Fuller test (ADF Test), from the statsmodels package. The reason being is that we need differencing only if the series is non-stationary. Else, no differencing is needed, that is, d=0.\nThe null hypothesis (Ho) of the ADF test is that the time series is non-stationary. So, if the p-value of the test is less than the significance level (0.05) then we reject the null hypothesis and infer that the time series is indeed stationary.\nSo, in our case, if P Value &gt; 0.05 we go ahead with finding the order of differencing."
  },
  {
    "objectID": "docs/models/arima.html#loading-libraries-and-data",
    "href": "docs/models/arima.html#loading-libraries-and-data",
    "title": "ARIMA Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead data\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/Esperanza_vida.csv\", usecols=[1,2])\ndf.head()\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n1960-01-01\n69.123902\n\n\n1\n1961-01-01\n69.760244\n\n\n2\n1962-01-01\n69.149756\n\n\n3\n1963-01-01\n69.248049\n\n\n4\n1964-01-01\n70.311707\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1960-01-01\n69.123902\n1\n\n\n1\n1961-01-01\n69.760244\n1\n\n\n2\n1962-01-01\n69.149756\n1\n\n\n3\n1963-01-01\n69.248049\n1\n\n\n4\n1964-01-01\n70.311707\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds            object\ny            float64\nunique_id     object\ndtype: object\n\n\nWe need to convert ds from the object type to datetime.\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/arima.html#explore-data-with-the-plot-method",
    "href": "docs/models/arima.html#explore-data-with-the-plot-method",
    "title": "ARIMA Model",
    "section": "Explore data with the plot method ",
    "text": "Explore data with the plot method \nPlot a series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\nLooking at the plot we can observe there is an upward trend over the period of time.\n\ndf[\"y\"].plot(kind='kde',figsize = (16,5))\ndf[\"y\"].describe()\n\ncount    60.000000\nmean     76.632439\nstd       4.495279\n           ...    \n50%      76.895122\n75%      80.781098\nmax      83.346341\nName: y, Length: 8, dtype: float64\n\n\n\n\n\n\nSeasonal Decomposed\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n\ndecomposed=seasonal_decompose(df[\"y\"], model = \"add\", period=1)\ndecomposed.plot()\nplt.show()\n\n\n\n\n\n\nThe Augmented Dickey-Fuller Test\nAn Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\nNull Hypothesis: Time Series is non-stationary. It gives a time-dependent trend. Alternate Hypothesis: Time Series is stationary. In another term, the series doesn‚Äôt depend on time.\nADF or t Statistic &lt; critical values: Reject the null hypothesis, time series is stationary. ADF or t Statistic &gt; critical values: Failed to reject the null hypothesis, time series is non-stationary.\n\nfrom statsmodels.tsa.stattools import adfuller\n\n\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],\"Life expectancy\")\n\nDickey-Fuller test results for columns: Life expectancy\nTest Statistic         -1.578590\np-value                 0.494339\nNo Lags Used            2.000000\n                          ...   \nCritical Value (1%)    -3.550670\nCritical Value (5%)    -2.913766\nCritical Value (10%)   -2.594624\nLength: 7, dtype: float64\nConclusion:====&gt;\nThe null hypothesis cannot be rejected\nThe data is not stationary\n\n\nWe can see in the result that we obtained the non-stationary series, because the p-value is greater than 5%.\nOne of the objectives of applying the ADF test is to know if our series is stationary, knowing the result of the ADF test, then we can determine the next step. For our case, it can be seen from the previous result that the time series is not stationary, so we will proceed to the next step, which is to differentiate our time series.\nWe are going to create a copy of our data, with the objective of investigating to find the stationarity in our time series.\nOnce we have made the copy of the time series, we are going to differentiate the time series, and then we will use the augmented Dickey Fuller test to investigate if our time series is stationary.\n\ndf1=df.copy()\ndf1['y_diff'] = df['y'].diff()\ndf1.dropna(inplace=True)\ndf1.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\ny_diff\n\n\n\n\n1\n1961-01-01\n69.760244\n1\n0.636341\n\n\n2\n1962-01-01\n69.149756\n1\n-0.610488\n\n\n3\n1963-01-01\n69.248049\n1\n0.098293\n\n\n4\n1964-01-01\n70.311707\n1\n1.063659\n\n\n5\n1965-01-01\n70.171707\n1\n-0.140000\n\n\n\n\n\n\n\nLet‚Äôs apply the Dickey Fuller test again to find out if our time series is already stationary.\n\nAugmented_Dickey_Fuller_Test_func(df1[\"y_diff\"],\"Life expectancy\")\n\nDickey-Fuller test results for columns: Life expectancy\nTest Statistic         -8.510100e+00\np-value                 1.173776e-13\nNo Lags Used            1.000000e+00\n                            ...     \nCritical Value (1%)    -3.550670e+00\nCritical Value (5%)    -2.913766e+00\nCritical Value (10%)   -2.594624e+00\nLength: 7, dtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\nWe can observe in the previous result that now if our time series is stationary, the p-value is less than 5%.\nNow our time series is stationary, that is, we have only differentiated 1 time, therefore, the order of our parameter \\(d=1\\).\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\n\n\nfig, axes = plt.subplots(2, 2, )\naxes[0, 0].plot(df1[\"y\"]); axes[0, 0].set_title('Original Series')\nplot_acf(df1[\"y\"], ax=axes[0, 1],lags=20)\n\naxes[1, 0].plot(df1[\"y\"].diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(df1[\"y\"].diff().dropna(), ax=axes[1, 1],lags=20)\n\n\nplt.show()\n\n\n\n\n\nFor the above data, we can see that the time series reaches stationarity with one orders of differencing."
  },
  {
    "objectID": "docs/models/arima.html#how-to-find-the-order-of-the-ar-term-p",
    "href": "docs/models/arima.html#how-to-find-the-order-of-the-ar-term-p",
    "title": "ARIMA Model",
    "section": "How to find the order of the AR term (p) ",
    "text": "How to find the order of the AR term (p) \n\nThe next step is to identify if the model needs any AR terms. We will find out the required number of AR terms by inspecting the Partial Autocorrelation (PACF) plot.\nPartial autocorrelation can be imagined as the correlation between the series and its lag, after excluding the contributions from the intermediate lags. So, PACF sort of conveys the pure correlation between a lag and the series. This way, we will know if that lag is needed in the AR term or not.\nPartial autocorrelation of lag (k) of a series is the coefficient of that lag in the autoregression equation of \\(Y\\).\n\n\\[Yt = \\alpha0 + \\alpha1 Y{t-1} + \\alpha2 Y{t-2} + \\alpha3 Y{t-3}\\]\n\nThat is, suppose, if \\(Y_t\\) is the current series and \\(Y_{t-1}\\) is the lag 1 of \\(Y\\), then the partial autocorrelation of lag 3 \\((Y_{t-3})\\) is the coefficient \\(\\alpha_3\\) of \\(Y_{t-3}\\) in the above equation.\nNow, we should find the number of AR terms. Any autocorrelation in a stationarized series can be rectified by adding enough AR terms. So, we initially take the order of AR term to be equal to as many lags that crosses the significance limit in the PACF plot.\n\n\nfig, axes = plt.subplots(1, 2)\naxes[0].plot(df1[\"y\"].diff()); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,5))\nplot_pacf(df1[\"y\"].diff().dropna(), ax=axes[1],lags=20)\n\nplt.show()\n\n\n\n\n\nWe can see that the PACF lag 1 is quite significant since it is well above the significance line. So, we will fix the value of p as 1."
  },
  {
    "objectID": "docs/models/arima.html#how-to-find-the-order-of-the-ma-term-q",
    "href": "docs/models/arima.html#how-to-find-the-order-of-the-ma-term-q",
    "title": "ARIMA Model",
    "section": "How to find the order of the MA term (q) ",
    "text": "How to find the order of the MA term (q) \n\nJust like how we looked at the PACF plot for the number of AR terms, we will look at the ACF plot for the number of MA terms. An MA term is technically, the error of the lagged forecast.\nThe ACF tells how many MA terms are required to remove any autocorrelation in the stationarized series.\nLet‚Äôs see the autocorrelation plot of the differenced series.\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nfig, axes = plt.subplots(1, 2)\naxes[0].plot(df1[\"y\"].diff()); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,1.2))\nplot_acf(df[\"y\"].diff().dropna(), ax=axes[1], lags=20)\n\nplt.show()\n\n\n\n\n\nWe can see that couple of lags are well above the significance line. So, we will fix q as 1. If there is any doubt, we will go with the simpler model that sufficiently explains the Y."
  },
  {
    "objectID": "docs/models/arima.html#how-to-handle-if-a-time-series-is-slightly-under-or-over-differenced",
    "href": "docs/models/arima.html#how-to-handle-if-a-time-series-is-slightly-under-or-over-differenced",
    "title": "ARIMA Model",
    "section": "How to handle if a time series is slightly under or over differenced ",
    "text": "How to handle if a time series is slightly under or over differenced \n\nIt may happen that the time series is slightly under differenced. Differencing it one more time makes it slightly over-differenced.\nIf the series is slightly under differenced, adding one or more additional AR terms usually makes it up. Likewise, if it is slightly over-differenced, we will try adding an additional MA term."
  },
  {
    "objectID": "docs/models/arima.html#implementation-of-arima-with-statsforecast",
    "href": "docs/models/arima.html#implementation-of-arima-with-statsforecast",
    "title": "ARIMA Model",
    "section": "Implementation of ARIMA with StatsForecast ",
    "text": "Implementation of ARIMA with StatsForecast \nNow, we have determined the values of p, d and q. We have everything needed to fit the ARIMA model. We will use the ARIMA() implementation in the statsforecast package.\nThe parameters found are: * For the autoregressive model, \\(p=1\\) * for the moving average model \\(q=1\\) * and for the stationarity of the model with a differential with an order \\(d=1\\)\nTherefore, the model that we are going to test is the ARIMA(1,1,1) model.\n\nfrom statsforecast.models import ARIMA\n\nTo also know more about the parameters of the functions of the ARIMA Model, they are listed below. For more information, visit the documentation\norder : tuple (default=(0, 0, 0))\n        A specification of the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order.\n    season_length : int (default=1)\n        Number of observations per unit of time. Ex: 24 Hourly data.\n    seasonal_order : tuple (default=(0, 0, 0))\n        A specification of the seasonal part of the ARIMA model.\n        (P, D, Q) for the  AR order, the degree of differencing, the MA order.\n    include_mean : bool (default=True)\n        Should the ARIMA model include a mean term?\n        The default is True for undifferenced series, False for differenced ones (where a mean would not affect the fit nor predictions).\n    include_drift : bool (default=False)\n        Should the ARIMA model include a linear drift term?\n        (i.e., a linear regression with ARIMA errors is fitted.)\n    include_constant : bool, optional (default=None)\n        If True, then includ_mean is set to be True for undifferenced series and include_drift is set to be True for differenced series.\n        Note that if there is more than one difference taken, no constant is included regardless of the value of this argument.\n        This is deliberate as otherwise quadratic and higher order polynomial trends would be induced.\n    blambda : float, optional (default=None)\n        Box-Cox transformation parameter.\n    biasadj : bool (default=False)\n        Use adjusted back-transformed mean Box-Cox.\n    method : str (default='CSS-ML')\n        Fitting method: maximum likelihood or minimize conditional sum-of-squares.\n        The default (unless there are missing values) is to use conditional-sum-of-squares to find starting values, then maximum likelihood.\n    fixed : dict, optional (default=None)\n        Dictionary containing fixed coefficients for the arima model. Example: `{'ar1': 0.5, 'ma2': 0.75}`.\n        For autoregressive terms use the `ar{i}` keys. For its seasonal version use `sar{i}`.\n        For moving average terms use the `ma{i}` keys. For its seasonal version use `sma{i}`.\n        For intercept and drift use the `intercept` and `drift` keys.\n        For exogenous variables use the `ex_{i}` keys.\n    alias : str\n        Custom name of the model.\n    prediction_intervals : Optional[ConformalIntervals]\n        Information to compute conformal prediction intervals.\n        By default, the model will compute the native prediction\n        intervals.\n\nInstantiate the model\n\narima = ARIMA(order=(1, 1, 1), season_length=1)\n\n\n\nFit the Model\n\narima = arima.fit(y=df[\"y\"].values)\n\n\n\nMaking the predictions\n\ny_hat_dict = arima.predict(h=6,)\ny_hat_dict\n\n{'mean': array([83.20155301, 83.20016307, 83.20064702, 83.20047852, 83.20053719,\n        83.20051676])}\n\n\nWe can make the predictions by adding the confidence interval, for example with 95%.\n\ny_hat_dict2 = arima.predict(h=6,level=[95])\ny_hat_dict2\n\n{'mean': array([83.20155301, 83.20016307, 83.20064702, 83.20047852, 83.20053719,\n        83.20051676]),\n 'lo-95': 0    82.399545\n 1    82.072363\n 2    81.820183\n 3    81.607471\n 4    81.420010\n 5    81.250474\n Name: 95%, dtype: float64,\n 'hi-95': 0    84.003561\n 1    84.327963\n 2    84.581111\n 3    84.793486\n 4    84.981064\n 5    85.150560\n Name: 95%, dtype: float64}\n\n\nYou can see that the result that has been extracted in the predictions or in any other method that we use from now on with the Arima model is a dictionary. To extract that result we can use the .get() function, which will help us to be able to extract the result of each part of the dictionary of each of the methods that we use.\n\n\nARIMA.forecast method\nMemory efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\nY_hat_df=arima.forecast(y=df[\"y\"].values, h=6, fitted=True)\nY_hat_df\n\n{'mean': array([83.20155301, 83.20016307, 83.20064702, 83.20047852, 83.20053719,\n        83.20051676]),\n 'fitted': array([69.05477857, 69.12394817, 69.75299437, 69.15912019, 69.24378043,\n        70.30108758, 70.1768715 , 70.91580642, 70.95970624, 70.7809505 ,\n        70.81126886, 71.55054441, 71.80679168, 72.07233443, 72.0279183 ,\n        72.72583278, 72.65118662, 72.98674108, 73.36216446, 73.69027839,\n        74.00014861, 73.94469901, 74.34873167, 74.8111531 , 74.64339332,\n        75.37995777, 75.47302934, 75.76655702, 76.2158311 , 76.37025745,\n        76.81458465, 76.97067731, 77.01897755, 77.41515795, 77.71998951,\n        77.92034484, 78.16845267, 78.51873753, 78.82204435, 78.97468575,\n        79.41961419, 79.77564854, 80.12368405, 80.2291665 , 79.9857536 ,\n        80.77049462, 80.7862653 , 81.27613438, 81.43472012, 81.48459197,\n        81.63513184, 82.03254064, 82.18745311, 82.23856226, 82.68528462,\n        83.08738006, 82.55106033, 83.23355629, 82.95319957, 83.33949704])}\n\n\nAs the result of the predictions that we have generated is a dictionary, to make use of that result we are going to save it in .DataFrame(). First we are going to generate a range of dates, then we use the .get() function to extract the predictions and then we concatenate them and save them in a DataFrame.\n\nforecast=pd.Series(pd.date_range(\"2014-01-01\", freq=\"ys\", periods=6))\nforecast=pd.DataFrame(forecast)\nforecast.columns=[\"ds\"]\nforecast\n\n\n\n\n\n\n\n\nds\n\n\n\n\n0\n2014-01-01\n\n\n1\n2015-01-01\n\n\n2\n2016-01-01\n\n\n3\n2017-01-01\n\n\n4\n2018-01-01\n\n\n5\n2019-01-01\n\n\n\n\n\n\n\n\ndf=df.set_index(\"ds\")\n\n\nforecast[\"unique_id\"]=\"1\"\nforecast[\"hat\"]=y_hat_dict.get(\"mean\")\nforecast[\"lo-95\"]=y_hat_dict2.get(\"lo-95\")\nforecast[\"hi-95\"]=y_hat_dict2.get(\"hi-95\")\nforecast=forecast.set_index(\"ds\")\nforecast\n\n\n\n\n\n\n\n\nunique_id\nhat\nlo-95\nhi-95\n\n\nds\n\n\n\n\n\n\n\n\n2014-01-01\n1\n83.201553\n82.399545\n84.003561\n\n\n2015-01-01\n1\n83.200163\n82.072363\n84.327963\n\n\n2016-01-01\n1\n83.200647\n81.820183\n84.581111\n\n\n2017-01-01\n1\n83.200479\n81.607471\n84.793486\n\n\n2018-01-01\n1\n83.200537\n81.420010\n84.981064\n\n\n2019-01-01\n1\n83.200517\n81.250474\n85.150560\n\n\n\n\n\n\n\nOnce the predictions have been generated we can perform a visualization to see the generated behavior of our model.\n\n_, ax = plt.subplots(1, 1)\ndf_plot = pd.concat([df, forecast])\ndf_plot[['y', 'hat']].plot(ax=ax, linewidth=2)\nax.fill_between(df_plot.index, \n                df_plot['lo-95'], \n                df_plot['hi-95'],\n                alpha=.35,\n                color='orange',\n                label='ARIMA-level-95')\nax.set_title('', fontsize=22)\nax.set_ylabel('', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=12)\nax.legend(prop={'size': 15})\nax.grid(True)"
  },
  {
    "objectID": "docs/models/arima.html#model-evaluation",
    "href": "docs/models/arima.html#model-evaluation",
    "title": "ARIMA Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nThe commonly used accuracy metrics to judge forecasts are:\n\nMean Absolute Percentage Error (MAPE)\nMean Error (ME)\nMean Absolute Error (MAE)\nMean Percentage Error (MPE)\nRoot Mean Squared Error (RMSE)\nCorrelation between the Actual and the Forecast (corr)\n\n\ndf.reset_index(\"ds\", inplace=True)\nY_train_df = df[df.ds&lt;='2013-01-01'] \nY_test_df = df[df.ds&gt;'2013-01-01'] \n\nY_train_df.shape, Y_test_df.shape\n\n((54, 3), (6, 3))\n\n\n\nfrom sklearn import metrics\n\ndef model_evaluation(y_true, y_pred, Model):\n    \n    def mean_absolute_percentage_error(y_true, y_pred): \n        y_true, y_pred = np.array(y_true), np.array(y_pred)\n        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n    print (f'Model Evaluation: {Model}')\n    print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}')\n    print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}')\n    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')\n    print(f'MAPE is : {mean_absolute_percentage_error(y_true, y_pred)}')\n    print(f'R2 is : {metrics.r2_score(y_true, y_pred)}')\n    print(f'corr is : {np.corrcoef(y_true, y_pred)[0,1]}',end='\\n\\n')\n\n\nmodel_evaluation(Y_test_df[\"y\"], forecast[\"hat\"], \"Arima\")\n\nModel Evaluation: Arima\nMSE is : 0.08846533957222678\nMAE is : 0.20228704669491057\nRMSE is : 0.29743123503127034\nMAPE is : 0.24430769688205894\nR2 is : -0.2792859149048943\ncorr is : 0.3634615475289058\n\n\n\n\nAround 24.43% MAPE implies the model is about 75.57% accurate in predicting the next 6 observations. Now we know how to build an ARIMA model manually."
  },
  {
    "objectID": "docs/models/arima.html#acknowledgements",
    "href": "docs/models/arima.html#acknowledgements",
    "title": "ARIMA Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/arima.html#references",
    "href": "docs/models/arima.html#references",
    "title": "ARIMA Model",
    "section": "References ",
    "text": "References \n\nNixtla-Arima\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù."
  },
  {
    "objectID": "docs/models/autoces.html",
    "href": "docs/models/autoces.html",
    "title": "AutoCES Model",
    "section": "",
    "text": "Introduction\nComplex Exponential Smoothing\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoCES with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/autoces.html#table-of-contents",
    "href": "docs/models/autoces.html#table-of-contents",
    "title": "AutoCES Model",
    "section": "",
    "text": "Introduction\nComplex Exponential Smoothing\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoCES with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/autoces.html#introduction",
    "href": "docs/models/autoces.html#introduction",
    "title": "AutoCES Model",
    "section": "Introduction ",
    "text": "Introduction \nExponential smoothing has been one of the most popular forecasting methods used to support various decisions in organizations, in activities such as inventory management, scheduling, revenue management, and other areas. Although its relative simplicity and transparency have made it very attractive for research and practice, identifying the underlying trend remains challenging with significant impact on the resulting accuracy. This has resulted in the development of various modifications of trend models, introducing a model selection problem. With the aim of addressing this problem, we propose the complex exponential smoothing (CES), based on the theory of functions of complex variables. The basic CES approach involves only two parameters and does not require a model selection procedure. Despite these simplifications, CES proves to be competitive with, or even superior to existing methods. We show that CES has several advantages over conventional exponential smoothing models: it can model and forecast both stationary and non-stationary processes, and CES can capture both level and trend cases, as defined in the conventional exponential smoothing classification. CES is evaluated on several forecasting competition datasets, demonstrating better performance than established benchmarks. We conclude that CES has desirable features for time series modeling and opens new promising avenues for research."
  },
  {
    "objectID": "docs/models/autoces.html#loading-libraries-and-data",
    "href": "docs/models/autoces.html#loading-libraries-and-data",
    "title": "AutoCES Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport pandas as pd\n\nimport scipy.stats as stats\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/Esperanza_vida.csv\", usecols=[1,2])\ndf.head()\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n1960-01-01\n69.123902\n\n\n1\n1961-01-01\n69.760244\n\n\n2\n1962-01-01\n69.149756\n\n\n3\n1963-01-01\n69.248049\n\n\n4\n1964-01-01\n70.311707\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1960-01-01\n69.123902\n1\n\n\n1\n1961-01-01\n69.760244\n1\n\n\n2\n1962-01-01\n69.149756\n1\n\n\n3\n1963-01-01\n69.248049\n1\n\n\n4\n1964-01-01\n70.311707\n1\n\n\n\n\n\n\n\nNow, let‚Äôs now check the last few rows of our time series using the .tail() function.\n\nprint(df.dtypes)\n\nds            object\ny            float64\nunique_id     object\ndtype: object\n\n\nWe need to convert the ds from object type to datetime.\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/autoces.html#explore-data-with-the-plot-method",
    "href": "docs/models/autoces.html#explore-data-with-the-plot-method",
    "title": "AutoCES Model",
    "section": "Explore data with the plot method ",
    "text": "Explore data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df, engine=\"matplotlib\")\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=20, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\n# Grafico\nplot_pacf(df[\"y\"],  lags=20, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\n#plt.savefig(\"Gr√°fico de Densidad y qq\")\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"add\", period=1)\na.plot();"
  },
  {
    "objectID": "docs/models/autoces.html#split-the-data-into-training-and-testing",
    "href": "docs/models/autoces.html#split-the-data-into-training-and-testing",
    "title": "AutoCES Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our model.\nData to test our model.\n\nFor the test data we will use the last 12 months to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2013-01-01'] \ntest = df[df.ds&gt;'2013-01-01']\n\n\ntrain.shape, test.shape\n\n((54, 3), (6, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.show()"
  },
  {
    "objectID": "docs/models/autoces.html#implementation-of-autoces-with-statsforecast",
    "href": "docs/models/autoces.html#implementation-of-autoces-with-statsforecast",
    "title": "AutoCES Model",
    "section": "Implementation of AutoCES with StatsForecast ",
    "text": "Implementation of AutoCES with StatsForecast \nTo also know more about the parameters of the functions of the AutoCES Model, they are listed below. For more information, visit the documentation\nmodel : str\n    Controlling state-space-equations.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoCES\n\nfrom statsforecast.arima import arima_string\n\n\n\nInstantiate Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods) by the master, Rob Hyndmann, can be useful season_length\nNote\nAutomatically selects the best Complex Exponential Smoothing model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(S\\) simple, \\(P\\) parial, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the kind of CES model: \\(N\\) for simple CES (withous seasonality), \\(S\\) for simple seasonality (lagged CES), \\(P\\) for partial seasonality (without complex part), \\(F\\) for full seasonality (lagged CES with real and complex seasonal parts).\nIf the component is selected as \\(Z\\), it operates as a placeholder to ask the AutoCES model to figure out the best parameter.\n\nseason_length = 1 # year data \nhorizon = len(test) # number of predictions\n\n# We call the model that we are going to use\nmodels = [AutoCES(season_length=season_length)]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='YS', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[CES])\n\n\n\nresult=sf.fitted_[0,0].model_\nprint(result.keys())\nprint(result['fit'])\n\ndict_keys(['loglik', 'aic', 'bic', 'aicc', 'mse', 'amse', 'fit', 'fitted', 'residuals', 'm', 'states', 'par', 'n', 'seasontype', 'sigma2', 'actual_residuals'])\nresults(x=array([1.63706552, 1.00511519]), fn=76.78049826760919, nit=27, simplex=array([[1.63400329, 1.00510199],\n       [1.63706552, 1.00511519],\n       [1.63638944, 1.00512037]]))\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\n-0.727729\n\n\n1\n0.144552\n\n\n2\n-0.762086\n\n\n...\n...\n\n\n51\n-0.073258\n\n\n52\n-0.234578\n\n\n53\n0.109990\n\n\n\n\n54 rows √ó 1 columns\n\n\n\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\n# plot[1,1]\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\n# plot\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\n# plot\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\n# plot\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n# Prediction\nY_hat = sf.forecast(horizon, fitted=True)\n\nY_hat\n\n\n\n\n\n\n\n\nds\nCES\n\n\nunique_id\n\n\n\n\n\n\n1\n2014-01-01\n82.906075\n\n\n1\n2015-01-01\n83.166687\n\n\n1\n2016-01-01\n83.424744\n\n\n1\n2017-01-01\n83.685760\n\n\n1\n2018-01-01\n83.946213\n\n\n1\n2019-01-01\n84.208359\n\n\n\n\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nCES\n\n\nunique_id\n\n\n\n\n\n\n\n1\n1960-01-01\n69.123901\n69.851631\n\n\n1\n1961-01-01\n69.760246\n69.615692\n\n\n1\n1962-01-01\n69.149757\n69.911842\n\n\n1\n1963-01-01\n69.248047\n69.657822\n\n\n1\n1964-01-01\n70.311707\n69.601196\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nCES\nCES-lo-95\nCES-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2014-01-01\n82.906075\n82.342484\n83.454018\n\n\n1\n2015-01-01\n83.166687\n82.604027\n83.717270\n\n\n1\n2016-01-01\n83.424744\n82.858574\n83.975868\n\n\n1\n2017-01-01\n83.685760\n83.118942\n84.239578\n\n\n1\n2018-01-01\n83.946213\n83.376907\n84.501137\n\n\n1\n2019-01-01\n84.208359\n83.637741\n84.765411\n\n\n\n\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nCES\n\n\n\n\n0\n1\n2014-01-01\n82.906075\n\n\n1\n1\n2015-01-01\n83.166687\n\n\n2\n1\n2016-01-01\n83.424744\n\n\n3\n1\n2017-01-01\n83.685760\n\n\n4\n1\n2018-01-01\n83.946213\n\n\n5\n1\n2019-01-01\n84.208359\n\n\n\n\n\n\n\n\n# Merge the forecasts with the true values\ntest['unique_id'] = test['unique_id'].astype(int)\nY_hat = test.merge(Y_hat, how='left', on=['unique_id', 'ds'])\nY_hat\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nCES\n\n\n\n\n0\n2014-01-01\n83.090244\n1\n82.906075\n\n\n1\n2015-01-01\n82.543902\n1\n83.166687\n\n\n2\n2016-01-01\n83.243902\n1\n83.424744\n\n\n3\n2017-01-01\n82.946341\n1\n83.685760\n\n\n4\n2018-01-01\n83.346341\n1\n83.946213\n\n\n5\n2019-01-01\n83.197561\n1\n84.208359\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat]).set_index('ds')\nplot_df[['y', \"CES\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Year ', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nCES\n\n\nunique_id\n\n\n\n\n\n\n1\n2014-01-01\n82.906075\n\n\n1\n2015-01-01\n83.166687\n\n\n1\n2016-01-01\n83.424744\n\n\n1\n2017-01-01\n83.685760\n\n\n1\n2018-01-01\n83.946213\n\n\n1\n2019-01-01\n84.208359\n\n\n\n\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[95]) \n\nforecast_df\n\n\n\n\n\n\n\n\nds\nCES\nCES-lo-95\nCES-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2014-01-01\n82.906075\n82.342484\n83.454018\n\n\n1\n2015-01-01\n83.166687\n82.604027\n83.717270\n\n\n1\n2016-01-01\n83.424744\n82.858574\n83.975868\n\n\n1\n2017-01-01\n83.685760\n83.118942\n84.239578\n\n\n1\n2018-01-01\n83.946213\n83.376907\n84.501137\n\n\n1\n2019-01-01\n84.208359\n83.637741\n84.765411\n\n\n\n\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nCES\nCES-lo-95\nCES-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n1960-01-01\n69.123902\n1\nNaN\nNaN\nNaN\n\n\n1961-01-01\n69.760244\n1\nNaN\nNaN\nNaN\n\n\n1962-01-01\n69.149756\n1\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n2017-01-01\nNaN\nNaN\n83.685760\n83.118942\n84.239578\n\n\n2018-01-01\nNaN\nNaN\n83.946213\n83.376907\n84.501137\n\n\n2019-01-01\nNaN\nNaN\n84.208359\n83.637741\n84.765411\n\n\n\n\n66 rows √ó 5 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2)\n    colors = ['orange', 'black', 'green']\n    for model, color in zip(models, colors):\n        ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-95'], \n                        df_plot[f'{model}-hi-95'],\n                        alpha=.35,\n                        color=color,\n                        label=f'{model}-level-90')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel('', fontsize=20)\n    ax.set_xlabel('Timestamp [t]', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid(True)\n\n\nplot_forecasts(train, test, forecast_df, models=['CES'])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df, level=[95])"
  },
  {
    "objectID": "docs/models/autoces.html#cross-validation",
    "href": "docs/models/autoces.html#cross-validation",
    "title": "AutoCES Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=12,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nCES\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n1984-01-01\n1983-01-01\n75.389511\n74.952705\n\n\n1\n1985-01-01\n1983-01-01\n75.470734\n75.161736\n\n\n1\n1986-01-01\n1983-01-01\n75.770729\n75.377945\n\n\n1\n1987-01-01\n1983-01-01\n76.219513\n75.590378\n\n\n1\n1988-01-01\n1983-01-01\n76.370735\n75.806343"
  },
  {
    "objectID": "docs/models/autoces.html#model-evaluation",
    "href": "docs/models/autoces.html#model-evaluation",
    "title": "AutoCES Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, AutoCES.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"CES\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  0.40604722\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\ndef evaluate_performace(y_hist, y_hat, model):\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_hat['y'].values, \n                                                y_hat[model].values, \n                                                y_hist['y'].values, seasonality=24)\n        else:\n            evaluation[model][metric_name] = metric(y_hat['y'].values, y_hat[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, Y_hat, model=\"CES\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nCES\n0.556314\n0.669916\n0.087037\n0.630183\n0.667133"
  },
  {
    "objectID": "docs/models/autoces.html#acknowledgements",
    "href": "docs/models/autoces.html#acknowledgements",
    "title": "AutoCES Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/autoces.html#references",
    "href": "docs/models/autoces.html#references",
    "title": "AutoCES Model",
    "section": "References ",
    "text": "References \n\nNixtla-AutoCES\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù.\nComplex exponential smoothing"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html",
    "href": "docs/models/multipleseasonaltrend.html",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "",
    "text": "Introduction\nIMAPA Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of IMAPA with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#table-of-contents",
    "href": "docs/models/multipleseasonaltrend.html#table-of-contents",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "",
    "text": "Introduction\nIMAPA Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of IMAPA with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#introduction",
    "href": "docs/models/multipleseasonaltrend.html#introduction",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "Introduction ",
    "text": "Introduction \nThe MSTL model (Multiple Seasonal-Trend decomposition using LOESS) is a method used to decompose a time series into its seasonal, trend and residual components. This approach is based on the use of LOESS (Local Regression Smoothing) to estimate the components of the time series.\nThe MSTL decomposition is an extension of the classic seasonal-trend decomposition method (also known as Holt-Winters decomposition), which is designed to handle situations where multiple seasonal patterns exist in the data. This can occur, for example, when a time series exhibits daily, weekly, and yearly patterns simultaneously.\nThe MSTL decomposition process is performed in several stages:\n\nTrend estimation: LOESS is used to estimate the trend component of the time series. LOESS is a non-parametric smoothing method that locally fits data and allows complex trend patterns to be captured.\nEstimation of seasonal components: Seasonal decomposition techniques are applied to identify and model the different seasonal patterns present in the data. This involves extracting and modeling seasonal components, such as daily, weekly, or yearly patterns.\nEstimation of the residuals: The residuals are calculated as the difference between the original time series and the sum of the estimates of trend and seasonal components. Residuals represent variation not explained by trend and seasonal patterns and may contain additional information or noise.\n\nMSTL decomposition allows you to analyze and understand the different components of a time series in more detail, which can make it easier to forecast and detect patterns or anomalies. Furthermore, the use of LOESS provides flexibility to adapt to different trend and seasonal patterns present in the data.\nIt is important to note that the MSTL model is only one of the available approaches for time series decomposition and that its choice will depend on the specific characteristics of the data and the application context."
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#mstl",
    "href": "docs/models/multipleseasonaltrend.html#mstl",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "MSTL ",
    "text": "MSTL \nAn important objective in time series analysis is the decomposition of a series into a set of non-observable (latent) components that can be associated with different types of temporal variations. The idea of time series decomposition is very old and was used for the calculation of planetary orbits by seventeenth century astronomers. Persons was the first to state explicitly the assumptions of unobserved components. As Persons saw it, time series was composed of four types of fluctuations:\n\na long-term tendency or secular trend;\ncyclical movements superimposed upon the long-term trend. These cycles appear to reach their peaks during periods of industrial prosperity and their troughs during periods of depressions, their rise and fall constituting the business cycle;\na seasonal movement within each year, the shape of which depends on the nature of the series;\nresidual variations due to changes impacting individual variables or other major events, such as wars and national catastrophes affecting a number of variables.\n\nTraditionally, the four variations have been assumed to be mutually independent from one another and specified by means of an additive decomposition model:\n\\[y_t= T_t +C_t +S_t +I_t, t=1,\\ \\cdots, n  \\tag 1\\]\nwhere \\(y_t\\) denotes the observed series at time \\(t\\), \\(T_t\\) the long-term trend, \\(C_t\\) the business cycle, \\(S_t\\) seasonality, and \\(I_t\\) the irregulars.\nIf there is dependence among the latent components, this relationship is specified through a multiplicative model\n\\[y_t= T_t \\times C_t \\times S_t \\times I_t, t=1,\\ \\cdots, n  \\tag 2\\]\nwhere now \\(S_t\\) and \\(I_t\\) are expressed in proportion to the trend-cycle \\(T_t \\times C_t\\) . In some cases, mixed additive‚Äìmultiplicative models are used.\n\nLOESS (Local Regression Smoothing)\nLOESS is a nonparametric smoothing method used to estimate a smooth function that locally fits the data. For each point in the time series, LOESS performs a weighted regression using nearest neighbors.\nThe LOESS calculation involves the following steps:\n\nFor each point t in the time series, a nearest neighbor window is selected.\nWeights are assigned to neighbors based on their proximity to t, using a weighting function, such as the Gaussian kernel.\nA weighted regression is performed using the neighbors and their assigned weights.\nThe fitted value for point t is obtained based on local regression.\nThe process is repeated for all points in the time series, thus obtaining a smoothed estimate of the trend.\n\n\n\nMSTL General Properties\nThe MSTL model (Multiple Seasonal-Trend decomposition using LOESS) has several properties that make it useful in time series analysis. Here is a list of some of its properties:\n\nDecomposition of multiple seasonal components: The MSTL model is capable of handling time series that exhibit multiple seasonal patterns simultaneously. You can effectively identify and model different seasonal components present in the data.\nFlexibility in detecting complex trends: Thanks to the use of LOESS, the MSTL model can capture complex trend patterns in the data. This includes non-linear trends and abrupt changes in the time series.\nAdaptability to different seasonal frequencies: The MSTL model is capable of handling data with different seasonal frequencies, such as daily, weekly, monthly, or even yearly patterns. You can identify and model seasonal patterns of different cycle lengths. (see) Seasonal periods\n\n\n\n\nFrecuencia\n\n\n\n\n\n\n\n\nData\nMinute\nHour\nDay\nWeek\nYear\n\n\n\n\nDaily\n\n\n\n7\n365.25\n\n\nHourly\n\n\n24\n168\n8766\n\n\nHalf-hourly\n\n\n48\n336\n17532\n\n\nMinutes\n\n60\n1440\n10080\n525960\n\n\nSeconds\n60\n3600\n86400\n604800\n31557600\n\n\n\n\nAbility to smooth noise and outliers: The smoothing process used in LOESS allows to reduce the impact of noise and outliers in the time series. This can improve detection of underlying patterns and make it easier to analyze trend and seasonality.\nImproved forecasting: By decomposing the time series into seasonal, trend, and residual components, the MSTL model can provide more accurate forecasts. Forecasts can be generated by extrapolating trend and seasonal patterns into the future, and adding the stochastic residuals.\nMore detailed interpretation and analysis: The MSTL decomposition allows you to analyze and understand the different components of the time series in a more detailed way. This facilitates the identification of seasonal patterns, changes in trend, and the evaluation of residual variability.\nEfficient Implementation: Although the specific implementation may vary, the MSTL model can be calculated efficiently, especially when LOESS is used in combination with optimized calculation algorithms.\n\nThese properties make the MSTL model a useful tool for exploratory time series analysis, data forecasting, and pattern detection in the presence of multiple seasonal components and complex trends."
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#loading-libraries-and-data",
    "href": "docs/models/multipleseasonaltrend.html#loading-libraries-and-data",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nimport pandas as pd\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/ads.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nTime\nAds\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n\n\n1\n2017-09-13T01:00:00\n79885\n\n\n2\n2017-09-13T02:00:00\n89325\n\n\n3\n2017-09-13T03:00:00\n101930\n\n\n4\n2017-09-13T04:00:00\n121630\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n1\n\n\n1\n2017-09-13T01:00:00\n79885\n1\n\n\n2\n2017-09-13T02:00:00\n89325\n1\n\n\n3\n2017-09-13T03:00:00\n101930\n1\n\n\n4\n2017-09-13T04:00:00\n121630\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#explore-data-with-the-plot-method",
    "href": "docs/models/multipleseasonaltrend.html#explore-data-with-the-plot-method",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\nAutocorrelation (ACF) and partial autocorrelation (PACF) plots are statistical tools used to analyze time series. ACF charts show the correlation between the values of a time series and their lagged values, while PACF charts show the correlation between the values of a time series and their lagged values, after the effect of previous lagged values has been removed.\nACF and PACF charts can be used to identify the structure of a time series, which can be helpful in choosing a suitable model for the time series. For example, if the ACF chart shows a repeating peak and valley pattern, this indicates that the time series is stationary, meaning that it has the same statistical properties over time. If the PACF chart shows a pattern of rapidly decreasing spikes, this indicates that the time series is invertible, meaning it can be reversed to get a stationary time series.\nThe importance of the ACF and PACF charts is that they can help analysts better understand the structure of a time series. This understanding can be helpful in choosing a suitable model for the time series, which can improve the ability to predict future values of the time series.\nTo analyze ACF and PACF charts:\n\nLook for patterns in charts. Common patterns include repeating peaks and valleys, sawtooth patterns, and plateau patterns.\nCompare ACF and PACF charts. The PACF chart generally has fewer spikes than the ACF chart.\nConsider the length of the time series. ACF and PACF charts for longer time series will have more spikes.\nUse a confidence interval. The ACF and PACF plots also show confidence intervals for the autocorrelation values. If an autocorrelation value is outside the confidence interval, it is likely to be significant.\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\n# Grafico\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=24,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#split-the-data-into-training-and-testing",
    "href": "docs/models/multipleseasonaltrend.html#split-the-data-into-training-and-testing",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our MSTL Model. 2. Data to test our model\nFor the test data we will use the last 30 Hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2017-09-20 17:00:00'] \ntest = df[df.ds&gt;'2017-09-20 17:00:00']\n\n\ntrain.shape, test.shape\n\n((186, 3), (30, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\",linewidth=2)\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\", linewidth=2, color=\"yellow\")\nplt.title(\"Ads watched (hourly data)\");\nplt.xlabel(\"Hours\")\nplt.show()"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#implementation-of-mstl-method-with-statsforecast",
    "href": "docs/models/multipleseasonaltrend.html#implementation-of-mstl-method-with-statsforecast",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "Implementation of MSTL Method with StatsForecast ",
    "text": "Implementation of MSTL Method with StatsForecast \nTo also know more about the parameters of the functions of the MSTL Model, they are listed below. For more information, visit the [documentation][here](https://nixtla.github.io/statsforecast/src/core/models.html#multiple-seasonalities).\nseason_length : Union[int, List[int]\n    Number of observations per unit of time. For multiple seasonalities use a list.\ntrend_forecaster : model\n    StatsForecast model used to forecast the trend component.\nalias : str\n    Custom name of the model.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\nFirst, we must define the model parameters. As mentioned before, the Candy production load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] for season length. The trend component will be forecasted with an AutoARIMA model. (You can also try with: AutoTheta, AutoCES, and AutoETS)\n\nfrom statsforecast.utils import ConformalIntervals\nhorizon = len(test) # number of predictions\n\nmodels = [MSTL(season_length=[24, 8766], # seasonalities of the time series \ntrend_forecaster=AutoARIMA(prediction_intervals=ConformalIntervals(n_windows=3, h=horizon)))]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit Model\n\nsf.fit()\n\nStatsForecast(models=[MSTL])\n\n\nLet‚Äôs see the results of our MSTL Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n\n\n\n\n\n\n\ndata\ntrend\nseasonal24\nseasonal8766\nremainder\n\n\n\n\n0\n80115.0\n14303.922343\n-42577.491717\n108388.569374\n5.456968e-12\n\n\n1\n79885.0\n14303.922343\n-44238.368960\n109819.446617\n-7.275958e-12\n\n\n2\n89325.0\n14303.922343\n-35702.091693\n110723.169350\n7.275958e-12\n\n\n...\n...\n...\n...\n...\n...\n\n\n213\n103080.0\n14303.922343\n-13544.680251\n102320.757908\n5.456968e-12\n\n\n214\n95155.0\n14303.922343\n-23707.619714\n104558.697371\n1.818989e-12\n\n\n215\n80285.0\n14303.922343\n-38570.551564\n104551.629220\n3.637979e-12\n\n\n\n\n216 rows √ó 5 columns\n\n\n\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hours ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\nY_hat = sf.forecast(horizon, fitted=True)\nY_hat\n\n\n\n\n\n\n\n\nds\nMSTL\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n81048.031250\n\n\n1\n2017-09-22 01:00:00\n82047.953125\n\n\n1\n2017-09-22 02:00:00\n91127.234375\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n107635.890625\n\n\n1\n2017-09-23 04:00:00\n134776.953125\n\n\n1\n2017-09-23 05:00:00\n119654.390625\n\n\n\n\n30 rows √ó 2 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nMSTL\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2017-09-13 00:00:00\n80115.0\n80100.695312\n\n\n1\n2017-09-13 01:00:00\n79885.0\n79885.000000\n\n\n1\n2017-09-13 02:00:00\n89325.0\n89325.000000\n\n\n1\n2017-09-13 03:00:00\n101930.0\n101930.000000\n\n\n1\n2017-09-13 04:00:00\n121630.0\n121630.000000\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-95\nMSTL-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n81048.031250\n81048.031250\n81048.031250\n\n\n1\n2017-09-22 01:00:00\n82047.953125\n82047.953125\n82047.953125\n\n\n1\n2017-09-22 02:00:00\n91127.234375\n91127.234375\n91127.234375\n\n\n...\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n107635.890625\n107635.890625\n107635.890625\n\n\n1\n2017-09-23 04:00:00\n134776.953125\n134776.953125\n134776.953125\n\n\n1\n2017-09-23 05:00:00\n119654.390625\n119654.390625\n119654.390625\n\n\n\n\n30 rows √ó 4 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nMSTL\n\n\n\n\n0\n1\n2017-09-22 00:00:00\n81048.031250\n\n\n1\n1\n2017-09-22 01:00:00\n82047.953125\n\n\n2\n1\n2017-09-22 02:00:00\n91127.234375\n\n\n...\n...\n...\n...\n\n\n27\n1\n2017-09-23 03:00:00\n107635.890625\n\n\n28\n1\n2017-09-23 04:00:00\n134776.953125\n\n\n29\n1\n2017-09-23 05:00:00\n119654.390625\n\n\n\n\n30 rows √ó 3 columns\n\n\n\n\n# Concat the forecasts with the true values\nY_hat1 = pd.concat([df,Y_hat],  keys=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nMSTL\n\n\n\n\nunique_id\n0\n2017-09-13 00:00:00\n80115.0\n1\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\n1\nNaN\n\n\n2\n2017-09-13 02:00:00\n89325.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\nds\n27\n2017-09-23 03:00:00\nNaN\n1\n107635.890625\n\n\n28\n2017-09-23 04:00:00\nNaN\n1\n134776.953125\n\n\n29\n2017-09-23 05:00:00\nNaN\n1\n119654.390625\n\n\n\n\n246 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df, Y_hat1]).set_index('ds')\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[\"MSTL\"].plot(ax=ax, linewidth=2, color=\"yellow\")\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Ads watched (hourly data)', fontsize=20)\nax.set_xlabel('Monthly [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hours ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nMSTL\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n81048.031250\n\n\n1\n2017-09-22 01:00:00\n82047.953125\n\n\n1\n2017-09-22 02:00:00\n91127.234375\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n107635.890625\n\n\n1\n2017-09-23 04:00:00\n134776.953125\n\n\n1\n2017-09-23 05:00:00\n119654.390625\n\n\n\n\n30 rows √ó 2 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \nforecast_df\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-95\nMSTL-lo-80\nMSTL-hi-80\nMSTL-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n81048.031250\n81048.031250\n81048.031250\n81048.031250\n81048.031250\n\n\n1\n2017-09-22 01:00:00\n82047.953125\n82047.953125\n82047.953125\n82047.953125\n82047.953125\n\n\n1\n2017-09-22 02:00:00\n91127.234375\n91127.234375\n91127.234375\n91127.234375\n91127.234375\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n107635.890625\n107635.890625\n107635.890625\n107635.890625\n107635.890625\n\n\n1\n2017-09-23 04:00:00\n134776.953125\n134776.953125\n134776.953125\n134776.953125\n134776.953125\n\n\n1\n2017-09-23 05:00:00\n119654.390625\n119654.390625\n119654.390625\n119654.390625\n119654.390625\n\n\n\n\n30 rows √ó 6 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nMSTL\nMSTL-lo-95\nMSTL-lo-80\nMSTL-hi-80\nMSTL-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n107635.890625\n107635.890625\n107635.890625\n107635.890625\n107635.890625\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n134776.953125\n134776.953125\n134776.953125\n134776.953125\n134776.953125\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n119654.390625\n119654.390625\n119654.390625\n119654.390625\n119654.390625\n\n\n\n\n246 rows √ó 7 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds')\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nMSTL\nMSTL-lo-95\nMSTL-lo-80\nMSTL-hi-80\nMSTL-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n107635.890625\n107635.890625\n107635.890625\n107635.890625\n107635.890625\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n134776.953125\n134776.953125\n134776.953125\n134776.953125\n134776.953125\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n119654.390625\n119654.390625\n119654.390625\n119654.390625\n119654.390625\n\n\n\n\n246 rows √ó 7 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_pred, models):\n  _, ax = plt.subplots(1, 1, figsize = (20, 7))\n  df_plot = pd.concat([y_hist, y_pred]).set_index('ds').tail(12*10)\n  df_plot[['y'] + models].plot(ax=ax, linewidth=3)\n  colors = [['blue', \"red\"]]\n  for model, color in zip(models, colors):\n    ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-95'], \n                        df_plot[f'{model}-hi-95'],\n                        alpha=.35,\n                        color=\"blue\",\n                        label=f'{model}-level-95')\n  for model, color in zip(models, colors):\n      ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-80'], \n                        df_plot[f'{model}-hi-80'],\n                        alpha=.20,\n                        color=\"white\",\n                        label=f'{model}-level-80')\n  ax.set_title('', fontsize=22)\n  ax.set_ylabel(\"Ads watched (hourly data)\", fontsize=20)\n  ax.set_xlabel('Hourly', fontsize=20)\n  ax.legend(prop={'size': 20})\n  ax.grid(True)\n  plt.show()\n\n\nplot_forecasts(df, forecast_df, models=[\"MSTL\"])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df, level=[80,95])"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#cross-validation",
    "href": "docs/models/multipleseasonaltrend.html#cross-validation",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=50). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 500 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=30,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nMSTL\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-15 18:00:00\n2017-09-15 17:00:00\n159725.0\n154065.828125\n\n\n1\n2017-09-15 19:00:00\n2017-09-15 17:00:00\n161085.0\n156565.984375\n\n\n1\n2017-09-15 20:00:00\n2017-09-15 17:00:00\n135520.0\n134848.375000\n\n\n...\n...\n...\n...\n...\n\n\n1\n2017-09-21 21:00:00\n2017-09-20 17:00:00\n103080.0\n113760.242188\n\n\n1\n2017-09-21 22:00:00\n2017-09-20 17:00:00\n95155.0\n101626.562500\n\n\n1\n2017-09-21 23:00:00\n2017-09-20 17:00:00\n80285.0\n82278.148438\n\n\n\n\n150 rows √ó 4 columns\n\n\n\nWe‚Äôll now plot the forecast for each cutoff period. To make the plots clearer, we‚Äôll rename the actual values in each period.\n\ncross_validation=crossvalidation_df.copy()\ncross_validation.rename(columns = {'y' : 'actual'}, inplace = True) # rename actual values \n\ncutoff = cross_validation['cutoff'].unique()\n\nfor k in range(len(cutoff)): \n    cv = cross_validation[cross_validation['cutoff'] == cutoff[k]]\n    StatsForecast.plot(df, cv.loc[:, cv.columns != 'cutoff'])"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#model-evaluation",
    "href": "docs/models/multipleseasonaltrend.html#model-evaluation",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, MSTL Model.\n\n\nrmse = rmse(cross_validation['actual'], cross_validation[\"MSTL\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  13587.256"
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#acknowledgements",
    "href": "docs/models/multipleseasonaltrend.html#acknowledgements",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/multipleseasonaltrend.html#references",
    "href": "docs/models/multipleseasonaltrend.html#references",
    "title": "Multiple Seasonal Trend (MSTL)",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/crostonoptimized.html",
    "href": "docs/models/crostonoptimized.html",
    "title": "CrostonOptimized Model",
    "section": "",
    "text": "Introduction\nCroston Optimized Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of CrostonOptimized with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/crostonoptimized.html#table-of-contents",
    "href": "docs/models/crostonoptimized.html#table-of-contents",
    "title": "CrostonOptimized Model",
    "section": "",
    "text": "Introduction\nCroston Optimized Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of CrostonOptimized with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/crostonoptimized.html#introduction",
    "href": "docs/models/crostonoptimized.html#introduction",
    "title": "CrostonOptimized Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Croston Optimized model is a forecasting method designed for intermittent demand time series data. It is an extension of the Croston‚Äôs method, which was originally developed for forecasting sporadic demand patterns.\nIntermittent demand time series are characterized by irregular and sporadic occurrences of non-zero demand values, often with long periods of zero demand. Traditional forecasting methods may struggle to handle such patterns effectively.\nThe Croston Optimized model addresses this challenge by incorporating two key components: exponential smoothing and intermittent demand estimation.\n\nExponential Smoothing: The Croston Optimized model uses exponential smoothing to capture the trend and seasonality in the intermittent demand data. This helps in identifying the underlying patterns and making more accurate forecasts.\nIntermittent Demand Estimation: Since intermittent demand data often consists of long periods of zero demand, the Croston Optimized model employs a separate estimation process for the occurrence and size of non-zero demand values. It estimates the probability of occurrence and the average size of non-zero demand intervals, enabling better forecasting of intermittent demand.\n\nThe Croston Optimized model aims to strike a balance between over-forecasting and under-forecasting intermittent demand, which are common challenges in traditional forecasting methods. By explicitly modeling intermittent demand patterns, it can provide more accurate forecasts for intermittent demand time series data.\nIt is worth noting that there are variations and adaptations of the Croston Optimized model, with different modifications and enhancements made to suit specific forecasting scenarios. These variations may incorporate additional features or algorithms to further improve the accuracy of the forecasts."
  },
  {
    "objectID": "docs/models/crostonoptimized.html#croston-optimized-method",
    "href": "docs/models/crostonoptimized.html#croston-optimized-method",
    "title": "CrostonOptimized Model",
    "section": "Croston Optimized method ",
    "text": "Croston Optimized method \nThe Croston Optimized model can be mathematically defined as follows:\n\nInitialization:\n\nLet \\((y_t)\\) represent the intermittent demand time series data at time \\(t\\).\nInitialize two sets of variables: \\((p_t)\\) for the probability of occurrence and \\((q_t)\\) for the average size of non-zero demand intervals.\nInitialize the forecast \\((F_t)\\) and forecast error \\((E_t)\\) variables as zero.\n\nCalculation of \\((p_t)\\) and \\((q_t)\\):\n\nCalculate the intermittent demand occurrence probability \\((p_t)\\) using exponential smoothing: \\[[p_t = \\alpha + (1 - \\alpha)(p_{t-1}),]\\] where \\((\\alpha)\\) is the smoothing parameter (typically set between 0.1 and 0.3).\nCalculate the average size of non-zero demand intervals \\((q_t)\\) using exponential smoothing: \\[[q_t = \\beta \\cdot y_t + (1 - \\beta)(q_{t-1}),]\\] where \\((\\beta)\\) is the smoothing parameter (typically set between 0.1 and 0.3).\n\nForecasting:\n\nIf \\((y_t &gt; 0)\\) (non-zero demand occurrence):\n\nCalculate the forecast \\((F_t)\\) as the previous forecast \\((F_{t-1})\\) divided by the average size of non-zero demand intervals \\((q_{t-1})\\): \\[[F_t = \\frac{{F_{t-1}}}{{q_{t-1}}}]\\]\nCalculate the forecast error \\((E_t)\\) as the difference between the actual demand \\((y_t)\\) and the forecast \\((F_t)\\): \\[[E_t = y_t - F_t]\\]\n\nIf \\((y_t = 0)\\) (zero demand occurrence):\n\nSet the forecast \\((F_t)\\) and forecast error \\((E_t)\\) as zero.\n\n\nUpdating the model:\n\nUpdate the intermittent demand occurrence probability \\((p_t)\\) and the average size of non-zero demand intervals \\((q_t)\\) using exponential smoothing as described in step 2.\n\nRepeat steps 3 and 4 for each time point in the time series.\n\nThe Croston Optimized model leverages exponential smoothing to capture the trend and seasonality in the intermittent demand data, and it estimates the occurrence probability and average size of non-zero demand intervals separately to handle intermittent demand patterns effectively. By updating the model parameters based on the observed data, it provides improved forecasts for intermittent demand time series.\n\nSome properties of the Optimized Croston Model\nThe optimized Croston model is a modification of the classic Croston model used to forecast intermittent demand. The classic Croston model forecasts demand using a weighted average of historical orders and the average interval between orders. The optimized Croston model uses a probability function to forecast the mean interval between orders.\nThe optimized Croston model has been shown to be more accurate than the classical Croston model for time series with irregular demand. The optimized Croston model is also more adaptable to different types of intermittent time series.\nThe optimized Croston model has the following properties:\n\nIt is accurate, even for time series with irregular demand.\nIt is adaptable to different types of intermittent time series.\nIt is easy to implement and understand.\nIt is robust to outliers.\n\nThe optimized Croston model has been used successfully to forecast a wide range of intermittent time series, including product demand, service demand, and resource demand.\nHere are some of the properties of the optimized Croston model:\n\nPrecision: The optimized Croston model has been shown to be more accurate than the classic Croston model for time series with irregular demand. This is because the optimized Croston model uses a probability function to forecast the average interval between orders, which is more accurate than the weighted average of historical orders.\nAdaptability: The optimized Croston model is also more adaptable to different types of intermittent time series. This is because the optimized Croston model uses a probability function to forecast the mean interval between orders, allowing it to accommodate different demand patterns.\nEase of Implementation and Understanding: The optimized Croston model is easy to implement and understand. This is because the optimized Croston model is a modification of the classical Croston model, which is a well-known and well-understood model.\nRobustness: The optimized Croston model is also robust to outliers. This is because the optimized Croston model uses a probability function to forecast the mean interval between orders, which allows it to ignore outliers."
  },
  {
    "objectID": "docs/models/crostonoptimized.html#loading-libraries-and-data",
    "href": "docs/models/crostonoptimized.html#loading-libraries-and-data",
    "title": "CrostonOptimized Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport plotly.graph_objects as go\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nimport pandas as pd\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/intermittend_demand2\")\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nsales\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n\n\n1\n2022-01-01 01:00:00\n10\n\n\n2\n2022-01-01 02:00:00\n0\n\n\n3\n2022-01-01 03:00:00\n0\n\n\n4\n2022-01-01 04:00:00\n100\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n1\n\n\n1\n2022-01-01 01:00:00\n10\n1\n\n\n2\n2022-01-01 02:00:00\n0\n1\n\n\n3\n2022-01-01 03:00:00\n0\n1\n\n\n4\n2022-01-01 04:00:00\n100\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/crostonoptimized.html#explore-data-with-the-plot-method",
    "href": "docs/models/crostonoptimized.html#explore-data-with-the-plot-method",
    "title": "CrostonOptimized Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\nAutocorrelation (ACF) and partial autocorrelation (PACF) plots are statistical tools used to analyze time series. ACF charts show the correlation between the values of a time series and their lagged values, while PACF charts show the correlation between the values of a time series and their lagged values, after the effect of previous lagged values has been removed.\nACF and PACF charts can be used to identify the structure of a time series, which can be helpful in choosing a suitable model for the time series. For example, if the ACF chart shows a repeating peak and valley pattern, this indicates that the time series is stationary, meaning that it has the same statistical properties over time. If the PACF chart shows a pattern of rapidly decreasing spikes, this indicates that the time series is invertible, meaning it can be reversed to get a stationary time series.\nThe importance of the ACF and PACF charts is that they can help analysts better understand the structure of a time series. This understanding can be helpful in choosing a suitable model for the time series, which can improve the ability to predict future values of the time series.\nTo analyze ACF and PACF charts:\n\nLook for patterns in charts. Common patterns include repeating peaks and valleys, sawtooth patterns, and plateau patterns.\nCompare ACF and PACF charts. The PACF chart generally has fewer spikes than the ACF chart.\nConsider the length of the time series. ACF and PACF charts for longer time series will have more spikes.\nUse a confidence interval. The ACF and PACF plots also show confidence intervals for the autocorrelation values. If an autocorrelation value is outside the confidence interval, it is likely to be significant.\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom plotly.subplots import make_subplots\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=24,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "docs/models/crostonoptimized.html#split-the-data-into-training-and-testing",
    "href": "docs/models/crostonoptimized.html#split-the-data-into-training-and-testing",
    "title": "CrostonOptimized Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\nLet‚Äôs divide our data into sets 1. Data to train our Croston Optimized Model. 2. Data to test our model\nFor the test data we will use the last 500 Hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2023-01-31 19:00:00'] \ntest = df[df.ds&gt;'2023-01-31 19:00:00']\n\n\ntrain.shape, test.shape\n\n((9500, 3), (500, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\",linewidth=2)\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\", linewidth=2, color=\"yellow\")\nplt.title(\"Store visit\");\nplt.xlabel(\"Hours\")\nplt.show()"
  },
  {
    "objectID": "docs/models/crostonoptimized.html#implementation-of-crostonoptimized-with-statsforecast",
    "href": "docs/models/crostonoptimized.html#implementation-of-crostonoptimized-with-statsforecast",
    "title": "CrostonOptimized Model",
    "section": "Implementation of CrostonOptimized with StatsForecast ",
    "text": "Implementation of CrostonOptimized with StatsForecast \nTo also know more about the parameters of the functions of the CrostonOptimized Model, they are listed below. For more information, visit the documentation.\nalias : str\n    Custom name of the model.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import CrostonOptimized\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\n# We call the model that we are going to use\nmodels = [CrostonOptimized()]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\n# fit the models\nsf.fit()\n\nStatsForecast(models=[CrostonOptimized])\n\n\nLet‚Äôs see the results of our Croston optimized Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([23.606695], dtype=float32)}\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon)\nY_hat\n\n\n\n\n\n\n\n\nds\nCrostonOptimized\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n23.606695\n\n\n1\n2023-02-21 17:00:00\n23.606695\n\n\n1\n2023-02-21 18:00:00\n23.606695\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n23.606695\n\n\n1\n2023-03-14 10:00:00\n23.606695\n\n\n1\n2023-03-14 11:00:00\n23.606695\n\n\n\n\n500 rows √ó 2 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nCrostonOptimized\n\n\n\n\n0\n1\n2023-02-21 16:00:00\n23.606695\n\n\n1\n1\n2023-02-21 17:00:00\n23.606695\n\n\n2\n1\n2023-02-21 18:00:00\n23.606695\n\n\n...\n...\n...\n...\n\n\n497\n1\n2023-03-14 09:00:00\n23.606695\n\n\n498\n1\n2023-03-14 10:00:00\n23.606695\n\n\n499\n1\n2023-03-14 11:00:00\n23.606695\n\n\n\n\n500 rows √ó 3 columns\n\n\n\n\nY_hat1 = pd.concat([df,Y_hat])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nCrostonOptimized\n\n\n\n\n0\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n1\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n497\n2023-03-14 09:00:00\nNaN\n1\n23.606695\n\n\n498\n2023-03-14 10:00:00\nNaN\n1\n23.606695\n\n\n499\n2023-03-14 11:00:00\nNaN\n1\n23.606695\n\n\n\n\n10500 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df, Y_hat1]).set_index('ds')\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[ \"CrostonOptimized\"].plot(ax=ax, linewidth=2, color=\"yellow\")\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel(\"Store visit (Hourly data)\", fontsize=20)\nax.set_xlabel('Hours', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \nforecast_df\n\n\n\n\n\n\n\n\nds\nCrostonOptimized\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n23.606695\n\n\n1\n2023-02-21 17:00:00\n23.606695\n\n\n1\n2023-02-21 18:00:00\n23.606695\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n23.606695\n\n\n1\n2023-03-14 10:00:00\n23.606695\n\n\n1\n2023-03-14 11:00:00\n23.606695\n\n\n\n\n500 rows √ó 2 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nCrostonOptimized\n\n\nds\n\n\n\n\n\n\n\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n23.606695\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n23.606695\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n23.606695\n\n\n\n\n10500 rows √ó 3 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds').tail(5000)\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nCrostonOptimized\n\n\nds\n\n\n\n\n\n\n\n2022-08-18 04:00:00\n0.0\n1\nNaN\n\n\n2022-08-18 05:00:00\n80.0\n1\nNaN\n\n\n2022-08-18 06:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n23.606695\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n23.606695\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n23.606695\n\n\n\n\n5000 rows √ó 3 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series.\n\nplt.plot(df_plot['y'],label=\"Actual\", linewidth=2.5)\nplt.plot(df_plot['CrostonOptimized'], label=\"Croston Optimized\", color=\"yellow\") # '-', '--', '-.', ':',\n\nplt.title(\"Store visit (Hourly data)\");\nplt.xlabel(\"Hourly\")\nplt.ylabel(\"Store visit\")\nplt.legend()\nplt.show();\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/crostonoptimized.html#cross-validation",
    "href": "docs/models/crostonoptimized.html#cross-validation",
    "title": "CrostonOptimized Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=50). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 500 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=50,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nCrostonOptimized\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2023-01-23 12:00:00\n2023-01-23 11:00:00\n0.0\n23.655830\n\n\n1\n2023-01-23 13:00:00\n2023-01-23 11:00:00\n0.0\n23.655830\n\n\n1\n2023-01-23 14:00:00\n2023-01-23 11:00:00\n0.0\n23.655830\n\n\n...\n...\n...\n...\n...\n\n\n1\n2023-02-21 13:00:00\n2023-01-31 19:00:00\n60.0\n27.418417\n\n\n1\n2023-02-21 14:00:00\n2023-01-31 19:00:00\n20.0\n27.418417\n\n\n1\n2023-02-21 15:00:00\n2023-01-31 19:00:00\n20.0\n27.418417\n\n\n\n\n2500 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/crostonoptimized.html#model-evaluation",
    "href": "docs/models/crostonoptimized.html#model-evaluation",
    "title": "CrostonOptimized Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Croston Optimized Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"CrostonOptimized\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  48.08823"
  },
  {
    "objectID": "docs/models/crostonoptimized.html#acknowledgements",
    "href": "docs/models/crostonoptimized.html#acknowledgements",
    "title": "CrostonOptimized Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/holt.html",
    "href": "docs/models/holt.html",
    "title": "Holt Model",
    "section": "",
    "text": "Introduction\nHolt Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of Holt with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/holt.html#table-of-contents",
    "href": "docs/models/holt.html#table-of-contents",
    "title": "Holt Model",
    "section": "",
    "text": "Introduction\nHolt Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of Holt with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/holt.html#introduction",
    "href": "docs/models/holt.html#introduction",
    "title": "Holt Model",
    "section": "Introduction",
    "text": "Introduction\nThe Holts model, also known as the double exponential smoothing method, is a forecasting technique widely used in time series analysis. It was developed by Charles Holt in 1957 as an improvement on Brown‚Äôs simple exponential smoothing method.\nThe Holts model is used to predict future values of a time series that exhibits a trend. The model uses two smoothing parameters, one for estimating the trend and the other for estimating the level or base level of the time series. These parameters are called \\(\\alpha\\) and \\(\\beta\\), respectively.\nThe Holts model is an extension of Brown‚Äôs simple exponential smoothing method, which uses only one smoothing parameter to estimate the trend and base level of the time series. The Holts model improves the accuracy of the forecasts by adding a second smoothing parameter for the trend.\nOne of the main advantages of the Holts model is that it is easy to implement and does not require a large amount of historical data to generate accurate predictions. Furthermore, the model is highly adaptable and can be customized to fit a wide variety of time series.\nHowever, Holts‚Äô model has some limitations. For example, the model assumes that the time series is stationary and that the trend is linear. If the time series is not stationary or has a non-linear trend, the Holts model may not be the most appropriate.\nIn general, the Holts model is a useful and widely used technique in time series analysis, especially when the series is expected to exhibit a linear trend."
  },
  {
    "objectID": "docs/models/holt.html#holt-method",
    "href": "docs/models/holt.html#holt-method",
    "title": "Holt Model",
    "section": "Holt Method ",
    "text": "Holt Method \nSimple exponential smoothing does not function well when the data has trends. In those cases, we can use double exponential smoothing. This is a more reliable method for handling data that consumes trends without seasonality than compared to other methods. This method adds a time trend equation in the formulation. Two different weights, or smoothing parameters, are used to update these two components at a time.\nHolt‚Äôs exponential smoothing is also sometimes called double exponential smoothing. The main idea here is to use SES and advance it to capture the trend component.\nHolt (1957) extended simple exponential smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level and one for the trend):\nAssume that a series has the following:\n\nLevel\nTrend\nNo seasonality\nNoise\n\n\\[\\begin{align*}\n  \\text{Forecast equation}&& \\hat{y}_{t+h|t} &= \\ell_{t} + hb_{t} \\\\\n  \\text{Level equation}   && \\ell_{t} &= \\alpha y_{t} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\\\\n  \\text{Trend equation}   && b_{t}    &= \\beta^*(\\ell_{t} - \\ell_{t-1}) + (1 -\\beta^*)b_{t-1},\n\\end{align*}\\]\nwhere \\(\\ell_{t}\\) denotes an estimate of the level of the series at time \\(t, b_t\\) denotes an estimate of the trend (slope) of the series at time \\(t, \\alpha\\) is the smoothing parameter for the level, \\(0\\le\\alpha\\le1\\), and \\(\\beta^{*}\\) is the smoothing parameter for the trend, \\(0\\le\\beta^*\\le1\\).\nAs with simple exponential smoothing, the level equation here shows that \\(\\ell_{t}\\) is a weighted average of observation \\(y_{t}\\) and the one-step-ahead training forecast for time \\(t\\), here given by \\(\\ell_{t-1} + b_{t-1}\\). The trend equation shows that \\(b_t\\) is a weighted average of the estimated trend at time \\(t\\) based on \\(\\ell_{t} - \\ell_{t-1}\\) and \\(b_{t-1}\\), the previous estimate of the trend.\nThe forecast function is no longer flat but trending. The \\(h\\)-step-ahead forecast is equal to the last estimated level plus \\(h\\) times the last estimated trend value. Hence the forecasts are a linear function of \\(h\\).\n\nInnovations state space models for exponential smoothing\nThe exponential smoothing methods presented in Table 7.6 are algorithms which generate point forecasts. The statistical models in this tutorial generate the same point forecasts, but can also generate prediction (or forecast) intervals. A statistical model is a stochastic (or random) data generating process that can produce an entire forecast distribution.\nEach model consists of a measurement equation that describes the observed data, and some state equations that describe how the unobserved components or states (level, trend, seasonal) change over time. Hence, these are referred to as state space models.\nFor each method there exist two models: one with additive errors and one with multiplicative errors. The point forecasts produced by the models are identical if they use the same smoothing parameter values. They will, however, generate different prediction intervals.\nTo distinguish between a model with additive errors and one with multiplicative errors. We label each state space model as ETS( .,.,.) for (Error, Trend, Seasonal). This label can also be thought of as ExponenTial Smoothing. Using the same notation as in Table 7.5, the possibilities for each component are: \\(Error=\\{A,M\\}\\), \\(Trend=\\{N,A,A_d\\}\\) and \\(Seasonal=\\{N,A,M\\}\\)\nFor our case, the linear Holt model with a trend, we are going to see two cases, both for the additive and the multiplicative\n\n\nETS(A,A,N): Holt‚Äôs linear method with additive errors\nFor this model, we assume that the one-step-ahead training errors are given by $ t=y_t-{t-1}-b_{t-1} (0,^2)$. Substituting this into the error correction equations for Holt‚Äôs linear method we obtain\n\\[\\begin{align*}\ny_t&=\\ell_{t-1}+b_{t-1}+\\varepsilon_t\\\\\n\\ell_t&=\\ell_{t-1}+b_{t-1}+\\alpha \\varepsilon_t\\\\\nb_t&=b_{t-1}+\\beta \\varepsilon_t,\n\\end{align*}\\]\nwhere, for simplicity, we have set \\(\\beta=\\alpha \\beta^*\\)\n\n\nETS(M,A,N): Holt‚Äôs linear method with multiplicative errors\nSpecifying one-step-ahead training errors as relative errors such that\n\\[\\varepsilon_t=\\frac{y_t-(\\ell_{t-1}+b_{t-1})}{(\\ell_{t-1}+b_{t-1})}\\]\nand following an approach similar to that used above, the innovations state space model underlying Holt‚Äôs linear method with multiplicative errors is specified as\n\\[\\begin{align*}\ny_t&=(\\ell_{t-1}+b_{t-1})(1+\\varepsilon_t)\\\\\n\\ell_t&=(\\ell_{t-1}+b_{t-1})(1+\\alpha \\varepsilon_t)\\\\\nb_t&=b_{t-1}+\\beta(\\ell_{t-1}+b_{t-1}) \\varepsilon_t\n\\end{align*}\\]\nwhere again \\(\\beta=\\alpha \\beta^*\\) and \\(\\varepsilon_t \\sim \\text{NID}(0,\\sigma^2)\\).\n\n\nA taxonomy of exponential smoothing methods\nBuilding on the idea of time series components, we can move to the ETS taxonomy. ETS stands for ‚ÄúError-Trend-Seasonality‚Äù and defines how specifically the components interact with each other. Based on the type of error, trend and seasonality, Pegels (1969) proposed a taxonomy, which was then developed further by Hyndman et al.¬†(2002) and refined by Hyndman et al.¬†(2008). According to this taxonomy, error, trend and seasonality can be:\n\nError: ‚ÄúAdditive‚Äù (A), or ‚ÄúMultiplicative‚Äù (M);\nTrend: ‚ÄúNone‚Äù (N), or ‚ÄúAdditive‚Äù (A), or ‚ÄúAdditive damped‚Äù (Ad), or ‚ÄúMultiplicative‚Äù (M), or ‚ÄúMultiplicative damped‚Äù (Md);\nSeasonality: ‚ÄúNone‚Äù (N), or ‚ÄúAdditive‚Äù (A), or ‚ÄúMultiplicative‚Äù (M).\n\nThe components in the ETS taxonomy have clear interpretations: level shows average value per time period, trend reflects the change in the value, while seasonality corresponds to periodic fluctuations (e.g.¬†increase in sales each January). Based on the the types of the components above, it is theoretically possible to devise 30 ETS models with different types of error, trend and seasonality. Figure 1 shows examples of different time series with deterministic (they do not change over time) level, trend, seasonality and with the additive error term.\n Figure 4.1: Time series corresponding to the additive error ETS models\nThings to note from the plots in Figure.1:\n\nWhen seasonality is multiplicative, its amplitude increases with the increase of the level of the data, while with additive seasonality, the amplitude is constant. Compare, for example, ETS(A,A,A) with ETS(A,A,M): for the former, the distance between the highest and the lowest points in the first year is roughly the same as in the last year. In the case of ETS(A,A,M) the distance increases with the increase in the level of series;\nWhen the trend is multiplicative, data exhibits exponential growth/decay;\nThe damped trend slows down both additive and multiplicative trends;\nIt is practically impossible to distinguish additive and multiplicative seasonality if the level of series does not change because the amplitude of seasonality will be constant in both cases (compare ETS(A,N,A) and ETS(A,N,M)).\n\n Figure 2: Time series corresponding to the multiplicative error ETS models\nThe graphs in Figure 2 show approximately the same idea as the additive case, the main difference is that the error variance increases with increasing data level; this becomes clearer in ETS(M,A,N) and ETS(M,M,N) data. This property is called heteroskedasticity in statistics, and Hyndman et al.¬†(2008) argue that the main benefit of multiplicative error models is to capture this characteristic.\n\n\nMathematical models in the ETS taxonomy\nI hope that it becomes more apparent to the reader how the ETS framework is built upon the idea of time series decomposition. By introducing different components, defining their types, and adding the equations for their update, we can construct models that would work better in capturing the key features of the time series. But we should also consider the potential change in components over time. The ‚Äútransition‚Äù or ‚Äústate‚Äù equations are supposed to reflect this change: they explain how the level, trend or seasonal components evolve.\nAs discussed in Section 2.2, given different types of components and their interactions, we end up with 30 models in the taxonomy. Tables 1 and 2 summarise mathematically all 30 ETS models shown graphically on Figures 1 and 2, presenting formulae for measurement and transition equations.\nTable 1: Additive error ETS models | | Nonseasonal |Additive |Multiplicative| |‚Äî-|‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äì| |No trend|$\n\\[\\begin{aligned} &y_{t} = l_{t-1} + \\epsilon_t \\\\  &l_t = l_{t-1} + \\alpha \\epsilon_t  \\end{aligned}\\]\n$ |\\(\\begin{aligned} &y_{t} = l_{t-1} + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) |\\(\\begin{aligned} &y_{t} = l_{t-1} s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}} \\end{aligned}\\)| |Additive| \\(\\begin{aligned} &y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\ &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + b_{t-1}} \\end{aligned}\\)| |Additive damped| \\(\\begin{aligned} &y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &b_t = \\phi b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + \\phi b_{t-1}} \\end{aligned}\\)| |Multiplicative| $\n\\[\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} + \\epsilon_t \\\\  &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\  &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}}  \\end{aligned}\\]\n$ | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\end{aligned}\\)| |Multiplicative damped| \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\end{aligned}\\)|\nTable 2: Multiplicative error ETS models | |Nonseasonal |Additive |Multiplicative| |‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äì| |No trend| \\(\\begin{aligned} &y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\ &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} s_{t-m}(1 + \\epsilon_t) \\\\ &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\ &s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\end{aligned}\\)| |Additive| \\(\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1})(1 + \\epsilon_t) \\\\ &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} + b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1} + \\beta (l_{t-1} + b_{t-1}) \\epsilon_t \\\\ &s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}\\)| |Additive damped| \\(\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1})(1 + \\epsilon_t) \\\\ &l_t = (l_{t-1} + \\phi b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} + \\phi b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &b_t = \\phi b_{t-1} + \\beta (l_{t-1} + \\phi b_{t-1}) \\epsilon_t \\\\ &s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\end{aligned}\\)| |Multiplicative| \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} (1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\)| \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\ &s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}\\)| |Multiplicative damped| \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi (1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1}^\\phi (1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1}^\\phi (1 + \\beta \\epsilon_t) \\end{aligned}\\)| \\(\\begin{aligned} &y_{t} = (l_{t-1} b_{t-1}^\\phi + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &b_t = b_{t-1}^\\phi + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} (1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1}^\\phi \\left(1 + \\alpha \\epsilon_t\\right) \\\\ &b_t = b_{t-1}^\\phi \\left(1 + \\beta \\epsilon_t\\right) \\\\ &s_t = s_{t-m} \\left(1 + \\gamma \\epsilon_t\\right) \\end{aligned}\\)|\nFrom a statistical point of view, formulae in Tables 1 and 2 correspond to the ‚Äútrue models‚Äù, they explain the models underlying potential data, but when it comes to their construction and estimation, the \\(\\epsilon_t\\) is substituted by the estimated \\(e_t\\) (which is calculated differently depending on the error type), and time series components and smoothing parameters are also replaced by their estimates (e.g.¬†\\(\\hat \\alpha\\) instead of \\(\\alpha\\)). However, if the values of these models‚Äô parameters were known, it would be possible to produce point forecasts and conditional h steps ahead expectations from these models.\n\n\nProperties Holt‚Äôs linear trend method\nHolt‚Äôs linear trend method is a time series forecasting technique that uses exponential smoothing to estimate the level and trend components of a time series. The method has several properties, including:\n\nAdditive model: Holt‚Äôs linear trend method assumes that the time series can be decomposed into an additive model, where the observed values are the sum of the level, trend, and error components.\nSmoothing parameters: The method uses two smoothing parameters, Œ± and Œ≤, to estimate the level and trend components of the time series. These parameters control the amount of smoothing applied to the level and trend components, respectively.\nLinear trend: Holt‚Äôs linear trend method assumes that the trend component of the time series follows a straight line. This means that the method is suitable for time series data that exhibit a constant linear trend over time.\nForecasting: The method uses the estimated level and trend components to forecast future values of the time series. The forecast for the next period is given by the sum of the level and trend components.\nOptimization: The smoothing parameters Œ± and Œ≤ are estimated through a process of optimization that minimizes the sum of squared errors between the predicted and observed values. This involves iterating over different values of the smoothing parameters until the optimal values are found.\nSeasonality: Holt‚Äôs linear trend method can be extended to incorporate seasonality components. This involves adding a seasonal component to the model, which captures any systematic variations in the time series that occur on a regular basis.\n\nOverall, Holt‚Äôs linear trend method is a powerful and widely used forecasting technique that can be used to generate accurate predictions for time series data with a constant linear trend. The method is easy to implement and can be extended to handle time series data with seasonal variations."
  },
  {
    "objectID": "docs/models/holt.html#loading-libraries-and-data",
    "href": "docs/models/holt.html#loading-libraries-and-data",
    "title": "Holt Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\nimport pandas as pd\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/ads.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nTime\nAds\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n\n\n1\n2017-09-13T01:00:00\n79885\n\n\n2\n2017-09-13T02:00:00\n89325\n\n\n3\n2017-09-13T03:00:00\n101930\n\n\n4\n2017-09-13T04:00:00\n121630\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n1\n\n\n1\n2017-09-13T01:00:00\n79885\n1\n\n\n2\n2017-09-13T02:00:00\n89325\n1\n\n\n3\n2017-09-13T03:00:00\n101930\n1\n\n\n4\n2017-09-13T04:00:00\n121630\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/holt.html#explore-data-with-the-plot-method",
    "href": "docs/models/holt.html#explore-data-with-the-plot-method",
    "title": "Holt Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nThe Augmented Dickey-Fuller Test\nAn Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\n\nNull Hypothesis: Time Series is non-stationary. It gives a time-dependent trend.\nAlternate Hypothesis: Time Series is stationary. In another term, the series doesn‚Äôt depend on time.\nADF or t Statistic &lt; critical values: Reject the null hypothesis, time series is stationary.\nADF or t Statistic &gt; critical values: Failed to reject the null hypothesis, time series is non-stationary.\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],'Ads')\n\nDickey-Fuller test results for columns: Ads\nTest Statistic         -7.089634e+00\np-value                 4.444804e-10\nNo Lags Used            9.000000e+00\n                            ...     \nCritical Value (1%)    -3.462499e+00\nCritical Value (5%)    -2.875675e+00\nCritical Value (10%)   -2.574304e+00\nLength: 7, dtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\n\n\nAutocorrelation plots\nAutocorrelation Function\nDefinition 1. Let \\(\\{x_t;1 ‚â§ t ‚â§ n\\}\\) be a time series sample of size n from \\(\\{X_t\\}\\). 1. \\(\\bar x = \\sum_{t=1}^n \\frac{x_t}{n}\\) is called the sample mean of \\(\\{X_t\\}\\). 2. \\(c_k =\\sum_{t=1}^{n‚àík} (x_{t+k}- \\bar x)(x_t‚àí\\bar x)/n\\) is known as the sample autocovariance function of \\(\\{X_t\\}\\). 3. \\(r_k = c_k /c_0\\) is said to be the sample autocorrelation function of \\(\\{X_t\\}\\).\nNote the following remarks about this definition:\n\nLike most literature, this guide uses ACF to denote the sample autocorrelation function as well as the autocorrelation function. What is denoted by ACF can easily be identified in context.\nClearly c0 is the sample variance of \\(\\{X_t\\}\\). Besides, \\(r_0 = c_0/c_0 = 1\\) and for any integer \\(k, |r_k| ‚â§ 1\\).\nWhen we compute the ACF of any sample series with a fixed length \\(n\\), we cannot put too much confidence in the values of \\(r_k\\) for large k‚Äôs, since fewer pairs of \\((x_{t +k }, x_t )\\) are available for calculating \\(r_k\\) as \\(k\\) is large. One rule of thumb is not to estimate \\(r_k\\) for \\(k &gt; n/3\\), and another is \\(n ‚â• 50, k ‚â§ n/4\\). In any case, it is always a good idea to be careful.\nWe also compute the ACF of a nonstationary time series sample by Definition 1. In this case, however, the ACF or \\(r_k\\) very slowly or hardly tapers off as \\(k\\) increases.\nPlotting the ACF \\((r_k)\\) against lag \\(k\\) is easy but very helpful in analyzing time series sample. Such an ACF plot is known as a correlogram.\nIf \\(\\{X_t\\}\\) is stationary with \\(E(X_t)=0\\) and \\(\\rho_k =0\\) for all \\(k \\neq 0\\),thatis,itisa white noise series, then the sampling distribution of \\(r_k\\) is asymptotically normal with the mean 0 and the variance of \\(1/n\\). Hence, there is about 95% chance that \\(r_k\\) falls in the interval \\([‚àí1.96/‚àön, 1.96/‚àön]\\).\n\nNow we can give a summary that (1) if the time series plot of a time series clearly shows a trend or/and seasonality, it is surely nonstationary; (2) if the ACF \\(r_k\\) very slowly or hardly tapers off as lag \\(k\\) increases, the time series should also be nonstationary.\nPartial autocorrelation\nLet \\(\\{X_t\\}\\) be a stationary time series with \\(E(X_t) = 0\\). Here the assumption \\(E(X_t ) = 0\\) is for conciseness only. If \\(E(X_t) = \\mu \\neq 0\\), it is okay to replace \\(\\{X_t\\}\\) by \\(\\{X_t ‚àí\\mu \\}\\). Now consider the linear regression (prediction) of \\(X_t\\) on \\(\\{X_{t‚àík+1:t‚àí1}\\}\\) for any integer \\(k ‚â• 2\\). We use \\(\\hat X_t\\) to denote this regression (prediction): \\[\\hat X_t =\\alpha_1 X_{t‚àí1}+¬∑¬∑¬∑+\\alpha_{k‚àí1} X_{t‚àík+1}\\]\nwhere \\(\\{\\alpha_1, ¬∑ ¬∑ ¬∑ , \\alpha_{k‚àí1} \\}\\) satisfy\n\\[\\{\\alpha_1, ¬∑ ¬∑ ¬∑ , \\alpha_{k‚àí1} \\}=\\argmin_{\\beta_1,¬∑¬∑¬∑,\\beta{k‚àí1}} E[X_t ‚àí(\\beta_1 X_{t‚àí1} +¬∑¬∑¬∑+\\beta_{k‚àí1} X_{t‚àík+1})]^2\\]\nThat is, \\(\\{\\alpha_1, ¬∑ ¬∑ ¬∑ , \\alpha_{k‚àí1} \\}\\) are chosen by minimizing the mean squared error of prediction. Similarly, let \\(\\hat X_{t ‚àík}\\) denote the regression (prediction) of \\(X_{t ‚àík}\\) on \\(\\{X_{t ‚àík+1:t ‚àí1}\\}\\):\n\\[\\hat X_{t‚àík} =\\eta_1 X_{t‚àí1}+¬∑¬∑¬∑+\\eta_{k‚àí1} X_{t‚àík+1} \\]\nNote that if \\(\\{X_t\\}\\) is stationary, then \\(\\{\\alpha_{1:k‚àí1} \\} = \\{\\eta_{1:k‚àí1} \\}\\). Now let \\(\\hat Z_{t‚àík} = X_{t‚àík} ‚àí \\hat X_{t‚àík}\\) and \\(\\hat Z_t = X_t ‚àí \\hat X_t\\). Then \\(\\hat Z_{t‚àík}\\) is the residual of removing the effect of the intervening variables \\(\\{X_{t‚àík+1:t‚àí1} \\}\\) from \\(X_{t‚àík}\\), and \\(\\hat Z_t\\) is the residual of removing the effect of \\(\\{X_{t ‚àík+1:t ‚àí1} \\}\\) from \\(X_t\\) .\nDefinition 2. The partial autocorrelation function (PACF) at lag \\(k\\) of a stationary time series \\(\\{X_t \\}\\) with \\(E(X_t ) = 0\\) is\n\\[\\phi_{11} = Corr(X_{t‚àí1}, X_t ) = \\frac{Cov(X_{t‚àí1}, X_t )} {[Var(X_{t‚àí1})Var(X_t)]^{1/2}} = \\rho_1\\] and\n\\[\\phi_{kk} = Corr(\\hat Z_{t‚àík},\\hat Z_t) = \\frac{Cov(\\hat Z_{t‚àík},\\hat Z_t)} {[Var(\\hat Z_{t ‚àík} )Var(\\hat Z_t )]^{1/2}}, \\ k ‚â• 2  \\]\nOn the other hand, the following theorem paves the way to estimate the PACF of a stationary time series, and its proof can be seen in Fan and Yao (2003).\nTheorem 1. Let \\(\\{X_t \\}\\) be a stationary time series with \\(E(X_t ) = 0\\), and \\(\\{a_{1k},¬∑¬∑¬∑ ,a_{kk}\\}\\) satisfy\n\\[\\{a_{1k},¬∑¬∑¬∑ ,a_{kk}\\}= \\argmin_{a_1 ,¬∑¬∑¬∑ ,a_k}  E(X_t ‚àí a_1 X_{t‚àí1}‚àí¬∑¬∑¬∑‚àía_k X_{t‚àík})^2 \\]\nThen \\(\\phi_{kk} =a_{kk}\\) for \\(k‚â•1\\).\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\n\nAdditive\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=12)\na.plot();\n\n\n\n\n\n\nMultiplicative\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=12)\na.plot();"
  },
  {
    "objectID": "docs/models/holt.html#split-the-data-into-training-and-testing",
    "href": "docs/models/holt.html#split-the-data-into-training-and-testing",
    "title": "Holt Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our Holt Model. 2. Data to test our model\nFor the test data we will use the last 30 hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2017-09-20 17:00:00'] \ntest = df[df.ds&gt;'2017-09-20 17:00:00']\n\n\ntrain.shape, test.shape\n\n((186, 3), (30, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"Ads watched (hourly data)\");\nplt.show()"
  },
  {
    "objectID": "docs/models/holt.html#implementation-of-holt-method-with-statsforecast",
    "href": "docs/models/holt.html#implementation-of-holt-method-with-statsforecast",
    "title": "Holt Model",
    "section": "Implementation of Holt Method with StatsForecast ",
    "text": "Implementation of Holt Method with StatsForecast \nTo also know more about the parameters of the functions of the Holt Model, they are listed below. For more information, visit the documentation.\nseason_length : int\n    Number of observations per unit of time. Ex: 12 Monthly data.\nerror_type : str\n    The type of error of the ETS model. Can be additive (A) or multiplicative (M).\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Holt\n\n\n\nInstantiate Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\nmodels = [Holt(season_length=season_length, error_type=\"A\", alias=\"Add\"),\n          Holt(season_length=season_length, error_type=\"M\", alias=\"Multi\")]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[Add,Multi])\n\n\nLet‚Äôs see the results of our Holt Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nprint(result.keys())\nprint(result['fit'])\n\ndict_keys(['loglik', 'aic', 'bic', 'aicc', 'mse', 'amse', 'fit', 'residuals', 'components', 'm', 'nstate', 'fitted', 'states', 'par', 'sigma2', 'n_params', 'method', 'actual_residuals'])\nresults(x=array([ 9.99900000e-01,  1.00000000e-04,  7.99488495e+04, -4.74727943e+00]), fn=5213.773789117043, nit=72, simplex=array([[ 9.99900000e-01,  1.00000000e-04,  7.99083958e+04,\n         3.48822281e+00],\n       [ 9.99900000e-01,  1.00000000e-04,  7.99488495e+04,\n        -4.74727943e+00],\n       [ 9.99900000e-01,  1.00000000e-04,  8.02890214e+04,\n         7.41636110e+00],\n       [ 9.99900000e-01,  1.00000000e-04,  8.01679532e+04,\n        -1.60232330e+01],\n       [ 9.99900000e-01,  1.00000000e-04,  8.03170190e+04,\n        -3.36503173e+00]]))\n\n\nLet us now visualize the fitted values of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\n170.897743\n\n\n1\n-225.252721\n\n\n2\n9444.730190\n\n\n...\n...\n\n\n213\n-20317.858920\n\n\n214\n-7924.609136\n\n\n215\n-14867.577350\n\n\n\n\n216 rows √ó 1 columns\n\n\n\n\nimport scipy.stats as stats\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon, fitted=True)\nY_hat\n\n\n\n\n\n\n\n\nds\nAdd\nMulti\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80281.781250\n81537.476562\n\n\n1\n2017-09-22 01:00:00\n80277.085938\n82788.335938\n\n\n1\n2017-09-22 02:00:00\n80272.382812\n84039.195312\n\n\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n80154.835938\n115310.718750\n\n\n1\n2017-09-23 04:00:00\n80150.132812\n116561.578125\n\n\n1\n2017-09-23 05:00:00\n80145.429688\n117812.437500\n\n\n\n\n30 rows √ó 3 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nAdd\nMulti\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-13 00:00:00\n80115.0\n79944.101562\n79406.656250\n\n\n1\n2017-09-13 01:00:00\n79885.0\n80110.250000\n81393.054688\n\n\n1\n2017-09-13 02:00:00\n89325.0\n79880.273438\n81163.125000\n\n\n1\n2017-09-13 03:00:00\n101930.0\n89320.250000\n90602.976562\n\n\n1\n2017-09-13 04:00:00\n121630.0\n101926.195312\n103208.789062\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nAdd\nAdd-lo-95\nAdd-hi-95\nMulti\nMulti-lo-95\nMulti-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80281.781250\n56698.507812\n103865.062500\n81537.476562\n65208.687500\n97866.257812\n\n\n1\n2017-09-22 01:00:00\n80277.085938\n46925.292969\n113628.875000\n82788.335938\n59458.406250\n106118.265625\n\n\n1\n2017-09-22 02:00:00\n80272.382812\n39423.585938\n121121.179688\n84039.195312\n55172.894531\n112905.500000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n80154.835938\n-44792.613281\n205102.281250\n115310.718750\n3485.836914\n227135.593750\n\n\n1\n2017-09-23 04:00:00\n80150.132812\n-47015.281250\n207315.546875\n116561.578125\n1745.947998\n231377.203125\n\n\n1\n2017-09-23 05:00:00\n80145.429688\n-49200.355469\n209491.218750\n117812.437500\n1.485114\n235623.390625\n\n\n\n\n30 rows √ó 7 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nAdd\nMulti\n\n\n\n\n0\n1\n2017-09-22 00:00:00\n80281.781250\n81537.476562\n\n\n1\n1\n2017-09-22 01:00:00\n80277.085938\n82788.335938\n\n\n2\n1\n2017-09-22 02:00:00\n80272.382812\n84039.195312\n\n\n...\n...\n...\n...\n...\n\n\n27\n1\n2017-09-23 03:00:00\n80154.835938\n115310.718750\n\n\n28\n1\n2017-09-23 04:00:00\n80150.132812\n116561.578125\n\n\n29\n1\n2017-09-23 05:00:00\n80145.429688\n117812.437500\n\n\n\n\n30 rows √ó 4 columns\n\n\n\n\n# Merge the forecasts with the true values\nY_hat1 = pd.concat([df,Y_hat],  keys=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nAdd\nMulti\n\n\n\n\nunique_id\n0\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\n\n\n2\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nds\n27\n2017-09-23 03:00:00\nNaN\n1\n80154.835938\n115310.718750\n\n\n28\n2017-09-23 04:00:00\nNaN\n1\n80150.132812\n116561.578125\n\n\n29\n2017-09-23 05:00:00\nNaN\n1\n80145.429688\n117812.437500\n\n\n\n\n246 rows √ó 5 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds').tail(300)\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[ \"Add\"].plot(ax=ax, linewidth=2)\nplot_df[ \"Multi\"].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Ads watched (hourly data)', fontsize=20)\nax.set_xlabel('Monthly [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nAdd\nMulti\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80281.781250\n81537.476562\n\n\n1\n2017-09-22 01:00:00\n80277.085938\n82788.335938\n\n\n1\n2017-09-22 02:00:00\n80272.382812\n84039.195312\n\n\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n80154.835938\n115310.718750\n\n\n1\n2017-09-23 04:00:00\n80150.132812\n116561.578125\n\n\n1\n2017-09-23 05:00:00\n80145.429688\n117812.437500\n\n\n\n\n30 rows √ó 3 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \n\nforecast_df\n\n\n\n\n\n\n\n\nds\nAdd\nAdd-lo-95\nAdd-lo-80\nAdd-hi-80\nAdd-hi-95\nMulti\nMulti-lo-95\nMulti-lo-80\nMulti-hi-80\nMulti-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80281.781250\n56698.507812\n64861.507812\n95702.062500\n103865.062500\n81537.476562\n65208.687500\n70860.656250\n92214.289062\n97866.257812\n\n\n1\n2017-09-22 01:00:00\n80277.085938\n46925.292969\n58469.519531\n102084.648438\n113628.875000\n82788.335938\n59458.406250\n67533.710938\n98042.953125\n106118.265625\n\n\n1\n2017-09-22 02:00:00\n80272.382812\n39423.585938\n53562.789062\n106981.976562\n121121.179688\n84039.195312\n55172.894531\n65164.535156\n102913.851562\n112905.500000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n80154.835938\n-44792.613281\n-1543.909790\n161853.578125\n205102.281250\n115310.718750\n3485.836914\n42192.359375\n188429.078125\n227135.593750\n\n\n1\n2017-09-23 04:00:00\n80150.132812\n-47015.281250\n-2998.863037\n163299.125000\n207315.546875\n116561.578125\n1745.947998\n41487.671875\n191635.484375\n231377.203125\n\n\n1\n2017-09-23 05:00:00\n80145.429688\n-49200.355469\n-4429.233887\n164720.093750\n209491.218750\n117812.437500\n1.485114\n40779.996094\n194844.875000\n235623.390625\n\n\n\n\n30 rows √ó 11 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nAdd\nAdd-lo-95\nAdd-lo-80\nAdd-hi-80\nAdd-hi-95\nMulti\nMulti-lo-95\nMulti-lo-80\nMulti-hi-80\nMulti-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n80154.835938\n-44792.613281\n-1543.909790\n161853.578125\n205102.281250\n115310.718750\n3485.836914\n42192.359375\n188429.078125\n227135.593750\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n80150.132812\n-47015.281250\n-2998.863037\n163299.125000\n207315.546875\n116561.578125\n1745.947998\n41487.671875\n191635.484375\n231377.203125\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n80145.429688\n-49200.355469\n-4429.233887\n164720.093750\n209491.218750\n117812.437500\n1.485114\n40779.996094\n194844.875000\n235623.390625\n\n\n\n\n246 rows √ó 12 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds')\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nAdd\nAdd-lo-95\nAdd-lo-80\nAdd-hi-80\nAdd-hi-95\nMulti\nMulti-lo-95\nMulti-lo-80\nMulti-hi-80\nMulti-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n80154.835938\n-44792.613281\n-1543.909790\n161853.578125\n205102.281250\n115310.718750\n3485.836914\n42192.359375\n188429.078125\n227135.593750\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n80150.132812\n-47015.281250\n-2998.863037\n163299.125000\n207315.546875\n116561.578125\n1745.947998\n41487.671875\n191635.484375\n231377.203125\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n80145.429688\n-49200.355469\n-4429.233887\n164720.093750\n209491.218750\n117812.437500\n1.485114\n40779.996094\n194844.875000\n235623.390625\n\n\n\n\n246 rows √ó 12 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    df_plot = pd.concat([y_hist, y_pred]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=3 , )\n    colors = ['green', \"lime\"]\n    ax.fill_between(df_plot.index, \n                df_plot['Add-lo-80'], \n                df_plot['Add-hi-80'],\n                alpha=.20,\n                color='fuchsia',\n                label='Holt_level_80')\n    ax.fill_between(df_plot.index, \n                df_plot['Add-lo-95'], \n                df_plot['Add-hi-95'],\n                alpha=.3,\n                color='lime',\n                label='Holt_level_95')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel(\"Ads watched (hourly data)\", fontsize=20)\n    ax.set_xlabel('Hourly', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid(True)\n    plt.show()\n\n\nplot_forecasts(df, forecast_df, models=[\"Add\"])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/holt.html#cross-validation",
    "href": "docs/models/holt.html#cross-validation",
    "title": "Holt Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 30 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=30,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAdd\nMulti\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n1\n2017-09-18 06:00:00\n2017-09-18 05:00:00\n99440.0\n111573.328125\n112901.664062\n\n\n1\n2017-09-18 07:00:00\n2017-09-18 05:00:00\n97655.0\n111820.390625\n114476.921875\n\n\n1\n2017-09-18 08:00:00\n2017-09-18 05:00:00\n97655.0\n112067.453125\n116052.179688\n\n\n...\n...\n...\n...\n...\n...\n\n\n1\n2017-09-21 21:00:00\n2017-09-20 17:00:00\n103080.0\n148230.671875\n183085.953125\n\n\n1\n2017-09-21 22:00:00\n2017-09-20 17:00:00\n95155.0\n148541.937500\n184642.046875\n\n\n1\n2017-09-21 23:00:00\n2017-09-20 17:00:00\n80285.0\n148853.203125\n186198.140625\n\n\n\n\n90 rows √ó 5 columns"
  },
  {
    "objectID": "docs/models/holt.html#model-evaluation",
    "href": "docs/models/holt.html#model-evaluation",
    "title": "Holt Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Holt Model.\n\n\nfrom datasetsforecast.losses import rmse\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['ds', 'cutoff', 'y']).columns.tolist()\n    evals = []\n    for model in models:\n        eval_ = df.groupby(['unique_id', 'cutoff']).apply(lambda x: metric(x['y'].values, x[model].values)).to_frame() # Calculate loss for every unique_id, model and cutoff.\n        eval_.columns = [model]\n        evals.append(eval_)\n    evals = pd.concat(evals, axis=1)\n    evals = evals.groupby(['unique_id']).mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n    evals['best_model'] = evals.idxmin(axis=1)\n    return evals\n\n\nevaluation_df = evaluate_cross_validation(crossvalidation_df, rmse)\nevaluation_df\n\n\n\n\n\n\n\n\nAdd\nMulti\nbest_model\n\n\nunique_id\n\n\n\n\n\n\n\n1\n31882.099609\n46589.265625\nAdd"
  },
  {
    "objectID": "docs/models/holt.html#acknowledgements",
    "href": "docs/models/holt.html#acknowledgements",
    "title": "Holt Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/holt.html#references",
    "href": "docs/models/holt.html#references",
    "title": "Holt Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html",
    "href": "docs/models/dynamicstandardtheta.html",
    "title": "Dynamic Standard Theta Model",
    "section": "",
    "text": "Dynamic Standard Theta Model (DOTM)\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of DynamicStandardTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#table-of-contents",
    "href": "docs/models/dynamicstandardtheta.html#table-of-contents",
    "title": "Dynamic Standard Theta Model",
    "section": "",
    "text": "Dynamic Standard Theta Model (DOTM)\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of DynamicStandardTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#dynamic-standard-theta-models-dstm",
    "href": "docs/models/dynamicstandardtheta.html#dynamic-standard-theta-models-dstm",
    "title": "Dynamic Standard Theta Model",
    "section": "Dynamic Standard Theta Models (DSTM) ",
    "text": "Dynamic Standard Theta Models (DSTM) \nThe Dynamic Standard Theta Model is a case-specific variation of the Optimized Dynamic Theta Model.\nAlso, for \\(\\theta=2\\), we have a stochastic approach of Theta, which is referred to hereafter as the dynamic standard Theta model (DSTM)."
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#loading-libraries-and-data",
    "href": "docs/models/dynamicstandardtheta.html#loading-libraries-and-data",
    "title": "Dynamic Standard Theta Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/milk_production.csv\", usecols=[1,2])\ndf.head()\n\n\n\n\n\n\n\n\nmonth\nproduction\n\n\n\n\n0\n1962-01-01\n589\n\n\n1\n1962-02-01\n561\n\n\n2\n1962-03-01\n640\n\n\n3\n1962-04-01\n656\n\n\n4\n1962-05-01\n727\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1962-01-01\n589\n1\n\n\n1\n1962-02-01\n561\n1\n\n\n2\n1962-03-01\n640\n1\n\n\n3\n1962-04-01\n656\n1\n\n\n4\n1962-05-01\n727\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#explore-data-with-the-plot-method",
    "href": "docs/models/dynamicstandardtheta.html#explore-data-with-the-plot-method",
    "title": "Dynamic Standard Theta Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\n\nAdditive\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=12)\na.plot();\n\n\n\n\n\n\nMultiplicative\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=12)\na.plot();"
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#split-the-data-into-training-and-testing",
    "href": "docs/models/dynamicstandardtheta.html#split-the-data-into-training-and-testing",
    "title": "Dynamic Standard Theta Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our Dynamic Standard Theta Model 2. Data to test our model\nFor the test data we will use the last 12 months to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='1974-12-01'] \ntest = df[df.ds&gt;'1974-12-01']\n\n\ntrain.shape, test.shape\n\n((156, 3), (12, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"Monthly Milk Production\")\nplt.show()"
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#implementation-of-dynamicstandardtheta-with-statsforecast",
    "href": "docs/models/dynamicstandardtheta.html#implementation-of-dynamicstandardtheta-with-statsforecast",
    "title": "Dynamic Standard Theta Model",
    "section": "Implementation of DynamicStandardTheta with StatsForecast ",
    "text": "Implementation of DynamicStandardTheta with StatsForecast \nTo also know more about the parameters of the functions of the DynamicStandardTheta Model, they are listed below. For more information, visit the documentation.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\ndecomposition_type : str\n    Sesonal decomposition type, 'multiplicative' (default) or 'additive'.\nalias : str\n    Custom name of the model.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import DynamicTheta\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 12 # Monthly data \nhorizon = len(test) # number of predictions\n\nmodels = [DynamicTheta(season_length=season_length, \n                decomposition_type=\"additive\")] # multiplicative additive\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='MS', \n                   n_jobs=-1)\n\n\n\nFit Model\n\nsf.fit()\n\nStatsForecast(models=[DynamicTheta])\n\n\nLet‚Äôs see the results of our Dynamic Standard Theta model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nprint(result.keys())\nprint(result['fit'])\n\ndict_keys(['mse', 'amse', 'fit', 'residuals', 'm', 'states', 'par', 'n', 'modeltype', 'mean_y', 'decompose', 'decomposition_type', 'seas_forecast', 'fitted'])\nresults(x=array([393.28739991,   0.76875   ]), fn=10.787112115489615, nit=20, simplex=array([[399.92916541,   0.771875  ],\n       [393.28739991,   0.76875   ],\n       [384.74798713,   0.771875  ]]))\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\n-18.247131\n\n\n1\n-46.247131\n\n\n2\n17.140198\n\n\n...\n...\n\n\n153\n-58.941711\n\n\n154\n-91.055420\n\n\n155\n-42.624939\n\n\n\n\n156 rows √ó 1 columns\n\n\n\n\nimport scipy.stats as stats\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon, fitted=True)\nY_hat\n\n\n\n\n\n\n\n\nds\nDynamicTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n1975-01-01\n838.531555\n\n\n1\n1975-02-01\n800.154968\n\n\n1\n1975-03-01\n893.430786\n\n\n...\n...\n...\n\n\n1\n1975-10-01\n815.959351\n\n\n1\n1975-11-01\n786.716431\n\n\n1\n1975-12-01\n823.539368\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nDynamicTheta\n\n\nunique_id\n\n\n\n\n\n\n\n1\n1962-01-01\n589.0\n607.247131\n\n\n1\n1962-02-01\n561.0\n607.247131\n\n\n1\n1962-03-01\n640.0\n622.859802\n\n\n1\n1962-04-01\n656.0\n606.987793\n\n\n1\n1962-05-01\n727.0\n605.021179\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nDynamicTheta\nDynamicTheta-lo-95\nDynamicTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n1975-01-01\n838.531555\n741.237366\n954.407166\n\n\n1\n1975-02-01\n800.154968\n640.697327\n945.673157\n\n\n1\n1975-03-01\n893.430786\n703.900635\n1065.418579\n\n\n...\n...\n...\n...\n...\n\n\n1\n1975-10-01\n815.959351\n536.423035\n1086.643677\n\n\n1\n1975-11-01\n786.716431\n484.476562\n1033.687012\n\n\n1\n1975-12-01\n823.539368\n509.187408\n1104.107788\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nDynamicTheta\n\n\n\n\n0\n1\n1975-01-01\n838.531555\n\n\n1\n1\n1975-02-01\n800.154968\n\n\n2\n1\n1975-03-01\n893.430786\n\n\n...\n...\n...\n...\n\n\n9\n1\n1975-10-01\n815.959351\n\n\n10\n1\n1975-11-01\n786.716431\n\n\n11\n1\n1975-12-01\n823.539368\n\n\n\n\n12 rows √ó 3 columns\n\n\n\n\ntest['unique_id'] = test['unique_id'].astype(int)\nY_hat1 = test.merge(Y_hat, how='left', on=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nDynamicTheta\n\n\n\n\n0\n1975-01-01\n834\n1\n838.531555\n\n\n1\n1975-02-01\n782\n1\n800.154968\n\n\n2\n1975-03-01\n892\n1\n893.430786\n\n\n...\n...\n...\n...\n...\n\n\n9\n1975-10-01\n827\n1\n815.959351\n\n\n10\n1975-11-01\n797\n1\n786.716431\n\n\n11\n1975-12-01\n843\n1\n823.539368\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds')\nplot_df[['y', \"DynamicTheta\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Monthly Milk Production', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nDynamicTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n1975-01-01\n838.531555\n\n\n1\n1975-02-01\n800.154968\n\n\n1\n1975-03-01\n893.430786\n\n\n...\n...\n...\n\n\n1\n1975-10-01\n815.959351\n\n\n1\n1975-11-01\n786.716431\n\n\n1\n1975-12-01\n823.539368\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \n\nforecast_df\n\n\n\n\n\n\n\n\nds\nDynamicTheta\nDynamicTheta-lo-80\nDynamicTheta-hi-80\nDynamicTheta-lo-95\nDynamicTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n1\n1975-01-01\n838.531555\n765.423828\n927.285400\n741.237366\n954.407166\n\n\n1\n1975-02-01\n800.154968\n701.099854\n899.316223\n640.697327\n945.673157\n\n\n1\n1975-03-01\n893.430786\n758.326416\n1007.631287\n703.900635\n1065.418579\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n1975-10-01\n815.959351\n608.699524\n992.552856\n536.423035\n1086.643677\n\n\n1\n1975-11-01\n786.716431\n558.429626\n970.648560\n484.476562\n1033.687012\n\n\n1\n1975-12-01\n823.539368\n588.707153\n1031.565063\n509.187408\n1104.107788\n\n\n\n\n12 rows √ó 6 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nDynamicTheta\nDynamicTheta-lo-80\nDynamicTheta-hi-80\nDynamicTheta-lo-95\nDynamicTheta-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n1962-01-01\n589.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1962-02-01\n561.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1962-03-01\n640.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1975-10-01\nNaN\nNaN\n815.959351\n608.699524\n992.552856\n536.423035\n1086.643677\n\n\n1975-11-01\nNaN\nNaN\n786.716431\n558.429626\n970.648560\n484.476562\n1033.687012\n\n\n1975-12-01\nNaN\nNaN\n823.539368\n588.707153\n1031.565063\n509.187408\n1104.107788\n\n\n\n\n180 rows √ó 7 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=3 , )\n    colors = ['green', \"lime\"]\n    ax.fill_between(df_plot.index, \n                df_plot['DynamicTheta-lo-80'], \n                df_plot['DynamicTheta-lo-80'],\n                alpha=.20,\n                color='orange',\n                label='DynamicTheta_level_80')\n    ax.fill_between(df_plot.index, \n                df_plot['DynamicTheta-lo-95'], \n                df_plot['DynamicTheta-hi-95'],\n                alpha=.3,\n                color='lime',\n                label='DynamicTheta_level_95')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel(\"Montly Mil Production\", fontsize=20)\n    ax.set_xlabel('Month-Days', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid(True)\n    plt.show()\n\n\nplot_forecasts(train, test, forecast_df, models=['DynamicTheta'])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df, level=[95])"
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#cross-validation",
    "href": "docs/models/dynamicstandardtheta.html#cross-validation",
    "title": "Dynamic Standard Theta Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=12,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nDynamicTheta\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n1972-01-01\n1971-12-01\n826.0\n827.107239\n\n\n1\n1972-02-01\n1971-12-01\n799.0\n789.924194\n\n\n1\n1972-03-01\n1971-12-01\n890.0\n879.664429\n\n\n...\n...\n...\n...\n...\n\n\n1\n1974-10-01\n1973-12-01\n812.0\n804.398560\n\n\n1\n1974-11-01\n1973-12-01\n773.0\n775.329285\n\n\n1\n1974-12-01\n1973-12-01\n813.0\n811.767639\n\n\n\n\n36 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#model-evaluation",
    "href": "docs/models/dynamicstandardtheta.html#model-evaluation",
    "title": "Dynamic Standard Theta Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Dynamic Standard Theta model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"DynamicTheta\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  12.610596\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, model):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=12)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, test, Y_hat, model=\"DynamicTheta\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nDynamicTheta\n8.182119\n0.97361\n0.367965\n9.817624\n0.974804"
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#acknowledgements",
    "href": "docs/models/dynamicstandardtheta.html#acknowledgements",
    "title": "Dynamic Standard Theta Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/dynamicstandardtheta.html#references",
    "href": "docs/models/dynamicstandardtheta.html#references",
    "title": "Dynamic Standard Theta Model",
    "section": "References ",
    "text": "References \n\nKostas I. Nikolopoulos, Dimitrios D. Thomakos. Forecasting with the Theta Method-Theory and Applications. 2019 John Wiley & Sons Ltd.\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/arch.html",
    "href": "docs/models/arch.html",
    "title": "ARCH Model",
    "section": "",
    "text": "Introduction\nARCH Models\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of ARCH with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/arch.html#table-of-contents",
    "href": "docs/models/arch.html#table-of-contents",
    "title": "ARCH Model",
    "section": "",
    "text": "Introduction\nARCH Models\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of ARCH with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/arch.html#introduction",
    "href": "docs/models/arch.html#introduction",
    "title": "ARCH Model",
    "section": "Introduction ",
    "text": "Introduction \nFinancial time series analysis has been one of the hottest research topics in the recent decades. In this guide, we illustrate the stylized facts of financial time series by real financial data. To characterize these facts, new models different from the Box- Jenkins ones are needed. And for this reason, ARCH models were firstly proposed by R. F. Engle in 1982 and have been extended by a great number of scholars since then. We also demonstrate how to use Python and its libraries to implement ARCH.\nAs we have known, there are lot of time series that possess the ARCH effect, that is, although the (modeling residual) series is white noise, its squared series may be autocorrelated. What is more, in practice, a large number of financial time series are found having this property so that the ARCH effect has become one of the stylized facts from financial time series.\n\nStylized Facts of Financial Time Series\nNow we briefly list and describe several important stylized facts (features) of financial return series:\n\nFat (heavy) tails: The distribution density function of returns often has fatter (heavier) tails than the tails of the corresponding normal distribution density.\nARCH effect: Although the return series can often be seen as a white noise, its squared (and absolute) series may usually be autocorrelated, and these autocorrelations are hardly negative.\nVolatility clustering: Large changes in returns tend to cluster in time, and small changes tend to be followed by small changes.\nAsymmetry: As we have know , the distribution of asset returns is slightly negatively skewed. One possible explanation could be that traders react more strongly to unfavorable information than favorable information."
  },
  {
    "objectID": "docs/models/arch.html#definition-of-arch-models",
    "href": "docs/models/arch.html#definition-of-arch-models",
    "title": "ARCH Model",
    "section": "Definition of ARCH Models ",
    "text": "Definition of ARCH Models \nSpecifically, we give the definition of the ARCH model as follows.\nDefinition 1. An \\(\\text{ARCH(p)}\\) model with order \\(p‚â•1\\) is ofthe form\n\\[\\begin{equation}\n    \\left\\{\n        \\begin{array}{ll}\n         X_t =\\sigma_t \\varepsilon_t      \\\\\n         \\sigma_{t}^2 =\\omega+ \\alpha_1 X_{t-1}^2 + \\alpha_2 X_{t-2}^2  + \\cdots+  \\alpha_p X_{t-p}^2  \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nwhere \\(\\omega ‚â• 0, \\alpha_i ‚â• 0\\), and \\(\\alpha_p &gt; 0\\) are constants, \\(\\varepsilon_t \\sim iid(0, 1)\\), and \\(\\varepsilon_t\\) is independent of \\(\\{X_k;k ‚â§ t ‚àí 1 \\}\\). A stochastic process \\(X_t\\) is called an \\(ARCH(p)\\) process if it satisfies Eq. (1).\nBy Definition 1, \\(\\sigma_{t}^2\\) (and \\(\\sigma_t\\) ) is independent of \\(\\varepsilon_t\\) . Besides, usually it is further assumed that \\(\\varepsilon_t \\sim N(0, 1)\\). Sometimes, however, we need to further suppose that \\(\\varepsilon_t\\) follows a standardized (skew) Student‚Äôs T distribution or a generalized error distribution in order to capture more features of a financial time series.\nLet \\(\\mathscr{F}_s\\) denote the information set generated by \\(\\{X_k;k ‚â§ s \\}\\), namely, the sigma field \\(\\sigma(X_k;k ‚â§ s)\\). It is easy to see that \\(\\mathscr{F}_s\\) is independent of \\(\\varepsilon_t\\) for any \\(s &lt;t\\). According to Definition 1 and the properties of the conditional mathematical expectation, we have that\n\\[E(X_t|\\mathscr{F}_{t‚àí1}) = E(\\sigma_t \\varepsilon_t|\\mathscr{F}_{t‚àí1}) = \\sigma_t E( \\varepsilon_t|\\mathscr{F}_{t‚àí1}) = \\sigma_t E(\\varepsilon_t) = 0 \\tag 2\\]\nand\n\\[ \\text{Var}(X_{t}^2| \\mathscr{F}_{t‚àí1}) = E(X_{t}^2|\\mathscr{F}_{t‚àí1}) =  E(\\sigma_{t}^2 \\varepsilon_{t}^2|\\mathscr{F}_{t‚àí1}) = \\sigma_{t}^2 E(\\varepsilon_{t}^2|\\mathscr{F}_{t‚àí1}) = \\sigma_{t}^2 E(\\varepsilon_{t}^2) = \\sigma_{t}^2. \\]\nThis implies that \\(\\sigma_{t}^2\\) is the conditional variance of \\(X_t\\) and it evolves according to the previous values of \\(\\{X_{k}^2; t ‚àíp ‚â§ k ‚â§ t ‚àí1\\}\\) like an \\(\\text{AR}(p)\\) model. And so Model (1) is named an \\(\\text{ARCH}(p)\\) model.\nAs an example of \\(\\text{ARCH}(p)\\) models, let us consider the \\(\\text{ARCH(1)}\\) model\n\\[\\begin{equation}\n    \\left\\{\n        \\begin{array}{ll} \\tag 3\n         X_t =\\sigma_t \\varepsilon_t      \\\\\n         \\sigma_{t}^2 =\\omega+ \\alpha_1 X_{t-1}^2   \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nExplicitly, the unconditional mean \\[E(X_t) = E(\\sigma_t \\varepsilon_t) = E(\\sigma_t) E(\\varepsilon_t) = 0. \\]\nAdditionally, the ARCH(1) model can be expressed as\n\\[X_{t}^2 =\\sigma_{t}^2 +X_{t}^2 ‚àí \\sigma_{t}^2  =\\omega +\\alpha_1 X_{t-1}^2 +\\sigma_{t}^2 \\varepsilon_{t}^2 ‚àí\\sigma_{t}^2  =\\omega +\\alpha_1 X_{t}^2 +\\eta_t ,\\]\nthat is, \\[X_{t}^2 =\\omega +\\alpha_1 X_{t}^2 +\\eta_t \\tag 4 \\]\nwhere \\(\\eta_t = \\sigma_{t}^2(\\varepsilon_{t}^2 ‚àí 1)\\). It can been shown that \\(\\eta_t\\) is a new white noise, which is left as an exercise for reader. Hence, if \\(0 &lt; \\alpha_1 &lt; 1\\), Eq. (4) is a stationary \\(\\text{AR(1)}\\) model for the series Xt2. Thus, the unconditional variance\n\\[Var ( X_t ) = E( X_{t}^2 ) = E(\\omega+ \\alpha_1 X_{t-1}^2 + \\eta_t ) = \\omega+ \\alpha_1 E( X_{t}^2 ) ,\\]\nthat is, \\[Var (X_t) = E (X_{t}^2 ) =\\frac{\\omega}{1-\\alpha_1}\\]\nMoreover, for \\(h &gt; 0\\), in light of the properties of the conditional mathematical expectation and by (2), we have that\n\\[E(X_{t+h} X_t) = E(E(X_{t+h} X_t|\\mathscr{F}_{t+h-1})) = E(X_t E(X_{t+h}|\\mathscr{F}_{t+h-1})) = 0.\\]\nIn conclusion, if \\(0 &lt; \\alpha_1 &lt; 1\\), we have that:\n\nAny \\(\\text{ARCH}(1)\\) process \\(\\{X_t \\}\\) defined by Eqs.(3) follows a white noise \\(WN(0, \\omega/(1 ‚àí \\alpha_1))\\) .\nSince \\(X_{t}^2\\) is an \\(\\text{AR}(1)\\) process defined by (4), \\(\\text{Corr}(X_{t}^2,X_{t+h}^2) = \\alpha_{1}^{|h|} &gt; 0\\), which reveals the ARCH effect.\nIt is clear that \\(E(\\eta_t|\\mathscr{F}_s)=0\\) for any \\(t&gt;s\\),and with Eq.(4),for any \\(k&gt;1\\): \\[Var(X_{t+k} |\\mathscr{F}_t ) = E(X_{t+K}^2 |\\mathscr{F}_t)\\] \\[= E(\\omega + \\alpha_1 X_{t+k-1}+ \\eta_{t+k}|\\mathscr{F}_t )\\] \\[= \\omega + \\alpha_1 Var(X_{t+k‚àí1}|\\mathscr{F}_t),\\]\n\nwhich reflects the volatility clustering, that is, large (small) volatility is followed by large (small) one.\nIn addition, we are able to prove that Xt defined by Eq. (3) has heavier tails than the corresponding normal distribution. At last, note that these properties of the ARCH(1) model can be generalized to ARCH(p) models.\n\nAdvantages and disadvantages of the Autoregressive Conditional Heteroskedasticity (ARCH) model:\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\n- The ARCH model is useful for modeling volatility in financial time series, which is important for investment decision making and risk management.\n- The ARCH model assumes that the forecast errors are independent and identically distributed, which may not be realistic in some cases.\n\n\n- The ARCH model takes heteroscedasticity into account, which means that it can model time series with variances that change over time.\n- The ARCH model can be difficult to fit to data with many parameters, which may require large amounts of data or advanced estimation techniques.\n\n\n- The ARCH model is relatively easy to use and can be implemented with standard econometrics software.\n- The ARCH model does not take into account the possible relationship between the mean and the variance of the time series, which may be important in some cases.\n\n\n\nNote:\nThe ARCH model is a useful tool for modeling volatility in financial time series, but like any econometric model, it has limitations and should be used with caution depending on the specific characteristics of the data being modeled.\n\n\nAutoregressive Conditional Heteroskedasticity (ARCH) Applications\n\nFinance ‚Äì The ARCH model is widely used in finance to model volatility in financial time series, such as stock prices, exchange rates, interest rates, etc.\nEconomics - The ARCH model can be used to model volatility in economic data, such as GDP, inflation, unemployment, among others.\nEngineering - The ARCH model can be used in engineering to model volatility in data related to energy, climate, pollution, industrial production, among others.\nSocial Sciences - The ARCH model can be used in the social sciences to model volatility in data related to demography, health, education, among others.\nBiology - The ARCH model can be used in biology to model volatility in data related to evolution, genetics, epidemiology, among others."
  },
  {
    "objectID": "docs/models/arch.html#loading-libraries-and-data",
    "href": "docs/models/arch.html#loading-libraries-and-data",
    "title": "ARCH Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\nLet‚Äôs pull the S&P500 stock data from the Yahoo Finance site.\n\nimport pandas as pd\nimport time\nfrom datetime import datetime\n\nticker = '^GSPC'\nperiod1 = int(time.mktime(datetime(2015, 1, 1, 23, 59).timetuple()))\nperiod2 = int(time.mktime(datetime.now().timetuple()))\ninterval = '1d' # 1d, 1m\n\nquery_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n\nSP_500 = pd.read_csv(query_string)\nSP_500.head()\n\n\n\n\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\n\n\n0\n2015-01-02\n2058.899902\n2072.360107\n2046.040039\n2058.199951\n2058.199951\n2708700000\n\n\n1\n2015-01-05\n2054.439941\n2054.439941\n2017.339966\n2020.579956\n2020.579956\n3799120000\n\n\n2\n2015-01-06\n2022.150024\n2030.250000\n1992.439941\n2002.609985\n2002.609985\n4460110000\n\n\n3\n2015-01-07\n2005.550049\n2029.609985\n2005.550049\n2025.900024\n2025.900024\n3805480000\n\n\n4\n2015-01-08\n2030.609985\n2064.080078\n2030.609985\n2062.139893\n2062.139893\n3934010000\n\n\n\n\n\n\n\n\ndf=SP_500[[\"Date\",\"Close\"]]\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2015-01-02\n2058.199951\n1\n\n\n1\n2015-01-05\n2020.579956\n1\n\n\n2\n2015-01-06\n2002.609985\n1\n\n\n3\n2015-01-07\n2025.900024\n1\n\n\n4\n2015-01-08\n2062.139893\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds            object\ny            float64\nunique_id     object\ndtype: object\n\n\nWe need to convert the object type to datetime.\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/arch.html#explore-data-with-the-plot-method",
    "href": "docs/models/arch.html#explore-data-with-the-plot-method",
    "title": "ARCH Model",
    "section": "Explore data with the plot method ",
    "text": "Explore data with the plot method \nPlot a series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nThe Augmented Dickey-Fuller Test\nAn Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\nNull Hypothesis: Time Series is non-stationary. It gives a time-dependent trend. Alternate Hypothesis: Time Series is stationary. In another term, the series doesn‚Äôt depend on time.\nADF or t Statistic &lt; critical values: Reject the null hypothesis, time series is stationary. ADF or t Statistic &gt; critical values: Failed to reject the null hypothesis, time series is non-stationary.\nLet‚Äôs check if our series that we are analyzing is a stationary series. Let‚Äôs create a function to check, using the Dickey Fuller test\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],'S&P500')\n\nDickey-Fuller test results for columns: S&P500\nTest Statistic          -0.814971\np-value                  0.814685\nNo Lags Used            10.000000\n                          ...    \nCritical Value (1%)     -3.433341\nCritical Value (5%)     -2.862861\nCritical Value (10%)    -2.567473\nLength: 7, dtype: float64\nConclusion:====&gt;\nThe null hypothesis cannot be rejected\nThe data is not stationary\n\n\nIn the previous result we can see that the Augmented_Dickey_Fuller test gives us a p-value of 0.864700, which tells us that the null hypothesis cannot be rejected, and on the other hand the data of our series are not stationary.\nWe need to differentiate our time series, in order to convert the data to stationary.\n\n\nReturn Series\nSince the 1970s, the financial industry has been very prosperous with advancement of computer and Internet technology. Trade of financial products (including various derivatives) generates a huge amount of data which form financial time series. For finance, the return on a financial product is most interesting, and so our attention focuses on the return series. If {Pt } is the closing price at time t for a certain financial product, then the return on this product is\n\\[X_t = \\frac{(P_t ‚àí P_{t‚àí1})}{P_{t‚àí1}} ‚âà log(P_t ) ‚àí log(P_{t‚àí1}).\\]\nIt is return series \\(\\{X_t \\}\\) that have been much independently studied. And important stylized features which are common across many instruments, markets, and time periods have been summarized. Note that if you purchase the financial product, then it becomes your asset, and its returns become your asset returns. Now let us look at the following examples.\nWe can estimate the series of returns using the pandas, DataFrame.pct_change() function. The pct_change() function has a periods parameter whose default value is 1. If you want to calculate a 30-day return, you must change the value to 30.\n\ndf['return'] = 100 * df[\"y\"].pct_change()\ndf.dropna(inplace=True, how='any')\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nreturn\n\n\n\n\n1\n2015-01-05\n2020.579956\n1\n-1.827811\n\n\n2\n2015-01-06\n2002.609985\n1\n-0.889347\n\n\n3\n2015-01-07\n2025.900024\n1\n1.162984\n\n\n4\n2015-01-08\n2062.139893\n1\n1.788828\n\n\n5\n2015-01-09\n2044.810059\n1\n-0.840381\n\n\n\n\n\n\n\n\nimport plotly.express as px\nfig = px.line(df, x=df[\"ds\"], y=\"return\",title=\"SP500 Return Chart\",template = \"plotly_dark\")\nfig.show()\n\n\n                                                \n\n\n\n\nCreating Squared Returns\n\ndf['sq_return'] = df[\"return\"].mul(df[\"return\"])\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nreturn\nsq_return\n\n\n\n\n1\n2015-01-05\n2020.579956\n1\n-1.827811\n3.340891\n\n\n2\n2015-01-06\n2002.609985\n1\n-0.889347\n0.790938\n\n\n3\n2015-01-07\n2025.900024\n1\n1.162984\n1.352532\n\n\n4\n2015-01-08\n2062.139893\n1\n1.788828\n3.199906\n\n\n5\n2015-01-09\n2044.810059\n1\n-0.840381\n0.706240\n\n\n\n\n\n\n\n\n\nReturns vs Squared Returns\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(go.Scatter(x=df[\"ds\"], y=df[\"return\"],\n                         mode='lines',\n                         name='return'),\nrow=1, col=1\n)\n\n\nfig.add_trace(go.Scatter(x=df[\"ds\"], y=df[\"sq_return\"],\n                         mode='lines',\n                         name='sq_return'), \n    row=1, col=2\n)\n\nfig.update_layout(height=600, width=800, title_text=\"Returns vs Squared Returns\", template = \"plotly_dark\")\nfig.show()\n\n\n                                                \n\n\n\nfrom scipy.stats import probplot, moment\nfrom statsmodels.tsa.stattools import adfuller, q_stat, acf\nimport numpy as np\nimport seaborn as sns\n\ndef plot_correlogram(x, lags=None, title=None):    \n    lags = min(10, int(len(x)/5)) if lags is None else lags\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 8))\n    x.plot(ax=axes[0][0], title='Return')\n    x.rolling(21).mean().plot(ax=axes[0][0], c='k', lw=1)\n    q_p = np.max(q_stat(acf(x, nlags=lags), len(x))[1])\n    stats = f'Q-Stat: {np.max(q_p):&gt;8.2f}\\nADF: {adfuller(x)[1]:&gt;11.2f}'\n    axes[0][0].text(x=.02, y=.85, s=stats, transform=axes[0][0].transAxes)\n    probplot(x, plot=axes[0][1])\n    mean, var, skew, kurtosis = moment(x, moment=[1, 2, 3, 4])\n    s = f'Mean: {mean:&gt;12.2f}\\nSD: {np.sqrt(var):&gt;16.2f}\\nSkew: {skew:12.2f}\\nKurtosis:{kurtosis:9.2f}'\n    axes[0][1].text(x=.02, y=.75, s=s, transform=axes[0][1].transAxes)\n    plot_acf(x=x, lags=lags, zero=False, ax=axes[1][0])\n    plot_pacf(x, lags=lags, zero=False, ax=axes[1][1])\n    axes[1][0].set_xlabel('Lag')\n    axes[1][1].set_xlabel('Lag')\n    fig.suptitle(title+ f'Dickey-Fuller: {adfuller(x)[1]:&gt;11.2f}', fontsize=14)\n    sns.despine()\n    fig.tight_layout()\n    fig.subplots_adjust(top=.9)\n\n\nplot_correlogram(df[\"return\"], lags=30, title=\"Time Series Analysis plot \\n\")\n\n\n\n\n\n\nLjung-Box Test\nLjung-Box is a test for autocorrelation that we can use in tandem with our ACF and PACF plots. The Ljung-Box test takes our data, optionally either lag values to test, or the largest lag value to consider, and whether to compute the Box-Pierce statistic. Ljung-Box and Box-Pierce are two similar test statisitcs, Q , that are compared against a chi-squared distribution to determine if the series is white noise. We might use the Ljung-Box test on the residuals of our model to look for autocorrelation, ideally our residuals would be white noise.\n\nHo : The data are independently distributed, no autocorrelation.\nHa : The data are not independently distributed; they exhibit serial correlation.\n\nThe Ljung-Box with the Box-Pierce option will return, for each lag, the Ljung-Box test statistic, Ljung-Box p-values, Box-Pierce test statistic, and Box-Pierce p-values.\nIf \\(p&lt;\\alpha (0.05)\\) we reject the null hypothesis.\n\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\nljung_res = acorr_ljungbox(df[\"return\"], lags= 40, boxpierce=True)\n\nljung_res.head()\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\nbp_stat\nbp_pvalue\n\n\n\n\n1\n49.222273\n2.285409e-12\n49.155183\n2.364927e-12\n\n\n2\n62.991348\n2.097020e-14\n62.899234\n2.195861e-14\n\n\n3\n63.944944\n8.433622e-14\n63.850663\n8.834380e-14\n\n\n4\n74.343652\n2.742989e-15\n74.221024\n2.911751e-15\n\n\n5\n80.234862\n7.494100e-16\n80.093498\n8.022242e-16"
  },
  {
    "objectID": "docs/models/arch.html#split-the-data-into-training-and-testing",
    "href": "docs/models/arch.html#split-the-data-into-training-and-testing",
    "title": "ARCH Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our ARCH model\nData to test our model\n\nFor the test data we will use the last 30 day to test and evaluate the performance of our model.\n\ndf=df[[\"ds\",\"unique_id\",\"return\"]]\ndf.columns=[\"ds\", \"unique_id\", \"y\"]\n\n\ntrain = df[df.ds&lt;='2023-05-24'] # Let's forecast the last 30 days\ntest = df[df.ds&gt;'2023-05-24']\n\n\ntrain.shape, test.shape\n\n((2112, 3), (87, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.show()"
  },
  {
    "objectID": "docs/models/arch.html#implementation-of-arch-with-statsforecast",
    "href": "docs/models/arch.html#implementation-of-arch-with-statsforecast",
    "title": "ARCH Model",
    "section": "Implementation of ARCH with StatsForecast ",
    "text": "Implementation of ARCH with StatsForecast \nTo also know more about the parameters of the functions of the ARCH Model, they are listed below. For more information, visit the documentation\np : int\n    Number of lagged versions of the series.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast \nfrom statsforecast.models import ARCH\n\n\n\nBuilding Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful.season_length.\n\nseason_length = 7 # Daily data\nhorizon = len(test) # number of predictions biasadj=True, include_drift=True,\n\nmodels = [ARCH(p=2,)]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='C', # custom business day frequency\n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[ARCH(2)])\n\n\nLet‚Äôs see the results of our ARCH model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'p': 2,\n 'q': 0,\n 'coeff': array([0.44320919, 0.34706759, 0.35171967]),\n 'message': 'Optimization terminated successfully',\n 'y_vals': array([-1.12220268, -0.73186004]),\n 'sigma2_vals': array([1.38768694,        nan, 1.89277546, ..., 0.76423015, 0.45064543,\n        0.88036943]),\n 'fitted': array([        nan,         nan,  2.23474473, ..., -1.48032981,\n         1.10018826, -0.98050094]),\n 'actual_residuals': array([        nan,         nan, -1.07176046, ...,  1.4958333 ,\n        -2.22239094,  0.2486409 ])}\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"actual_residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n2\n-1.071760\n\n\n...\n...\n\n\n2109\n1.495833\n\n\n2110\n-2.222391\n\n\n2111\n0.248641\n\n\n\n\n2112 rows √ó 1 columns\n\n\n\n\nimport scipy.stats as stats\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\n# plot[1,1]\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\n# plot\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\n# plot\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\n# plot\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon, fitted=True)\n\nY_hat\n\n\n\n\n\n\n\n\nds\nARCH(2)\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-05-25\n1.681836\n\n\n1\n2023-05-26\n-0.777028\n\n\n1\n2023-05-29\n-0.677960\n\n\n...\n...\n...\n\n\n1\n2023-09-20\n0.136752\n\n\n1\n2023-09-21\n0.082173\n\n\n1\n2023-09-22\n-0.450958\n\n\n\n\n87 rows √ó 2 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nARCH(2)\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2015-01-05\n-1.827811\nNaN\n\n\n1\n2015-01-06\n-0.889347\nNaN\n\n\n1\n2015-01-07\n1.162984\n2.234745\n\n\n1\n2015-01-08\n1.788828\n-0.667577\n\n\n1\n2015-01-09\n-0.840381\n-0.752437\n\n\n\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nARCH(2)\nARCH(2)-lo-95\nARCH(2)-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2023-05-25\n1.681836\n-0.419322\n3.782995\n\n\n1\n2023-05-26\n-0.777028\n-3.939044\n2.384989\n\n\n1\n2023-05-29\n-0.677960\n-3.907244\n2.551323\n\n\n...\n...\n...\n...\n...\n\n\n1\n2023-09-20\n0.136752\n-0.795371\n1.068876\n\n\n1\n2023-09-21\n0.082173\n-0.852268\n1.016615\n\n\n1\n2023-09-22\n-0.450958\n-1.337117\n0.435202\n\n\n\n\n87 rows √ó 4 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nARCH(2)\n\n\n\n\n0\n1\n2023-05-25\n1.681836\n\n\n1\n1\n2023-05-26\n-0.777028\n\n\n2\n1\n2023-05-29\n-0.677960\n\n\n...\n...\n...\n...\n\n\n84\n1\n2023-09-20\n0.136752\n\n\n85\n1\n2023-09-21\n0.082173\n\n\n86\n1\n2023-09-22\n-0.450958\n\n\n\n\n87 rows √ó 3 columns\n\n\n\n\n# Merge the forecasts with the true values\ntest['unique_id'] = test['unique_id'].astype(int)\nY_hat1 = test.merge(Y_hat, how='left', on=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\nds\nunique_id\ny\nARCH(2)\n\n\n\n\n0\n2023-05-25\n1\n0.875758\n1.681836\n\n\n1\n2023-05-26\n1\n1.304909\n-0.777028\n\n\n2\n2023-05-30\n1\n0.001660\n-0.968701\n\n\n...\n...\n...\n...\n...\n\n\n84\n2023-09-26\n1\n-1.473453\nNaN\n\n\n85\n2023-09-27\n1\n0.022931\nNaN\n\n\n86\n2023-09-28\n1\n0.589317\nNaN\n\n\n\n\n87 rows √ó 4 columns\n\n\n\n\n# Merge the forecasts with the true values\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds')\nplot_df[['y', \"ARCH(2)\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Year ', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\nplt.show()\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nARCH(2)\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-05-25\n1.681836\n\n\n1\n2023-05-26\n-0.777028\n\n\n1\n2023-05-29\n-0.677960\n\n\n...\n...\n...\n\n\n1\n2023-09-20\n0.136752\n\n\n1\n2023-09-21\n0.082173\n\n\n1\n2023-09-22\n-0.450958\n\n\n\n\n87 rows √ó 2 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \n\nforecast_df\n\n\n\n\n\n\n\n\nds\nARCH(2)\nARCH(2)-lo-95\nARCH(2)-lo-80\nARCH(2)-hi-80\nARCH(2)-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n1\n2023-05-25\n1.681836\n-0.419322\n0.307963\n3.055710\n3.782995\n\n\n1\n2023-05-26\n-0.777028\n-3.939044\n-2.844559\n1.290504\n2.384989\n\n\n1\n2023-05-29\n-0.677960\n-3.907244\n-2.789475\n1.433555\n2.551323\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2023-09-20\n0.136752\n-0.795371\n-0.472731\n0.746235\n1.068876\n\n\n1\n2023-09-21\n0.082173\n-0.852268\n-0.528825\n0.693172\n1.016615\n\n\n1\n2023-09-22\n-0.450958\n-1.337117\n-1.030386\n0.128471\n0.435202\n\n\n\n\n87 rows √ó 6 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\ndf_plot=pd.concat([df, forecast_df]).set_index('ds').tail(220)\ndf_plot\n\n\n\n\n\n\n\n\nunique_id\ny\nARCH(2)\nARCH(2)-lo-95\nARCH(2)-lo-80\nARCH(2)-hi-80\nARCH(2)-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n2023-03-21\n1\n1.298219\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2023-03-22\n1\n-1.646322\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2023-03-23\n1\n0.298453\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-09-20\nNaN\nNaN\n0.136752\n-0.795371\n-0.472731\n0.746235\n1.068876\n\n\n2023-09-21\nNaN\nNaN\n0.082173\n-0.852268\n-0.528825\n0.693172\n1.016615\n\n\n2023-09-22\nNaN\nNaN\n-0.450958\n-1.337117\n-1.030386\n0.128471\n0.435202\n\n\n\n\n220 rows √ó 7 columns\n\n\n\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2 )\n    colors = ['green']\n    # Specify graph features:\n    ax.fill_between(df_plot.index, \n                df_plot['ARCH(2)-lo-80'], \n                df_plot['ARCH(2)-hi-80'],\n                alpha=.20,\n                color='lime',\n                label='ARCH(2)_level_80')\n    ax.fill_between(df_plot.index, \n                df_plot['ARCH(2)-lo-95'], \n                df_plot['ARCH(2)-hi-95'],\n                alpha=.2,\n                color='white',\n                label='ARCH(2)_level_95')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel(\"Return\", fontsize=20)\n    ax.set_xlabel('Month-Days', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid(True)\n    plt.show()\n\n\nplot_forecasts(train, test, forecast_df, models=[\"ARCH(2)\"])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below."
  },
  {
    "objectID": "docs/models/arch.html#cross-validation",
    "href": "docs/models/arch.html#cross-validation",
    "title": "ARCH Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=6,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nARCH(2)\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2022-12-21\n2022-12-20\n-0.605272\n1.889850\n\n\n1\n2022-12-22\n2022-12-20\n-2.492167\n-0.850434\n\n\n1\n2022-12-23\n2022-12-20\n-1.113775\n-0.742012\n\n\n...\n...\n...\n...\n...\n\n\n1\n2023-05-22\n2023-01-23\n0.015503\n0.135570\n\n\n1\n2023-05-23\n2023-01-23\n-1.122203\n0.081367\n\n\n1\n2023-05-24\n2023-01-23\n-0.731860\n-0.446374\n\n\n\n\n435 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/arch.html#model-evaluation",
    "href": "docs/models/arch.html#model-evaluation",
    "title": "ARCH Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, ARCH.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"ARCH(2)\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  1.3816124\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import mae, mape, mase, rmse, smape\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, model):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=12)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, test, Y_hat, model=\"ARCH(2)\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nARCH(2)\n0.935485\n1064.149021\nNaN\n1.152612\n138.403076"
  },
  {
    "objectID": "docs/models/arch.html#acknowledgements",
    "href": "docs/models/arch.html#acknowledgements",
    "title": "ARCH Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/arch.html#references",
    "href": "docs/models/arch.html#references",
    "title": "ARCH Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007..\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html",
    "href": "docs/models/dynamicoptimizedtheta.html",
    "title": "Dynamic Optimized Theta Model",
    "section": "",
    "text": "Introduction\nDynamic Optimized Theta Model (DOTM)\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of DynamicOptimizedTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#table-of-contents",
    "href": "docs/models/dynamicoptimizedtheta.html#table-of-contents",
    "title": "Dynamic Optimized Theta Model",
    "section": "",
    "text": "Introduction\nDynamic Optimized Theta Model (DOTM)\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of DynamicOptimizedTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#introduction",
    "href": "docs/models/dynamicoptimizedtheta.html#introduction",
    "title": "Dynamic Optimized Theta Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Dynamic Optimized Theta Model (DOTM) is a forecasting technique that is used to predict future values of a time series. It is a variant of the Theta method, which combines exponential smoothing and a linear trend to forecast future values.\nThe DOTM extends the Theta method by introducing a dynamic optimization process that selects the optimal smoothing parameters for the exponential smoothing component and the optimal weights for the linear trend component based on the historical data. This optimization process is performed iteratively using a genetic algorithm that searches for the combination of parameters that minimizes the forecast error.\nThe DOTM is designed to handle time series data that exhibit non-linear and non-stationary behavior over time. It is particularly useful for forecasting time series with complex patterns such as seasonality, trend, and cyclical fluctuations.\nThe DOTM has several advantages over other forecasting methods. First, it is a simple and easy-to-implement method that does not require extensive statistical knowledge. Second, it is a highly adaptable method that can be customized to fit a wide range of time series data. Third, it is a robust method that can handle missing data, outliers, and other anomalies in the time series.\nOverall, the Dynamic Optimized Theta Model is a powerful forecasting technique that can be used to generate accurate and reliable predictions for a wide range of time series data."
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#dynamic-optimized-theta-models-dotm",
    "href": "docs/models/dynamicoptimizedtheta.html#dynamic-optimized-theta-models-dotm",
    "title": "Dynamic Optimized Theta Model",
    "section": "Dynamic Optimized Theta Models (DOTM) ",
    "text": "Dynamic Optimized Theta Models (DOTM) \nSo far, we have set \\(A_n\\) and \\(B_n\\) as fixed coefficients for all \\(t\\). We will now consider these coefficients as dynamic functions; i.e., for updating the state \\(t\\) to \\(t+1\\) we will only consider the prior information \\(Y_1, \\cdots, Y_t\\) when computing \\(A_t\\) and \\(B_t\\). Hence, We replace \\(A_n\\) and \\(B_n\\) in equations (3) and (4) of the notebook of the optimized theta model with \\(A_t\\) and \\(B_t\\). Then, after applying the new Eq. (4) to the new Eq. (3) and rewriting the result at time \\(t\\) with \\(h=1\\), we have\n\\[\\hat Y_{t+1|t}=\\ell_{t}+(1-\\frac{1}{\\theta}) \\{(1-\\alpha)^t A_t +[\\frac{1-(1-\\alpha)^{t+1}}{\\alpha}] B_t  \\tag{1} \\} \\]\nThen, assuming additive one-step-ahead errors and rewriting Eqs. (3) (see AutoTheta Model), (1), we obtain\n\\[Y_t=\\mu_t +\\varepsilon_t \\tag{2} \\] \\[\\mu_t=\\ell_{t-1}+(1-\\frac{1}{\\theta}) [(1-\\alpha)^{t-1} A_{t-1} +(\\frac{1-(1-\\alpha)^{t}}{\\alpha} ) B_{t-1} \\tag{3} ] \\] \\[\\ell_{t}=\\alpha Y_t+ (1-\\alpha) \\ell_{t-1} \\tag {4} \\] \\[A_t=\\bar Y_t - \\frac{t+1}{2} B_t \\tag {5}  \\] \\[B_t=\\frac{1}{t+1} [(t-2) B_{t-1} +\\frac{6}{t} (Y_t - \\bar Y_{t-1}) ] \\tag {6} \\] \\[\\bar Y_t=\\frac{1}{t} [(t-1) \\bar Y_{t-1} + Y_t ] \\tag {7} \\]\nfor \\(t=1, \\cdots ,n\\). Eqs. (2), (3), (4), (5), (6), (7) configure a state space model with parameters \\(\\ell_{0} \\in \\mathbb{R}, \\alpha \\in (0,1)\\), and \\(\\theta \\in [1,\\infty )\\). The initialisation of the states is performed assuming \\(A_0 =B_0=B_1=\\bar Y_0 =0\\). From here on, we will refer to this model as the dynamic optimised Theta model (DOTM).\nAn important property of the DOTM is that when \\(\\theta=1\\), which implies that \\(Z_t(1)=Y_t\\), the forecasting vector given by Eq. (3)(See OTM) will be equal to \\[\\hat Y_{t+h|t} = \\tilde Z_{t+h|t} \\]\nThus, when \\(\\theta=1\\), the DOTM is the SES method. When \\(\\theta&gt;1\\), DOTM (as SES-d) acts as a extension of SES, by adding a long-term component.\nThe out-of-sample one-step-ahead forecasts produced by DOTM at origin are given by\n\\[\\hat Y_{n+1|n}=E[Y_{n+1|Y_1, \\cdots, Y_n} ]=\\ell_{n} +(1-\\frac{1}{\\theta}) \\{(1-\\alpha)^n A_n + [\\frac{1-(1-\\alpha)^{n+1}}{\\alpha}] B_n  \\}  \\tag{8}\\]\nfor a horizon \\(h \\geq 2\\), the forecast \\(\\hat Y_{n+2|n}, \\cdots , \\hat Y_{n+h|n}\\) are computed recursively using Eqs. (3), (4), (5), (6), (7), (8) by replacing the non-observed values \\(Y_{n+1}, \\cdots , Y_{n+h-1}\\) with their expected values \\(\\hat Y_{n+1|n}, \\cdots , \\hat Y_{n+h-1|n}\\). The conditional variance \\(Var[Y_{n+h}|Y_{1}, \\cdots, Y_n ]\\) is hard to write analytically. However, the variance and prediction intervals for \\(Y_{n+h}\\) can be estimated using the bootstrapping technique, where a (usually large) sample of possible values of \\(Y_{n+h}\\) is simulated from the estimated model.\nNote that, in contrast to STheta, STM and OTM, the forecasts produced by DSTM and DOTM are not necessary linear. This is also a fundamental difference between DSTM/DOTM and SES-d: while the long-term trend () in SES-d is constant, this is not the case for DSTM/DOTM, for either the in-sample fit or the out-of-sample predictions."
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#loading-libraries-and-data",
    "href": "docs/models/dynamicoptimizedtheta.html#loading-libraries-and-data",
    "title": "Dynamic Optimized Theta Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\nimport pandas as pd\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/milk_production.csv\", usecols=[1,2])\ndf.head()\n\n\n\n\n\n\n\n\nmonth\nproduction\n\n\n\n\n0\n1962-01-01\n589\n\n\n1\n1962-02-01\n561\n\n\n2\n1962-03-01\n640\n\n\n3\n1962-04-01\n656\n\n\n4\n1962-05-01\n727\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1962-01-01\n589\n1\n\n\n1\n1962-02-01\n561\n1\n\n\n2\n1962-03-01\n640\n1\n\n\n3\n1962-04-01\n656\n1\n\n\n4\n1962-05-01\n727\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#explore-data-with-the-plot-method",
    "href": "docs/models/dynamicoptimizedtheta.html#explore-data-with-the-plot-method",
    "title": "Dynamic Optimized Theta Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df, )\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\n\nAdditive\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=12)\na.plot();\n\n\n\n\n\n\nMultiplicative\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=12)\na.plot();"
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#split-the-data-into-training-and-testing",
    "href": "docs/models/dynamicoptimizedtheta.html#split-the-data-into-training-and-testing",
    "title": "Dynamic Optimized Theta Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our Dynamic Optimized Theta Model(DOTM).\nData to test our model\n\nFor the test data we will use the last 12 months to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='1974-12-01'] \ntest = df[df.ds&gt;'1974-12-01']\n\n\ntrain.shape, test.shape\n\n((156, 3), (12, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"Monthly Milk Production\");\nplt.show()"
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#implementation-of-dynamicoptimizedtheta-with-statsforecast",
    "href": "docs/models/dynamicoptimizedtheta.html#implementation-of-dynamicoptimizedtheta-with-statsforecast",
    "title": "Dynamic Optimized Theta Model",
    "section": "Implementation of DynamicOptimizedTheta with StatsForecast ",
    "text": "Implementation of DynamicOptimizedTheta with StatsForecast \nTo also know more about the parameters of the functions of the DynamicOptimizedTheta Model, they are listed below. For more information, visit the documentation.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\ndecomposition_type : str\n    Sesonal decomposition type, 'multiplicative' (default) or 'additive'.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import DynamicOptimizedTheta\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 12 # Monthly data \nhorizon = len(test) # number of predictions\n\n# We call the model that we are going to use\nmodels = [DynamicOptimizedTheta(season_length=season_length, \n                decomposition_type=\"additive\")] # multiplicative   additive\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='MS', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[DynamicOptimizedTheta])\n\n\nLet‚Äôs see the results of our Dynamic Optimized Theta Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nprint(result.keys())\nprint(result['fit'])\n\ndict_keys(['mse', 'amse', 'fit', 'residuals', 'm', 'states', 'par', 'n', 'modeltype', 'mean_y', 'decompose', 'decomposition_type', 'seas_forecast', 'fitted'])\nresults(x=array([250.83207246,   0.75624902,   4.67964777]), fn=10.697567725248804, nit=55, simplex=array([[237.42075735,   0.75306547,   4.46023813],\n       [250.83207246,   0.75624902,   4.67964777],\n       [257.164453  ,   0.75229688,   4.42377059],\n       [256.90854919,   0.75757957,   4.43171897]]))\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\n-18.247131\n\n\n1\n-88.625732\n\n\n2\n2.864929\n\n\n...\n...\n\n\n153\n-59.747070\n\n\n154\n-91.901550\n\n\n155\n-43.503296\n\n\n\n\n156 rows √ó 1 columns\n\n\n\n\nimport scipy.stats as stats\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\n# Prediction\nY_hat = sf.forecast(horizon, fitted=True)\n\nY_hat\n\n\n\n\n\n\n\n\nds\nDynamicOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n1975-01-01\n839.259705\n\n\n1\n1975-02-01\n801.399170\n\n\n1\n1975-03-01\n895.189148\n\n\n...\n...\n...\n\n\n1\n1975-10-01\n821.271179\n\n\n1\n1975-11-01\n792.530396\n\n\n1\n1975-12-01\n829.854492\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nDynamicOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n\n1\n1962-01-01\n589.0\n607.247131\n\n\n1\n1962-02-01\n561.0\n649.625732\n\n\n1\n1962-03-01\n640.0\n637.135071\n\n\n1\n1962-04-01\n656.0\n609.225830\n\n\n1\n1962-05-01\n727.0\n604.995300\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nDynamicOptimizedTheta\nDynamicOptimizedTheta-lo-95\nDynamicOptimizedTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n1975-01-01\n839.259705\n741.963562\n955.137634\n\n\n1\n1975-02-01\n801.399170\n641.886230\n946.029053\n\n\n1\n1975-03-01\n895.189148\n707.210693\n1066.337036\n\n\n...\n...\n...\n...\n...\n\n\n1\n1975-10-01\n821.271179\n546.113647\n1088.162476\n\n\n1\n1975-11-01\n792.530396\n494.657928\n1037.432007\n\n\n1\n1975-12-01\n829.854492\n519.697144\n1108.181885\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nDynamicOptimizedTheta\n\n\n\n\n0\n1\n1975-01-01\n839.259705\n\n\n1\n1\n1975-02-01\n801.399170\n\n\n2\n1\n1975-03-01\n895.189148\n\n\n...\n...\n...\n...\n\n\n9\n1\n1975-10-01\n821.271179\n\n\n10\n1\n1975-11-01\n792.530396\n\n\n11\n1\n1975-12-01\n829.854492\n\n\n\n\n12 rows √ó 3 columns\n\n\n\n\n# Merge the forecasts with the true values\ntest['unique_id'] = test['unique_id'].astype(int)\nY_hat1 = test.merge(Y_hat, how='left', on=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nDynamicOptimizedTheta\n\n\n\n\n0\n1975-01-01\n834\n1\n839.259705\n\n\n1\n1975-02-01\n782\n1\n801.399170\n\n\n2\n1975-03-01\n892\n1\n895.189148\n\n\n...\n...\n...\n...\n...\n\n\n9\n1975-10-01\n827\n1\n821.271179\n\n\n10\n1975-11-01\n797\n1\n792.530396\n\n\n11\n1975-12-01\n843\n1\n829.854492\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds')\nplot_df[['y', \"DynamicOptimizedTheta\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Monthly Milk Production', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nDynamicOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n1975-01-01\n839.259705\n\n\n1\n1975-02-01\n801.399170\n\n\n1\n1975-03-01\n895.189148\n\n\n...\n...\n...\n\n\n1\n1975-10-01\n821.271179\n\n\n1\n1975-11-01\n792.530396\n\n\n1\n1975-12-01\n829.854492\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \nforecast_df\n\n\n\n\n\n\n\n\nds\nDynamicOptimizedTheta\nDynamicOptimizedTheta-lo-80\nDynamicOptimizedTheta-hi-80\nDynamicOptimizedTheta-lo-95\nDynamicOptimizedTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n1\n1975-01-01\n839.259705\n766.150513\n928.015320\n741.963562\n955.137634\n\n\n1\n1975-02-01\n801.399170\n702.992554\n899.872864\n641.886230\n946.029053\n\n\n1\n1975-03-01\n895.189148\n760.141418\n1008.321960\n707.210693\n1066.337036\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n1975-10-01\n821.271179\n617.415344\n996.678284\n546.113647\n1088.162476\n\n\n1\n1975-11-01\n792.530396\n568.329102\n975.049255\n494.657928\n1037.432007\n\n\n1\n1975-12-01\n829.854492\n598.124878\n1035.452515\n519.697144\n1108.181885\n\n\n\n\n12 rows √ó 6 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nDynamicOptimizedTheta\nDynamicOptimizedTheta-lo-80\nDynamicOptimizedTheta-hi-80\nDynamicOptimizedTheta-lo-95\nDynamicOptimizedTheta-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n1962-01-01\n589.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1962-02-01\n561.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1962-03-01\n640.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1975-10-01\nNaN\nNaN\n821.271179\n617.415344\n996.678284\n546.113647\n1088.162476\n\n\n1975-11-01\nNaN\nNaN\n792.530396\n568.329102\n975.049255\n494.657928\n1037.432007\n\n\n1975-12-01\nNaN\nNaN\n829.854492\n598.124878\n1035.452515\n519.697144\n1108.181885\n\n\n\n\n180 rows √ó 7 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=3 , )\n    colors = ['green', \"lime\"]\n    ax.fill_between(df_plot.index, \n                df_plot['DynamicOptimizedTheta-lo-80'], \n                df_plot['DynamicOptimizedTheta-hi-80'],\n                alpha=.20,\n                color='orange',\n                label='DynamicOptimizedTheta_level_80')\n    ax.fill_between(df_plot.index, \n                df_plot['DynamicOptimizedTheta-lo-95'], \n                df_plot['DynamicOptimizedTheta-hi-95'],\n                alpha=.3,\n                color='lime',\n                label='DynamicOptimizedTheta_level_95')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel(\"Montly Mil Production\", fontsize=20)\n    ax.set_xlabel('Month-Days', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid(True)\n    plt.show()\n\n\nplot_forecasts(train, test, forecast_df, models=['DynamicOptimizedTheta'])"
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#cross-validation",
    "href": "docs/models/dynamicoptimizedtheta.html#cross-validation",
    "title": "Dynamic Optimized Theta Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=12,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nDynamicOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n1972-01-01\n1971-12-01\n826.0\n828.692017\n\n\n1\n1972-02-01\n1971-12-01\n799.0\n792.444092\n\n\n1\n1972-03-01\n1971-12-01\n890.0\n883.122620\n\n\n...\n...\n...\n...\n...\n\n\n1\n1974-10-01\n1973-12-01\n812.0\n810.342834\n\n\n1\n1974-11-01\n1973-12-01\n773.0\n781.845703\n\n\n1\n1974-12-01\n1973-12-01\n813.0\n818.855103\n\n\n\n\n36 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#model-evaluation",
    "href": "docs/models/dynamicoptimizedtheta.html#model-evaluation",
    "title": "Dynamic Optimized Theta Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Dynamic Optimized Theta Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"DynamicOptimizedTheta\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  14.320624\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, model):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=12)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, test, Y_hat, model=\"DynamicOptimizedTheta\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nDynamicOptimizedTheta\n6.861959\n0.804529\n0.308595\n8.64746\n0.801941"
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#acknowledgements",
    "href": "docs/models/dynamicoptimizedtheta.html#acknowledgements",
    "title": "Dynamic Optimized Theta Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/dynamicoptimizedtheta.html#references",
    "href": "docs/models/dynamicoptimizedtheta.html#references",
    "title": "Dynamic Optimized Theta Model",
    "section": "References ",
    "text": "References \n\nKostas I. Nikolopoulos, Dimitrios D. Thomakos. Forecasting with the Theta Method-Theory and Applications. 2019 John Wiley & Sons Ltd.\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html",
    "href": "docs/models/simpleexponentialoptimized.html",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "",
    "text": "Introduction\nSimple Exponential Smoothing Optimized Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of SimpleExponentialSmoothingOptimized with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#table-of-contents",
    "href": "docs/models/simpleexponentialoptimized.html#table-of-contents",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "",
    "text": "Introduction\nSimple Exponential Smoothing Optimized Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of SimpleExponentialSmoothingOptimized with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#introduction",
    "href": "docs/models/simpleexponentialoptimized.html#introduction",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Introduction ",
    "text": "Introduction \nSimple Exponential Smoothing Optimized (SES Optimized) is a forecasting model used to predict future values in univariate time series. It is a variant of the simple exponential smoothing (SES) method that uses an optimization approach to estimate the model parameters more accurately.\nThe SES Optimized method uses a single smoothing parameter to estimate the trend and seasonality in the time series data. The model attempts to minimize the mean squared error (MSE) between the predictions and the actual values in the training sample using an optimization algorithm.\nThe SES Optimized approach is especially useful for time series with strong trend and seasonality patterns, or for time series with noisy data. However, it is important to note that this model assumes that the time series is stationary and that the variation in the data is random and there are no non-random patterns in the data. If these assumptions are not met, the SES Optimized model may not perform well and another forecasting method may be required."
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#simple-exponential-smoothing-model",
    "href": "docs/models/simpleexponentialoptimized.html#simple-exponential-smoothing-model",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Simple Exponential Smoothing Model ",
    "text": "Simple Exponential Smoothing Model \nThe simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern.\nUsing the na√Øve method, all forecasts for the future are equal to the last observed value of the series, \\[\\hat{y}_{T+h|T} = y_{T},\\]\nfor $h=1,2,$. Hence, the na√Øve method assumes that the most recent observation is the only important one, and all previous observations provide no information for the future. This can be thought of as a weighted average where all of the weight is given to the last observation.\nUsing the average method, all future forecasts are equal to a simple average of the observed data, \\[\\hat{y}_{T+h|T} = \\frac1T \\sum_{t=1}^T y_t, \\]\nfor $h=1,2,$ Hence, the average method assumes that all observations are of equal importance, and gives them equal weights when generating forecasts.\nWe often want something between these two extremes. For example, it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past ‚Äî the smallest weights are associated with the oldest observations:\n\\[\\begin{equation}\n  \\hat{y}_{T+1|T} = \\alpha y_T + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^2 y_{T-2}+ \\cdots,   \\tag{1}\n\\end{equation}\\]\nwhere \\(0 \\le \\alpha \\le 1\\) is the smoothing parameter. The one-step-ahead forecast for time \\(T+1\\) is a weighted average of all of the observations in the series \\(y_1,\\dots,y_T\\). The rate at which the weights decrease is controlled by the parameter \\(\\alpha\\).\nFor any \\(\\alpha\\) between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name ‚Äúexponential smoothing‚Äù. If \\(\\alpha\\) is small (i.e., close to 0), more weight is given to observations from the more distant past. If \\(\\alpha\\) is large (i.e., close to 1), more weight is given to the more recent observations. For the extreme case where \\(\\alpha=1\\), \\(\\hat{y}_{T+1|T}=y_T\\) and the forecasts are equal to the na√Øve forecasts."
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#optimisation",
    "href": "docs/models/simpleexponentialoptimized.html#optimisation",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Optimisation",
    "text": "Optimisation\nThe application of every exponential smoothing method requires the smoothing parameters and the initial values to be chosen. In particular, for simple exponential smoothing, we need to select the values of \\(\\alpha\\) and \\(\\ell_0\\) . All forecasts can be computed from the data once we know those values. For the methods that follow there is usually more than one smoothing parameter and more than one initial component to be chosen.\nIn some cases, the smoothing parameters may be chosen in a subjective manner ‚Äî the forecaster specifies the value of the smoothing parameters based on previous experience. However, a more reliable and objective way to obtain values for the unknown parameters is to estimate them from the observed data.\nFrom regression models we estimated the coefficients of a regression model by minimising the sum of the squared residuals (usually known as SSE or ‚Äúsum of squared errors‚Äù). Similarly, the unknown parameters and the initial values for any exponential smoothing method can be estimated by minimising the SSE. The residuals are specified as \\(e_t=y_t - \\hat{y}_{t|t-1}\\) for \\(t=1,\\dots,T\\). Hence, we find the values of the unknown parameters and the initial values that minimise\n\\[\\begin{equation}\n\\text{SSE}=\\sum_{t=1}^T(y_t - \\hat{y}_{t|t-1})^2=\\sum_{t=1}^Te_t^2. \\tag{2}\n\\end{equation}\\]\nUnlike the regression case (where we have formulas which return the values of the regression coefficients that minimise the SSE), this involves a non-linear minimisation problem, and we need to use an optimisation tool to solve it."
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#loading-libraries-and-data",
    "href": "docs/models/simpleexponentialoptimized.html#loading-libraries-and-data",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\nimport pandas as pd\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/ads.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nTime\nAds\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n\n\n1\n2017-09-13T01:00:00\n79885\n\n\n2\n2017-09-13T02:00:00\n89325\n\n\n3\n2017-09-13T03:00:00\n101930\n\n\n4\n2017-09-13T04:00:00\n121630\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n1\n\n\n1\n2017-09-13T01:00:00\n79885\n1\n\n\n2\n2017-09-13T02:00:00\n89325\n1\n\n\n3\n2017-09-13T03:00:00\n101930\n1\n\n\n4\n2017-09-13T04:00:00\n121630\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#explore-data-with-the-plot-method",
    "href": "docs/models/simpleexponentialoptimized.html#explore-data-with-the-plot-method",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();"
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#split-the-data-into-training-and-testing",
    "href": "docs/models/simpleexponentialoptimized.html#split-the-data-into-training-and-testing",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our Simple Exponential Smoothing Optimized Model\nData to test our model\n\nFor the test data we will use the last 30 Hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2017-09-20 17:00:00'] \ntest = df[df.ds&gt;'2017-09-20 17:00:00']\n\n\ntrain.shape, test.shape\n\n((186, 3), (30, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"Ads watched (hourly data)\");\nplt.show()"
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#implementation-of-simpleexponentialsmoothingoptimized-with-statsforecast",
    "href": "docs/models/simpleexponentialoptimized.html#implementation-of-simpleexponentialsmoothingoptimized-with-statsforecast",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Implementation of SimpleExponentialSmoothingOptimized with StatsForecast ",
    "text": "Implementation of SimpleExponentialSmoothingOptimized with StatsForecast \nTo also know more about the parameters of the functions of the SimpleExponentialSmoothingOptimized Model, they are listed below. For more information, visit the documentation.\nalpha : float\n    Smoothing parameter.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import SimpleExponentialSmoothingOptimized\n\n\n\nInstantiating Model\n\nhorizon = len(test) # number of predictions\n\nmodels = [SimpleExponentialSmoothingOptimized()] # multiplicative   additive\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[SESOpt])\n\n\nLet‚Äôs see the results of our Simple Exponential Smoothing Optimized model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([80434.516], dtype=float32),\n 'fitted': array([       nan,  80115.   ,  79887.3  ,  89230.625, 101803.01 ,\n        121431.73 , 116524.57 , 106595.3  , 102833.   , 108002.78 ,\n        116043.78 , 130880.14 , 148838.6  , 157502.48 , 150782.88 ,\n        149309.88 , 150092.1  , 144833.12 , 150631.44 , 163707.92 ,\n        166209.73 , 139786.89 , 106233.92 ,  96874.54 ,  82663.55 ,\n         80150.38 ,  75383.16 ,  85007.78 , 101909.28 , 124902.74 ,\n        118098.73 , 109313.734, 102543.39 , 102243.03 , 115704.03 ,\n        130391.64 , 144185.67 , 148922.16 , 149147.72 , 148051.08 ,\n        148802.4  , 149819.72 , 150562.5  , 149451.22 , 150509.31 ,\n        129343.8  , 104070.29 ,  92293.95 ,  82860.29 ,  76380.45 ,\n         75142.51 ,  82565.02 ,  88732.7  , 118133.02 , 115219.43 ,\n        110982.8  ,  98981.23 , 104132.96 , 108619.68 , 126459.8  ,\n        140295.25 , 152348.25 , 146335.73 , 148003.16 , 147737.69 ,\n        145769.88 , 149249.84 , 159620.25 , 161070.36 , 135775.5  ,\n        113173.305, 100329.734,  87742.15 ,  87834.07 ,  88834.89 ,\n         92314.85 , 104343.5  , 115824.03 , 128818.74 , 141259.34 ,\n        144408.19 , 143261.58 , 133290.72 , 131260.5  , 142367.81 ,\n        157224.92 , 152547.25 , 153723.12 , 151220.28 , 150650.75 ,\n        147467.16 , 152474.42 , 146931.   , 125461.86 , 118000.37 ,\n         96913.   ,  93643.03 ,  89105.83 ,  89342.61 ,  90562.68 ,\n         98212.73 , 112426.43 , 129299.56 , 141283.95 , 152447.23 ,\n        152578.67 , 141284.1  , 147487.34 , 160973.77 , 166281.39 ,\n        166775.02 , 163176.34 , 157363.72 , 159038.1  , 160010.19 ,\n        168261.66 , 169883.61 , 142981.73 , 113255.266,  97504.1  ,\n         81833.29 ,  79533.234,  78361.836,  87948.17 ,  99671.58 ,\n        123538.914, 111447.14 ,  99560.07 ,  97674.05 ,  97655.19 ,\n        102515.9  , 119755.86 , 135595.02 , 140074.75 , 141713.45 ,\n        142214.94 , 145328.55 , 145334.94 , 150359.25 , 161408.39 ,\n        153494.94 , 134907.75 , 107343.43 ,  95167.984,  79671.53 ,\n         78348.37 ,  74706.78 ,  81917.164,  97789.67 , 119129.445,\n        113175.14 ,  99022.95 ,  94050.23 ,  93663.9  , 104079.79 ,\n        119593.3  , 135826.03 , 146348.7  , 139236.84 , 147145.12 ,\n        144957.1  , 151305.88 , 156032.27 , 161331.47 , 164973.22 ,\n        134398.83 , 105873.14 ,  92985.18 ,  79407.15 ,  79974.27 ,\n         78128.64 ,  85708.44 ,  99866.984, 123639.87 , 116408.05 ,\n        104411.18 , 101469.71 ,  97673.34 , 108159.086, 121119.09 ,\n        140652.69 , 138575.98 , 140965.86 , 141519.4  , 141589.3  ,\n        140619.8  , 139526.05 , 146148.11 , 142462.23 , 124130.17 ,\n        101587.7  ,  88304.18 ,  76172.54 ,  70393.375,  72132.44 ,\n         80114.375,  94796.695, 121638.87 , 114026.89 , 106570.32 ,\n         97382.805,  98845.23 , 105567.1  , 114291.87 , 132154.56 ,\n        146485.25 , 142039.9  , 142807.25 , 145987.88 , 152058.67 ,\n        151792.69 , 155626.28 , 155887.36 , 123719.92 , 103286.4  ,\n         95236.31 ], dtype=float32)}\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nfitted=pd.DataFrame(result.get(\"fitted\"), columns=[\"fitted\"])\nfitted[\"ds\"]=df[\"ds\"]\nfitted\n\n\n\n\n\n\n\n\nfitted\nds\n\n\n\n\n0\nNaN\n2017-09-13 00:00:00\n\n\n1\n80115.000000\n2017-09-13 01:00:00\n\n\n2\n79887.296875\n2017-09-13 02:00:00\n\n\n...\n...\n...\n\n\n213\n123719.921875\n2017-09-21 21:00:00\n\n\n214\n103286.398438\n2017-09-21 22:00:00\n\n\n215\n95236.312500\n2017-09-21 23:00:00\n\n\n\n\n216 rows √ó 2 columns\n\n\n\n\nsns.lineplot(df, x=\"ds\", y=\"y\", label=\"Actual\", linewidth=2)\nsns.lineplot(fitted,x=\"ds\", y=\"fitted\", label=\"Fitted\", linestyle=\"--\" )\n\nplt.title(\"Ads watched (hourly data)\");\nplt.show()\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hors ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\n# Prediction\nY_hat = sf.forecast(horizon, fitted=True)\n\nY_hat\n\n\n\n\n\n\n\n\nds\nSESOpt\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80434.515625\n\n\n1\n2017-09-22 01:00:00\n80434.515625\n\n\n1\n2017-09-22 02:00:00\n80434.515625\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n80434.515625\n\n\n1\n2017-09-23 04:00:00\n80434.515625\n\n\n1\n2017-09-23 05:00:00\n80434.515625\n\n\n\n\n30 rows √ó 2 columns\n\n\n\nLet‚Äôs visualize the fitted values\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nSESOpt\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2017-09-13 00:00:00\n80115.0\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\n80115.000000\n\n\n1\n2017-09-13 02:00:00\n89325.0\n79887.296875\n\n\n1\n2017-09-13 03:00:00\n101930.0\n89230.625000\n\n\n1\n2017-09-13 04:00:00\n121630.0\n101803.007812\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\n\nsf.forecast(h=horizon)\n\n\n\n\n\n\n\n\nds\nSESOpt\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80434.515625\n\n\n1\n2017-09-22 01:00:00\n80434.515625\n\n\n1\n2017-09-22 02:00:00\n80434.515625\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n80434.515625\n\n\n1\n2017-09-23 04:00:00\n80434.515625\n\n\n1\n2017-09-23 05:00:00\n80434.515625\n\n\n\n\n30 rows √ó 2 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat.head(10)\n\n\n\n\n\n\n\n\nunique_id\nds\nSESOpt\n\n\n\n\n0\n1\n2017-09-22 00:00:00\n80434.515625\n\n\n1\n1\n2017-09-22 01:00:00\n80434.515625\n\n\n2\n1\n2017-09-22 02:00:00\n80434.515625\n\n\n...\n...\n...\n...\n\n\n7\n1\n2017-09-22 07:00:00\n80434.515625\n\n\n8\n1\n2017-09-22 08:00:00\n80434.515625\n\n\n9\n1\n2017-09-22 09:00:00\n80434.515625\n\n\n\n\n10 rows √ó 3 columns\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \n\nforecast_df\n\n\n\n\n\n\n\n\nds\nSESOpt\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80434.515625\n\n\n1\n2017-09-22 01:00:00\n80434.515625\n\n\n1\n2017-09-22 02:00:00\n80434.515625\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n80434.515625\n\n\n1\n2017-09-23 04:00:00\n80434.515625\n\n\n1\n2017-09-23 05:00:00\n80434.515625\n\n\n\n\n30 rows √ó 2 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\ndf_plot=pd.concat([df, forecast_df]).set_index('ds')\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nSESOpt\n\n\nds\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n80434.515625\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n80434.515625\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n80434.515625\n\n\n\n\n246 rows √ó 3 columns\n\n\n\n\n# Plot the data and the exponentially smoothed data\nplt.plot(df_plot['y'],label=\"Actual\", linewidth=2.5)\nplt.plot(df_plot['SESOpt'], label=\"SESOpt\") # '-', '--', '-.', ':',\nplt.title(\"Ads watched (hourly data)\");\nplt.ylabel(\"\")\nplt.legend()\nplt.show()\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#cross-validation",
    "href": "docs/models/simpleexponentialoptimized.html#cross-validation",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 30 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=30,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nSESOpt\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-18 06:00:00\n2017-09-18 05:00:00\n99440.0\n111447.140625\n\n\n1\n2017-09-18 07:00:00\n2017-09-18 05:00:00\n97655.0\n111447.140625\n\n\n1\n2017-09-18 08:00:00\n2017-09-18 05:00:00\n97655.0\n111447.140625\n\n\n...\n...\n...\n...\n...\n\n\n1\n2017-09-21 21:00:00\n2017-09-20 17:00:00\n103080.0\n139526.046875\n\n\n1\n2017-09-21 22:00:00\n2017-09-20 17:00:00\n95155.0\n139526.046875\n\n\n1\n2017-09-21 23:00:00\n2017-09-20 17:00:00\n80285.0\n139526.046875\n\n\n\n\n90 rows √ó 4 columns\n\n\n\n\ncross_validation=crossvalidation_df.copy()\ncross_validation.rename(columns = {'y' : 'actual'}, inplace = True) # rename actual values \n\ncutoff = cross_validation['cutoff'].unique()\n\nfor k in range(len(cutoff)): \n    cv = cross_validation[cross_validation['cutoff'] == cutoff[k]]\n    StatsForecast.plot(df, cv.loc[:, cv.columns != 'cutoff'])"
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#evaluate-model",
    "href": "docs/models/simpleexponentialoptimized.html#evaluate-model",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Evaluate Model ",
    "text": "Evaluate Model \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Simple Exponential Smoothing Optimized Model.\n\n\nrmse = rmse(cross_validation['actual'], cross_validation[\"SESOpt\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  30098.377"
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#acknowledgements",
    "href": "docs/models/simpleexponentialoptimized.html#acknowledgements",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/simpleexponentialoptimized.html#references",
    "href": "docs/models/simpleexponentialoptimized.html#references",
    "title": "Simple Exponential Smoothing Optimized Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/standardtheta.html",
    "href": "docs/models/standardtheta.html",
    "title": "Standard Theta Model",
    "section": "",
    "text": "Introduction\nStandard Theta\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of StandardTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/standardtheta.html#table-of-contents",
    "href": "docs/models/standardtheta.html#table-of-contents",
    "title": "Standard Theta Model",
    "section": "",
    "text": "Introduction\nStandard Theta\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of StandardTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/standardtheta.html#introduction",
    "href": "docs/models/standardtheta.html#introduction",
    "title": "Standard Theta Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Theta method (Assimakopoulos & Nikolopoulos, 2000, hereafter A&N) is applied to non-seasonal or deseasonalised time series, where the deseasonalisation is usually performed via the multiplicative classical decomposition. The method decomposes the original time series into two new lines through the so-called theta coefficients, denoted by \\({\\theta}_1\\) and \\({\\theta}_2\\) for \\({\\theta}_1, {\\theta}_2 \\in \\mathbb{R}\\), which are applied to the second difference of the data. The second differences are reduced when \\({\\theta}&lt;1\\), resulting in a better approximation of the long-term behaviour of the series (Assimakopoulos, 1995). If \\({\\theta}\\) is equal to zero, the new line is a straight line. When \\({\\theta}&gt;1\\) the local curvatures are increased, magnifying the short-term movements of the time series (A&N). The new lines produced are called theta lines, denoted here by \\(\\text{Z}(\\theta_1)\\) and \\(\\text{Z}(\\theta_2)\\). These lines have the same mean value and slope as the original data, but the local curvatures are either filtered out or enhanced, depending on the value of the \\(\\theta\\) coefficient.\nIn other words, the decomposition process has the advantage of exploiting information in the data that usually cannot be captured and modelled completely through the extrapolation of the original time series. The theta lines can be regarded as new time series and are extrapolated separately using an appropriate forecasting method. Once the extrapolation of each theta line has been completed, recomposition takes place through a combination scheme in order to calculate the point forecasts of the original time series. Combining has long been considered as a useful practice in the forecasting literature (for example, Clemen, 1989, Makridakis and Winkler, 1983, Petropoulos et al., 2014), and therefore its application to the Theta method is expected to result in more accurate and robust forecasts.\nThe Theta method is quite versatile in terms of choosing the number of theta lines, the theta coefficients and the extrapolation methods, and combining these to obtain robust forecasts. However, A&N proposed a simplified version involving the use of only two theta lines with prefixed \\(\\theta\\) coefficients that are extrapolated over time using a linear regression (LR) model for the theta line with \\({\\theta}_1 =0\\) and simple exponential smoothing (SES) for the theta line with \\({\\theta}_2 =2\\). The final forecasts are produced by combining the forecasts of the two theta lines with equal weights.\nThe performance of the Theta method has also been confirmed by other empirical studies (for example Nikolopoulos et al., 2012, Petropoulos and Nikolopoulos, 2013). Moreover, Hyndman and Billah (2003), hereafter H&B, showed that the simple exponential smoothing with drift model (SES-d) is a statistical model for the simplified version of the Theta method. More recently, Thomakos and Nikolopoulos (2014) provided additional theoretical insights, while Thomakos and Nikolopoulos (2015) derived new theoretical formulations for the application of the method to multivariate time series, and investigated the conditions under which the bivariate Theta method is expected to forecast better than the univariate one. Despite these advances, we believe that the Theta method deserves more attention from the forecasting community, given its simplicity and superior forecasting performance.\nOne key aspect of the Theta method is that, by definition, it is dynamic. One can choose different theta lines and combine the produced forecasts using either equal or unequal weights. However, AN limit this important property by fixing the theta coefficients to have predefined values."
  },
  {
    "objectID": "docs/models/standardtheta.html#standard-theta-model",
    "href": "docs/models/standardtheta.html#standard-theta-model",
    "title": "Standard Theta Model",
    "section": "Standard Theta Model",
    "text": "Standard Theta Model\nAssimakopoulos and Nikolopoulo for standard theta model proposed the Theta line as the solution of the equation\n\\[D^2 \\zeta_t(\\theta) = \\theta D^2 Y_t, t = 1,\\cdots,T \\tag 1\\]\nwhere \\(Y_1, \\cdots , Y_T\\) represent the original time series data and \\(DX_t = (X_t ‚àí X_{t‚àí1})\\). The initial values \\(\\zeta_1\\) and \\(\\zeta_2\\) are obtained by minimizing $_{i=1}^{T} [Y_t - _t () ]^2 $. However, the analytical solution of (1) is given by\n\\[\\zeta_t(\\theta)=\\theta Y_t +(1‚àí\\theta)(A_T +B_T t),\\ t=1, \\cdots, T,  \\tag 2\\]\nwhere \\(A_T\\) and \\(B_T\\) are the minimum square coefficients of a simple linear regression over \\(Y_1, \\cdots,Y_T\\) against \\(1, \\cdots , T\\) which are only dependent on the original data and given as follow\n\\[A_T=\\frac{1}{T} \\sum_{i=1}^{T} Y_t - \\frac{T+1}{2} B_T \\tag 3 \\]\n\\[B_T=\\frac{6}{T^2 - 1} (\\frac{2}{T} \\sum_{t=1}^{T} tY_t - \\frac{T+1}{T} \\sum_{t=1}^{T} Y_t  \\tag 4)   \\]\nTheta lines can be understood as functions of the linear regression model directly applied to the data from this perspective. Indeed, the Theta method‚Äôs projections for h steps ahead are an ad hoc combination (50 percent - 50 percent) of the linear extrapolations of \\(\\zeta(0)\\) and \\(\\zeta(2)\\).\n\nWhen \\(\\theta &lt; 1\\) is applied to the second differences of the data, the decomposition process is defined by a theta coefficient, which reduces the second differences and improves the approximation of series behavior.\nIf \\(\\theta = 0\\), the deconstructed line is turned into a constant straight line. (see Fig)\nIf \\(\\theta &gt; 1\\) then the short term movements of the analyzed series show more local curvatures (see fig)\n\n\n\n\nFigure\n\n\nWe will refer to the above setup as the standard Theta method. The steps for building the theta method are as follows:\n\nDeseasonalisation: Firstly, the time series data is tested for statistically significant seasonal behaviour. A time series is seasonal if\n\n\\[|\\rho_m| &gt; q_{1- \\frac{\\alpha}{2} } \\sqrt{\\frac{1+2 \\sum_{i=1}^{m-1} \\rho_{i}^{2} }{T} } \\]\nwhere œÅk denotes the lag \\(k\\) autocorrelation function, \\(m\\) is the number of the periods within a seasonal cycle (for example, 12 for monthly data), \\(T\\) is the sample size, \\(q\\) is the quantile function of the standard normal distribution, and \\((1 ‚àí a)\\%\\) is the confidence level. Assimakopoulos and Nikolopoulo [Standar Theta model] opted for a 90% confidence level. If the time series is identified as seasonal, then it is deseasonalised via the classical decomposition method, assuming the seasonal component to have a multiplicative relationship.\n\nDecomposition: The second step consits for the decomposition of the seasonally adjusted time series into two Theta lines, the linear regression line \\(\\zeta(0)\\) and the theta line \\(\\zeta(2)\\).\nExtrapolation: \\(\\zeta(2)\\) is extrapolated using simple exponential smoothing (SES), while \\(\\zeta(0)\\) is extrapolated as a normal linear regression line.\nCombination: the final forecast is a combination of the forecasts of the two \\(\\theta\\) lines using equal weights.\nReseasonalisation: In the presence of seasonality in first step, then the final forecasts are multiplied by the respective seasonal indices."
  },
  {
    "objectID": "docs/models/standardtheta.html#loading-libraries-and-data",
    "href": "docs/models/standardtheta.html#loading-libraries-and-data",
    "title": "Standard Theta Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/milk_production.csv\", usecols=[1,2])\ndf.head()\n\n\n\n\n\n\n\n\nmonth\nproduction\n\n\n\n\n0\n1962-01-01\n589\n\n\n1\n1962-02-01\n561\n\n\n2\n1962-03-01\n640\n\n\n3\n1962-04-01\n656\n\n\n4\n1962-05-01\n727\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1962-01-01\n589\n1\n\n\n1\n1962-02-01\n561\n1\n\n\n2\n1962-03-01\n640\n1\n\n\n3\n1962-04-01\n656\n1\n\n\n4\n1962-05-01\n727\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/standardtheta.html#explore-data-with-the-plot-method",
    "href": "docs/models/standardtheta.html#explore-data-with-the-plot-method",
    "title": "Standard Theta Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df, engine=\"matplotlib\")\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\n\nAdditive\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=12)\na.plot();\n\n\n\n\n\n\nMultiplicative\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=12)\na.plot();"
  },
  {
    "objectID": "docs/models/standardtheta.html#split-the-data-into-training-and-testing",
    "href": "docs/models/standardtheta.html#split-the-data-into-training-and-testing",
    "title": "Standard Theta Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our Theta model 2. Data to test our model\nFor the test data we will use the last 12 months to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='1974-12-01'] \ntest = df[df.ds&gt;'1974-12-01']\n\n\ntrain.shape, test.shape\n\n((156, 3), (12, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"Monthly Milk Production\");\nplt.show()"
  },
  {
    "objectID": "docs/models/standardtheta.html#implementation-of-standardtheta-with-statsforecast",
    "href": "docs/models/standardtheta.html#implementation-of-standardtheta-with-statsforecast",
    "title": "Standard Theta Model",
    "section": "Implementation of StandardTheta with StatsForecast ",
    "text": "Implementation of StandardTheta with StatsForecast \nTo also know more about the parameters of the functions of the StandardTheta Model, they are listed below. For more information, visit the documentation.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\ndecomposition_type : str\n    Sesonal decomposition type, 'multiplicative' (default) or 'additive'.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Theta\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 12 # Monthly data \nhorizon = len(test) # number of predictions\n\nmodels = [Theta(season_length=season_length, \n                decomposition_type=\"additive\")] # multiplicative   additive\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='MS', \n                   n_jobs=-1)\n\n\n\nFit Model\n\nsf.fit()\n\nStatsForecast(models=[Theta])\n\n\nLet‚Äôs see the results of our Theta model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nprint(result.keys())\nprint(result['fit'])\n\ndict_keys(['mse', 'amse', 'fit', 'residuals', 'm', 'states', 'par', 'n', 'modeltype', 'mean_y', 'decompose', 'decomposition_type', 'seas_forecast', 'fitted'])\nresults(x=array([225.82002697,   0.76015625]), fn=10.638733596938769, nit=19, simplex=array([[241.83142594,   0.76274414],\n       [225.82002697,   0.76015625],\n       [212.41789302,   0.76391602]]))\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\n-17.596375\n\n\n1\n-46.997192\n\n\n2\n23.093933\n\n\n...\n...\n\n\n153\n-59.003235\n\n\n154\n-91.150085\n\n\n155\n-42.749451\n\n\n\n\n156 rows √ó 1 columns\n\n\n\n\nimport scipy.stats as stats\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\n# Prediction\nY_hat = sf.forecast(horizon, fitted=True)\nY_hat\n\n\n\n\n\n\n\n\nds\nTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n1975-01-01\n838.559814\n\n\n1\n1975-02-01\n800.188232\n\n\n1\n1975-03-01\n893.472900\n\n\n...\n...\n...\n\n\n1\n1975-10-01\n816.166931\n\n\n1\n1975-11-01\n786.962036\n\n\n1\n1975-12-01\n823.826538\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nTheta\n\n\nunique_id\n\n\n\n\n\n\n\n1\n1962-01-01\n589.0\n606.596375\n\n\n1\n1962-02-01\n561.0\n607.997192\n\n\n1\n1962-03-01\n640.0\n616.906067\n\n\n1\n1962-04-01\n656.0\n608.873047\n\n\n1\n1962-05-01\n727.0\n607.395142\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nTheta\nTheta-lo-95\nTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n1975-01-01\n838.559814\n741.324280\n954.365540\n\n\n1\n1975-02-01\n800.188232\n640.785645\n944.996887\n\n\n1\n1975-03-01\n893.472900\n705.123901\n1064.757324\n\n\n...\n...\n...\n...\n...\n\n\n1\n1975-10-01\n816.166931\n539.706848\n1083.791870\n\n\n1\n1975-11-01\n786.962036\n487.946075\n1032.028931\n\n\n1\n1975-12-01\n823.826538\n512.674866\n1101.965942\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nTheta\n\n\n\n\n0\n1\n1975-01-01\n838.559814\n\n\n1\n1\n1975-02-01\n800.188232\n\n\n2\n1\n1975-03-01\n893.472900\n\n\n...\n...\n...\n...\n\n\n9\n1\n1975-10-01\n816.166931\n\n\n10\n1\n1975-11-01\n786.962036\n\n\n11\n1\n1975-12-01\n823.826538\n\n\n\n\n12 rows √ó 3 columns\n\n\n\n\n# Merge the forecasts with the true values\ntest['unique_id'] = test['unique_id'].astype(int)\nY_hat1 = test.merge(Y_hat, how='left', on=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nTheta\n\n\n\n\n0\n1975-01-01\n834\n1\n838.559814\n\n\n1\n1975-02-01\n782\n1\n800.188232\n\n\n2\n1975-03-01\n892\n1\n893.472900\n\n\n...\n...\n...\n...\n...\n\n\n9\n1975-10-01\n827\n1\n816.166931\n\n\n10\n1975-11-01\n797\n1\n786.962036\n\n\n11\n1975-12-01\n843\n1\n823.826538\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds')\nplot_df[['y', \"Theta\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Monthly Milk Production ', fontsize=20)\nax.set_xlabel('Monthly [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n1975-01-01\n838.559814\n\n\n1\n1975-02-01\n800.188232\n\n\n1\n1975-03-01\n893.472900\n\n\n...\n...\n...\n\n\n1\n1975-10-01\n816.166931\n\n\n1\n1975-11-01\n786.962036\n\n\n1\n1975-12-01\n823.826538\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \n\nforecast_df\n\n\n\n\n\n\n\n\nds\nTheta\nTheta-lo-80\nTheta-hi-80\nTheta-lo-95\nTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n1\n1975-01-01\n838.559814\n765.496155\n927.260071\n741.324280\n954.365540\n\n\n1\n1975-02-01\n800.188232\n701.729797\n898.807434\n640.785645\n944.996887\n\n\n1\n1975-03-01\n893.472900\n758.481018\n1006.847656\n705.123901\n1064.757324\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n1975-10-01\n816.166931\n611.404541\n991.667419\n539.706848\n1083.791870\n\n\n1\n1975-11-01\n786.962036\n561.990845\n969.637634\n487.946075\n1032.028931\n\n\n1\n1975-12-01\n823.826538\n591.283691\n1029.491577\n512.674866\n1101.965942\n\n\n\n\n12 rows √ó 6 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nTheta\nTheta-lo-80\nTheta-hi-80\nTheta-lo-95\nTheta-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n1962-01-01\n589.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1962-02-01\n561.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1962-03-01\n640.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1975-10-01\nNaN\nNaN\n816.166931\n611.404541\n991.667419\n539.706848\n1083.791870\n\n\n1975-11-01\nNaN\nNaN\n786.962036\n561.990845\n969.637634\n487.946075\n1032.028931\n\n\n1975-12-01\nNaN\nNaN\n823.826538\n591.283691\n1029.491577\n512.674866\n1101.965942\n\n\n\n\n180 rows √ó 7 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2 )\n    colors = ['green']\n    ax.fill_between(df_plot.index, \n                df_plot['Theta-lo-80'], \n                df_plot['Theta-hi-80'],\n                alpha=.20,\n                color='lime',\n                label='Theta_level_80')\n    ax.fill_between(df_plot.index, \n                df_plot['Theta-lo-95'], \n                df_plot['Theta-hi-95'],\n                alpha=.2,\n                color='white',\n                label='Theta_level_95')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel(\"Return\", fontsize=20)\n    ax.set_xlabel('Month-Days', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid(True)\n    plt.show()\n\n\nplot_forecasts(train, test, forecast_df, models=['Theta'])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df, level=[95])"
  },
  {
    "objectID": "docs/models/standardtheta.html#cross-validation",
    "href": "docs/models/standardtheta.html#cross-validation",
    "title": "Standard Theta Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=12,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value."
  },
  {
    "objectID": "docs/models/standardtheta.html#evaluate-model",
    "href": "docs/models/standardtheta.html#evaluate-model",
    "title": "Standard Theta Model",
    "section": "Evaluate Model ",
    "text": "Evaluate Model \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Theta.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"Theta\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  12.643162\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import mae, mape, mase, rmse, smape\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, model):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=12)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, test, Y_hat, model=\"Theta\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nTheta\n8.111287\n0.964855\n0.36478\n9.730347\n0.965874"
  },
  {
    "objectID": "docs/models/standardtheta.html#acknowledgements",
    "href": "docs/models/standardtheta.html#acknowledgements",
    "title": "Standard Theta Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/standardtheta.html#references",
    "href": "docs/models/standardtheta.html#references",
    "title": "Standard Theta Model",
    "section": "References ",
    "text": "References \n\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html",
    "href": "docs/how-to-guides/automatic_forecasting.html",
    "title": "Automatic Time Series Forecasting",
    "section": "",
    "text": "Tip\n\n\n\nAutomatic forecasts of large numbers of univariate time series are often needed. It is common to have multiple product lines or skus that need forecasting. In these circumstances, an automatic forecasting algorithm is an essential tool. Automatic forecasting algorithms must determine an appropriate time series model, estimate the parameters and compute the forecasts. They must be robust to unusual time series patterns, and applicable to large numbers of series without user intervention.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#install-statsforecast-and-load-data",
    "href": "docs/how-to-guides/automatic_forecasting.html#install-statsforecast-and-load-data",
    "title": "Automatic Time Series Forecasting",
    "section": "1. Install statsforecast and load data",
    "text": "1. Install statsforecast and load data\nUse pip to install statsforecast and load Air Passangers dataset as an example\n\n!pip install statsforecast\n\nfrom statsforecast.utils import AirPassengersDF\n\nY_df = AirPassengersDF"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#import-statsforecast-and-models",
    "href": "docs/how-to-guides/automatic_forecasting.html#import-statsforecast-and-models",
    "title": "Automatic Time Series Forecasting",
    "section": "2. Import StatsForecast and models",
    "text": "2. Import StatsForecast and models\nImport the core StatsForecast class and the models you want to use\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, AutoETS, AutoTheta, AutoCES"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#instatiate-the-class",
    "href": "docs/how-to-guides/automatic_forecasting.html#instatiate-the-class",
    "title": "Automatic Time Series Forecasting",
    "section": "3. Instatiate the class",
    "text": "3. Instatiate the class\nInstantiate the StatsForecast class with the appropriate parameters\n\nseason_length = 12 # Define season length as 12 months for monthly data\nhorizon = 1 # Forecast horizon is set to 1 month\n\n# Define a list of models for forecasting\nmodels = [\n    AutoARIMA(season_length=season_length), # ARIMA model with automatic order selection and seasonal component\n    AutoETS(season_length=season_length), # ETS model with automatic error, trend, and seasonal component\n    AutoTheta(season_length=season_length), # Theta model with automatic seasonality detection\n    AutoCES(season_length=season_length), # CES model with automatic seasonality detection\n]\n\n# Instantiate StatsForecast class with models, data frequency ('M' for monthly),\n# and parallel computation on all CPU cores (n_jobs=-1)\nsf = StatsForecast(\n    models=models, # models for forecasting\n    freq='M',  # frequency of the data\n    n_jobs=-1  # number of jobs to run in parallel, -1 means using all processors\n)"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#a-forecast-with-forecast-method",
    "href": "docs/how-to-guides/automatic_forecasting.html#a-forecast-with-forecast-method",
    "title": "Automatic Time Series Forecasting",
    "section": "4. a) Forecast with forecast method",
    "text": "4. a) Forecast with forecast method\nThe .forecast method is faster for distributed computing and does not save the fittted models\n\n# Generate forecasts for the specified horizon using the sf object\nY_hat_df = sf.forecast(df=Y_df, h=horizon) # forecast data\n\n# Display the first few rows of the forecast DataFrame\nY_hat_df.head() # preview of forecasted data"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#b-forecast-with-fit-and-predict",
    "href": "docs/how-to-guides/automatic_forecasting.html#b-forecast-with-fit-and-predict",
    "title": "Automatic Time Series Forecasting",
    "section": "4. b) Forecast with fit and predict",
    "text": "4. b) Forecast with fit and predict\nThe .fit method saves the fitted models\n\nsf.fit(df=Y_df) # Fit the models to the data using the fit method of the StatsForecast object\n\nsf.fitted_ # Access fitted models from the StatsForecast object\n\nY_hat_df = sf.predict(h=horizon) # Predict or forecast 'horizon' steps ahead using the predict method\n\nY_hat_df.head() # Preview the first few rows of the forecasted data"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#references",
    "href": "docs/how-to-guides/automatic_forecasting.html#references",
    "title": "Automatic Time Series Forecasting",
    "section": "References",
    "text": "References\nHyndman, RJ and Khandakar, Y (2008) ‚ÄúAutomatic time series forecasting: The forecast package for R‚Äù, Journal of Statistical Software, 26(3)."
  },
  {
    "objectID": "docs/how-to-guides/migrating_R.html",
    "href": "docs/how-to-guides/migrating_R.html",
    "title": "Migrating from R",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/migrating_R.html#we-are-working-on-this-site.",
    "href": "docs/how-to-guides/migrating_R.html#we-are-working-on-this-site.",
    "title": "Migrating from R",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it."
  },
  {
    "objectID": "docs/how-to-guides/dask.html",
    "href": "docs/how-to-guides/dask.html",
    "title": "Dask",
    "section": "",
    "text": "StatsForecast works on top of Spark, Dask, and Ray through Fugue. StatsForecast will read the input DataFrame and use the corresponding engine. For example, if the input is a Spark DataFrame, StatsForecast will use the existing Spark session to run the forecast.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/dask.html#installation",
    "href": "docs/how-to-guides/dask.html#installation",
    "title": "Dask",
    "section": "Installation",
    "text": "Installation\nAs long as Dask is installed and configured, StatsForecast will be able to use it. If executing on a distributed Dask cluster, make use the statsforecast library is installed across all the workers."
  },
  {
    "objectID": "docs/how-to-guides/dask.html#statsforecast-on-pandas",
    "href": "docs/how-to-guides/dask.html#statsforecast-on-pandas",
    "title": "Dask",
    "section": "StatsForecast on Pandas",
    "text": "StatsForecast on Pandas\nBefore running on Dask, it‚Äôs recommended to test on a smaller Pandas dataset to make sure everything is working. This example also helps show the small differences when using Dask.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    AutoARIMA,\n    AutoETS,\n)\nfrom statsforecast.utils import generate_series\n\nn_series = 4\nhorizon = 7\n\nseries = generate_series(n_series)\n\nsf = StatsForecast(\n    models=[AutoETS(season_length=7)],\n    freq='D',\n)\nsf.forecast(df=series, h=horizon).head()\n\n\n\n\n\n\n\n\nds\nAutoETS\n\n\nunique_id\n\n\n\n\n\n\n0\n2000-08-10\n5.261609\n\n\n0\n2000-08-11\n6.196357\n\n\n0\n2000-08-12\n0.282309\n\n\n0\n2000-08-13\n1.264195\n\n\n0\n2000-08-14\n2.262453"
  },
  {
    "objectID": "docs/how-to-guides/dask.html#executing-on-dask",
    "href": "docs/how-to-guides/dask.html#executing-on-dask",
    "title": "Dask",
    "section": "Executing on Dask",
    "text": "Executing on Dask\nTo run the forecasts distributed on Dask, just pass in a Dask DataFrame instead. Instead of having the unique_id as an index, it needs to be a column because Dask handles the index differently.\n\nimport dask.dataframe as dd\n\n# Make unique_id a column\nseries = series.reset_index()\nseries['unique_id'] = series['unique_id'].astype(str)\n\nddf = dd.from_pandas(series, npartitions=4)\n\n\nsf.forecast(df=ddf, h=horizon).compute().head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoETS\n\n\n\n\n0\n0\n2000-08-10\n5.261609\n\n\n1\n0\n2000-08-11\n6.196357\n\n\n2\n0\n2000-08-12\n0.282309\n\n\n3\n0\n2000-08-13\n1.264195\n\n\n4\n0\n2000-08-14\n2.262453"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html",
    "href": "docs/how-to-guides/exogenous.html",
    "title": "Exogenous Regressors",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#introduction",
    "href": "docs/how-to-guides/exogenous.html#introduction",
    "title": "Exogenous Regressors",
    "section": "Introduction",
    "text": "Introduction\nExogenous regressors are variables that can affect the values of a time series. They may not be directly related to the variable that is beging forecasted, but they can still have an impact on it. Examples of exogenous regressors are weather data, economic indicators, or promotional sales. They are typically collected from external sources and by incorporating them into a forecasting model, they can improve the accuracy of our predictions.\nBy the end of this tutorial, you‚Äôll have a good understanding of how to incorporate exogenous regressors into StatsForecast‚Äôs models. Furthermore, you‚Äôll see how to evaluate their performance and decide whether or not they can help enhance the forecast.\nOutline\n\nInstall libraries\nLoad and explore the data\nSplit train/test set\nAdd exogenous regressors\nCreate future exogenous regressors\nTrain model\nEvaluate results\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#install-libraries",
    "href": "docs/how-to-guides/exogenous.html#install-libraries",
    "title": "Exogenous Regressors",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\n# pip install statsforecast -U"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#load-and-explore-the-data",
    "href": "docs/how-to-guides/exogenous.html#load-and-explore-the-data",
    "title": "Exogenous Regressors",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nIn this example, we‚Äôll use a single time series from the M5 Competition dataset. This series represents the daily sales of a product in a Walmart store. We‚Äôll first import the complete dataset from datasetsforecast, which you can install using pip install datasetsforecast.\n\n# pip install datasetsforecast -U\n\n\nfrom datasetsforecast.m5 import M5\n\nThe function to load the data is M5.load, which requieres the following argument.\n\ndirectory: (str) The directory where the data will be downloaded.\n\nThis function returns multiple outputs. We need the first two.\n\nY_df: (pandas DataFrame) The target time series with columns [unique_id, ds, y].\nX_df: (pandas DataFrame) Exogenous time series with columns [unique_id, ds, exogenous regressors].\n\n\nY_df, X_df, *_ = M5.load('./data')\n\nWe now need to filter the dataset. The product-store combination that we‚Äôll use in this notebook has unique_id = FOODS_3_586_CA_3. This time series was chosen because it is not intermittent and has exogenous regressors that will be useful for forecasting.\n\n# Filter data \nY_ts = Y_df[Y_df['unique_id'] == 'FOODS_3_586_CA_3'].reset_index(drop = True)\nX_ts = X_df[X_df['unique_id'] == 'FOODS_3_586_CA_3'].reset_index(drop = True)\n\nY_ts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_3_586_CA_3\n2011-01-29\n56.0\n\n\n1\nFOODS_3_586_CA_3\n2011-01-30\n55.0\n\n\n2\nFOODS_3_586_CA_3\n2011-01-31\n45.0\n\n\n3\nFOODS_3_586_CA_3\n2011-02-01\n57.0\n\n\n4\nFOODS_3_586_CA_3\n2011-02-02\n54.0\n\n\n\n\n\n\n\nWe can plot the sales of this product-store combination with the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the requiered ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nengine: str = matplotlib. It can also be plotly. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(Y_ts, engine='plotly')\n\n\n                                                \n\n\nThe M5 Competition included several exogenous regressors. Here we‚Äôll use the following two.\n\nsell_price: The price of the product for the given store. The price is provided per week.\nsnap_CA: A binary variable indicating whether the store allows SNAP purchases (1 if yes, 0 otherwise). SNAP stands for Supplement Nutrition Assitance Program, and it gives individuals and families money to help them purchase food products.\n\n\nX_ts = X_ts[['unique_id', 'ds', 'sell_price', 'snap_CA']]\nX_ts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nsell_price\nsnap_CA\n\n\n\n\n0\nFOODS_3_586_CA_3\n2011-01-29\n1.48\n0\n\n\n1\nFOODS_3_586_CA_3\n2011-01-30\n1.48\n0\n\n\n2\nFOODS_3_586_CA_3\n2011-01-31\n1.48\n0\n\n\n3\nFOODS_3_586_CA_3\n2011-02-01\n1.48\n1\n\n\n4\nFOODS_3_586_CA_3\n2011-02-02\n1.48\n1\n\n\n\n\n\n\n\nHere the unique_id is a category, but for the exogenous regressors it needs to be a string.\n\nX_ts['unique_id'] = X_ts.unique_id.astype(str)\n\nWe can plot the exogenous regressors using plotly. We could use statsforecast.plot, but then one of the regressors must be renamed y, and the name must be changed back to the original before generating the forecast.\n\nStatsForecast.plot(Y_ts.head(0), X_ts, unique_ids=['FOODS_3_586_CA_3'], engine='plotly')\n\n\n                                                \n\n\nFrom this plot, we can conclude that price has increased twice and that SNAP occurs at regular intervals."
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#split-traintest-set",
    "href": "docs/how-to-guides/exogenous.html#split-traintest-set",
    "title": "Exogenous Regressors",
    "section": "Split train/test set",
    "text": "Split train/test set\nIn the M5 Competition, participants had to forecast sales for the last 28 days in the dataset. We‚Äôll use the same forecast horizon and create the train and test sets accordingly.\n\n# Extract dates for train and test set \ndates = Y_df['ds'].unique()\ndtrain = dates[:-28]\ndtest = dates[-28:]\n\nY_train = Y_ts.query('ds in @dtrain')\nY_test = Y_ts.query('ds in @dtest') \n\nX_train = X_ts.query('ds in @dtrain') \nX_test = X_ts.query('ds in @dtest')"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#add-exogenous-regressors",
    "href": "docs/how-to-guides/exogenous.html#add-exogenous-regressors",
    "title": "Exogenous Regressors",
    "section": "Add exogenous regressors",
    "text": "Add exogenous regressors\nThe exogenous regressors need to be place after the target variable y.\n\ntrain = Y_train.merge(X_ts, how = 'left', on = ['unique_id', 'ds']) \ntrain.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nsell_price\nsnap_CA\n\n\n\n\n0\nFOODS_3_586_CA_3\n2011-01-29\n56.0\n1.48\n0\n\n\n1\nFOODS_3_586_CA_3\n2011-01-30\n55.0\n1.48\n0\n\n\n2\nFOODS_3_586_CA_3\n2011-01-31\n45.0\n1.48\n0\n\n\n3\nFOODS_3_586_CA_3\n2011-02-01\n57.0\n1.48\n1\n\n\n4\nFOODS_3_586_CA_3\n2011-02-02\n54.0\n1.48\n1"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#create-future-exogenous-regressors",
    "href": "docs/how-to-guides/exogenous.html#create-future-exogenous-regressors",
    "title": "Exogenous Regressors",
    "section": "Create future exogenous regressors",
    "text": "Create future exogenous regressors\nWe need to include the future values of the exogenous regressors so that we can produce the forecasts. Notice that we already have this information in X_test.\n\nX_test.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nsell_price\nsnap_CA\n\n\n\n\n1941\nFOODS_3_586_CA_3\n2016-05-23\n1.68\n0\n\n\n1942\nFOODS_3_586_CA_3\n2016-05-24\n1.68\n0\n\n\n1943\nFOODS_3_586_CA_3\n2016-05-25\n1.68\n0\n\n\n1944\nFOODS_3_586_CA_3\n2016-05-26\n1.68\n0\n\n\n1945\nFOODS_3_586_CA_3\n2016-05-27\n1.68\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the future values of the exogenous regressors are not available, then they must be forecasted or the regressors need to be eliminated from the model. Without them, it is not possible to generate the forecast."
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#train-model",
    "href": "docs/how-to-guides/exogenous.html#train-model",
    "title": "Exogenous Regressors",
    "section": "Train model",
    "text": "Train model\nTo generate the forecast, we‚Äôll use AutoARIMA, which is one of the models available in StatsForecast that allows exogenous regressors. To use this model, we first need to import it from statsforecast.models and then we need to instatiate it. Given that we‚Äôre working with daily data, we need to set season_length = 7.\n\nfrom statsforecast.models import AutoARIMA\n\n# Create a list with the model and its instantiation parameters \nmodels = [AutoARIMA(season_length = 7)]\n\nNext, we need to instantiate a new StatsForecast object, which has the following parameters.\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\nfreq: A string indicating the frequency of the data. See pandas‚Äô available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    models=models, \n    freq='D', \n    n_jobs=-1\n)\n\nNow we‚Äôre ready to generate the forecast. To do this, we‚Äôll use the forecast method, which takes the following arguments.\n\nh: An integer that represents the forecast horizon. In this case, we‚Äôll forecast the next 28 days.\nX_df: A pandas dataframe with the future values of the exogenous regressors.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nhorizon = 28\nlevel = [95]\n\nfcst = sf.forecast(df=train, h=horizon, X_df=X_test, level=level)\nfcst = fcst.reset_index()\nfcst.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoARIMA\nAutoARIMA-lo-95\nAutoARIMA-hi-95\n\n\n\n\n0\nFOODS_3_586_CA_3\n2016-05-23\n73.246307\n44.440239\n102.052383\n\n\n1\nFOODS_3_586_CA_3\n2016-05-24\n71.572723\n41.271477\n101.873962\n\n\n2\nFOODS_3_586_CA_3\n2016-05-25\n68.349289\n37.841881\n98.856697\n\n\n3\nFOODS_3_586_CA_3\n2016-05-26\n65.689095\n35.130322\n96.247864\n\n\n4\nFOODS_3_586_CA_3\n2016-05-27\n65.225807\n34.642029\n95.809593\n\n\n\n\n\n\n\nWe can plot the forecasts with the statsforecast.plot method described above.\n\nStatsForecast.plot(Y_ts, fcst, max_insample_length=28*2, engine='plotly')"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#evaluate-results",
    "href": "docs/how-to-guides/exogenous.html#evaluate-results",
    "title": "Exogenous Regressors",
    "section": "Evaluate results",
    "text": "Evaluate results\nWe‚Äôll merge the test set and the forecast to evaluate the accuracy using the mean absolute error (MAE).\n\nres = Y_test.merge(fcst, how='left', on=['unique_id', 'ds'])\nres.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nAutoARIMA\nAutoARIMA-lo-95\nAutoARIMA-hi-95\n\n\n\n\n0\nFOODS_3_586_CA_3\n2016-05-23\n66.0\n73.246307\n44.440239\n102.052383\n\n\n1\nFOODS_3_586_CA_3\n2016-05-24\n62.0\n71.572723\n41.271477\n101.873962\n\n\n2\nFOODS_3_586_CA_3\n2016-05-25\n40.0\n68.349289\n37.841881\n98.856697\n\n\n3\nFOODS_3_586_CA_3\n2016-05-26\n72.0\n65.689095\n35.130322\n96.247864\n\n\n4\nFOODS_3_586_CA_3\n2016-05-27\n69.0\n65.225807\n34.642029\n95.809593\n\n\n\n\n\n\n\n\nmae = abs(res['y']-res['AutoARIMA']).mean()\nprint('The MAE with exogenous regressors is '+str(round(mae,2)))\n\nThe MAE with exogenous regressors is 11.45\n\n\nTo check whether the exogenous regressors were useful or not, we need to generate the forecast again, now without them. To do this, we simple pass the dataframe wihtout exogenous variables to the forecast method. Notice that the data only includes unique_id, ds, and y. The forecast method no longer requieres the future values of the exogenous regressors X_df.\n\n# univariate model \nfcst_u = sf.forecast(df=train[['unique_id', 'ds', 'y']], h=28)\n\nres_u = Y_test.merge(fcst_u, how='left', on=['unique_id', 'ds'])\nmae_u = abs(res_u['y']-res_u['AutoARIMA']).mean()\n\n\nprint('The MAE without exogenous regressors is '+str(round(mae_u,2)))\n\nHence, we can conclude that using sell_price and snap_CA as external regressors helped improve the forecast."
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html",
    "href": "docs/how-to-guides/getting_started_complete_polars.html",
    "title": "End to End Walkthrough with Polars",
    "section": "",
    "text": "This document aims to highlight the recent integration of Polars, a robust and high-speed DataFrame library developed in Rust, into the functionality of StatsForecast. Polars, with its nimble and potent capabilities, has rapidly established a strong reputation within the Data Science community, further solidifying its position as a reliable tool for managing and manipulating substantial data sets.\nAvailable in languages including Rust, Python, Node.js, and R, Polars demonstrates a remarkable ability to handle sizable data sets with efficiency and speed that surpasses many other DataFrame libraries, such as Pandas. Polars‚Äô open-source nature invites ongoing enhancements and contributions, augmenting its appeal within the data science arena.\nThe most significant features of Polars that contribute to its rapid adoption are:\n\nPerformance Efficiency: Constructed using Rust, Polars exhibits an exemplary ability to manage substantial datasets with remarkable speed and minimal memory usage.\nLazy Evaluation: Polars operates on the principle of ‚Äòlazy evaluation‚Äô, creating an optimized logical plan of operations for efficient execution, a feature that mirrors the functionality of Apache Spark.\nParallel Execution: Demonstrating the capability to exploit multi-core CPUs, Polars facilitates parallel execution of operations, substantially accelerating data processing tasks.\n\n\n\n\n\n\n\nPrerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\n\n\n\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\n\nInstall packages.\nRead the data.\nExplore the data.\nTrain many models for every unique combination of time series.\nEvaluate the model‚Äôs performance using cross-validation.\nSelect the best model for every unique time series.\n\n\n\n\n\n\n\nNot Covered in this guide\n\n\n\n\n\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\n\nTraining models on Multiple Seasonalities.\n\nLearn to use multiple seasonality in this Electricity Load forecasting tutorial.\n\nUsing external regressors or exogenous variables\n\nFollow this tutorial to include exogenous variables like weather or holidays or static variables like category or family.\n\nComparing StatsForecast with other popular libraries.\n\nYou can reproduce our benchmarks here.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#introducing-polars-a-high-performance-dataframe-library",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#introducing-polars-a-high-performance-dataframe-library",
    "title": "End to End Walkthrough with Polars",
    "section": "",
    "text": "This document aims to highlight the recent integration of Polars, a robust and high-speed DataFrame library developed in Rust, into the functionality of StatsForecast. Polars, with its nimble and potent capabilities, has rapidly established a strong reputation within the Data Science community, further solidifying its position as a reliable tool for managing and manipulating substantial data sets.\nAvailable in languages including Rust, Python, Node.js, and R, Polars demonstrates a remarkable ability to handle sizable data sets with efficiency and speed that surpasses many other DataFrame libraries, such as Pandas. Polars‚Äô open-source nature invites ongoing enhancements and contributions, augmenting its appeal within the data science arena.\nThe most significant features of Polars that contribute to its rapid adoption are:\n\nPerformance Efficiency: Constructed using Rust, Polars exhibits an exemplary ability to manage substantial datasets with remarkable speed and minimal memory usage.\nLazy Evaluation: Polars operates on the principle of ‚Äòlazy evaluation‚Äô, creating an optimized logical plan of operations for efficient execution, a feature that mirrors the functionality of Apache Spark.\nParallel Execution: Demonstrating the capability to exploit multi-core CPUs, Polars facilitates parallel execution of operations, substantially accelerating data processing tasks.\n\n\n\n\n\n\n\nPrerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\n\n\n\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\n\nInstall packages.\nRead the data.\nExplore the data.\nTrain many models for every unique combination of time series.\nEvaluate the model‚Äôs performance using cross-validation.\nSelect the best model for every unique time series.\n\n\n\n\n\n\n\nNot Covered in this guide\n\n\n\n\n\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\n\nTraining models on Multiple Seasonalities.\n\nLearn to use multiple seasonality in this Electricity Load forecasting tutorial.\n\nUsing external regressors or exogenous variables\n\nFollow this tutorial to include exogenous variables like weather or holidays or static variables like category or family.\n\nComparing StatsForecast with other popular libraries.\n\nYou can reproduce our benchmarks here."
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#install-libraries",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#install-libraries",
    "title": "End to End Walkthrough with Polars",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast."
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#read-the-data",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#read-the-data",
    "title": "End to End Walkthrough with Polars",
    "section": "Read the data",
    "text": "Read the data\nWe will use polars to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary polars operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\nThis data set already satisfies the requirement.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport polars as pl\n\n\nY_df = pl.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\nY_df.head()\n\n\nshape: (5, 3)\n\n\n\nunique_id\nds\ny\n\n\nstr\ni64\nf64\n\n\n\n\n\"H1\"\n1\n605.0\n\n\n\"H1\"\n2\n586.0\n\n\n\"H1\"\n3\n586.0\n\n\n\"H1\"\n4\n559.0\n\n\n\"H1\"\n5\n511.0\n\n\n\n\n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility‚Äôs sake, we will select only 10 unique IDs and keep only the last week. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\nuids = Y_df['unique_id'].unique(maintain_order=True)[:10] # Select 10 ids to make the example faster\n\nY_df = Y_df.filter(pl.col('unique_id').is_in(uids))\n\nY_df = Y_df.groupby('unique_id').tail(7 * 24) #Select last 7 days of data to make example faster"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#explore-data-with-the-plot-method",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#explore-data-with-the-plot-method",
    "title": "End to End Walkthrough with Polars",
    "section": "Explore Data with the plot method",
    "text": "Explore Data with the plot method\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses matplotlib as a default engine. You can change to plotly by setting engine=\"plotly\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)\n\n/hdd/github/statsforecast/statsforecast/core.py:19: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#train-multiple-models-for-many-series",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#train-multiple-models-for-many-series",
    "title": "End to End Walkthrough with Polars",
    "section": "Train multiple models for many series",
    "text": "Train multiple models for many series\nStatsForecast can train many models on many time series efficiently.\nStart by importing and instantiating the desired models. StatsForecast offers a wide variety of models grouped in the following categories:\n\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt‚Äôs Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nHere you can check the complete list of models.\nFor this example we will use:\n\nAutoARIMA: Automatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Ref: AutoARIMA.\nHoltWinters: triple exponential smoothing, Holt-Winters‚Äô method is an extension of exponential smoothing for series that contain both trend and seasonality. Ref: HoltWinters\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive\nHistoricAverage: arthimetic mean. Ref: HistoricAverage.\nDynamicOptimizedTheta: The theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series. Ref: DynamicOptimizedTheta.\n\nImport and instantiate the models. Setting the season_length argument is sometimes tricky. This article on Seasonal periods) by the master, Rob Hyndmann, can be useful.\n\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive\n)\n\n\n# Create a list of models and instantiation parameters\nmodels = [\n    AutoARIMA(season_length=24),\n    HoltWinters(),\n    Croston(),\n    SeasonalNaive(season_length=24),\n    HistoricAverage(),\n    DOT(season_length=24)\n]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.) This is also available with Polars.\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n    n_jobs=-1,\n    fallback_model = SeasonalNaive(season_length=7),\n    verbose=True\n)\n\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nThe forecast method is compatible with distributed clusters, so it does not store any model parameters. If you want to store parameters for every model you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nforecasts_df = sf.forecast(h=48, level=[90])\n\nforecasts_df.head()\n\n\nshape: (5, 20)\n\n\n\nunique_id\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\nHoltWinters\nHoltWinters-lo-90\nHoltWinters-hi-90\nCrostonClassic\nCrostonClassic-lo-90\nCrostonClassic-hi-90\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nDynamicOptimizedTheta\nDynamicOptimizedTheta-lo-90\nDynamicOptimizedTheta-hi-90\n\n\nstr\ni64\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\n\n\n\n\n\"H1\"\n749\n592.461792\n572.325623\n612.597961\n829.0\n-246.367554\n1904.367554\n829.0\n-246.367554\n1904.367554\n635.0\n537.471191\n732.528809\n660.982117\n398.03775\n923.926514\n592.701843\n577.677307\n611.652649\n\n\n\"H1\"\n750\n527.174316\n495.321777\n559.026855\n807.0\n-268.367554\n1882.367554\n807.0\n-268.367554\n1882.367554\n572.0\n474.471222\n669.528809\n660.982117\n398.03775\n923.926514\n525.589111\n505.449738\n546.621826\n\n\n\"H1\"\n751\n488.418549\n445.535583\n531.301514\n785.0\n-290.367554\n1860.367554\n785.0\n-290.367554\n1860.367554\n532.0\n434.471222\n629.528809\n660.982117\n398.03775\n923.926514\n489.251801\n462.072876\n512.424133\n\n\n\"H1\"\n752\n452.284454\n400.677155\n503.891785\n756.0\n-319.367554\n1831.367554\n756.0\n-319.367554\n1831.367554\n493.0\n395.471222\n590.528809\n660.982117\n398.03775\n923.926514\n456.195038\n430.554291\n478.260956\n\n\n\"H1\"\n753\n433.127563\n374.070984\n492.184143\n719.0\n-356.367554\n1794.367554\n719.0\n-356.367554\n1794.367554\n477.0\n379.471222\n574.528809\n660.982117\n398.03775\n923.926514\n436.290527\n411.051239\n461.815948\n\n\n\n\n\n\nPlot the results of 8 randon series using the StatsForecast.plot method.\n\nsf.plot(Y_df,forecasts_df, engine='plotly')\n\n\n                                                \n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nsf.plot(Y_df, forecasts_df, models=[\"HoltWinters\",\"DynamicOptimizedTheta\"], unique_ids=[\"H10\", \"H105\"], level=[90], engine='plotly')\n\n\n                                                \n\n\n\n# Explore other models \nsf.plot(Y_df, forecasts_df, models=[\"AutoARIMA\"], unique_ids=[\"H10\", \"H105\"], level=[90], engine='plotly')"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#evaluate-the-models-performance",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#evaluate-the-models-performance",
    "title": "End to End Walkthrough with Polars",
    "section": "Evaluate the model‚Äôs performance",
    "text": "Evaluate the model‚Äôs performance\nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 2 days (n_windows=2), forecasting every second day (step_size=48). Depending on your computer, this step should take around 1 min.\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvaldation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=2\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvaldation_df.head()\n\n\nshape: (5, 10)\n\n\n\nunique_id\nds\ncutoff\ny\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\n\n\nstr\ni64\ni64\nf32\nf32\nf32\nf32\nf32\nf32\nf32\n\n\n\n\n\"H1\"\n701\n700\n619.0\n603.925415\n847.0\n742.668762\n691.0\n661.674988\n612.767517\n\n\n\"H1\"\n702\n700\n565.0\n507.591736\n820.0\n742.668762\n618.0\n661.674988\n536.846252\n\n\n\"H1\"\n703\n700\n532.0\n481.281677\n790.0\n742.668762\n563.0\n661.674988\n497.82428\n\n\n\"H1\"\n704\n700\n495.0\n444.410248\n784.0\n742.668762\n529.0\n661.674988\n464.723236\n\n\n\"H1\"\n705\n700\n481.0\n421.168762\n752.0\n742.668762\n504.0\n661.674988\n440.972351\n\n\n\n\n\n\nNext, we will evaluate the performance of every model for every series using common error metrics like Mean Absolute Error (MAE) or Mean Square Error (MSE) Define a utility function to evaluate different error metrics for the cross validation data frame.\nFirst import the desired error metrics from datasetsforecast.losses. Then define a utility function that takes a cross-validation data frame as a metric and returns an evaluation data frame with the average of the error metric for every unique id and fitted model and all cutoffs.\n\nfrom utilsforecast.losses import mse\nfrom utilsforecast.evaluation import evaluate\n\n\ndef evaluate_cross_validation(df, metric):\n    models = [c for c in df.columns if c not in ('unique_id', 'ds', 'cutoff', 'y')]\n    evals = []\n    # Calculate loss for every unique_id and cutoff.    \n    for cutoff in df['cutoff'].unique():\n        eval_ = evaluate(df.filter(pl.col('cutoff') == cutoff), metrics=[metric], models=models)\n        evals.append(eval_)\n    evals = pl.concat(evals).drop('metric')\n    # Calculate the mean of each 'unique_id' group\n    evals = evals.groupby(['unique_id'], maintain_order=True).mean() \n\n    # For each row in evals (excluding 'unique_id'), find the model with the lowest value\n    best_model = [min(row, key=row.get) for row in evals.drop('unique_id').rows(named=True)]\n\n    # Add a 'best_model' column to evals dataframe with the best model for each 'unique_id'\n    evals = evals.with_columns(pl.Series(best_model).alias('best_model')).sort(by=['unique_id'])\n    return evals\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = evaluate_cross_validation(crossvaldation_df, mse)\n\nevaluation_df.head()\n\n\nshape: (5, 8)\n\n\n\nunique_id\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\nbest_model\n\n\nstr\nf32\nf32\nf32\nf32\nf32\nf32\nstr\n\n\n\n\n\"H1\"\n1979.302368\n44888.019531\n28038.734375\n1422.666748\n20927.666016\n1296.333984\n\"DynamicOptimiz‚Ä¶\n\n\n\"H10\"\n458.8927\n2812.916504\n1483.483887\n96.895828\n1980.367676\n379.621094\n\"SeasonalNaive\"\n\n\n\"H100\"\n8629.949219\n121625.375\n91945.140625\n12019.0\n78491.195312\n21699.648438\n\"AutoARIMA\"\n\n\n\"H101\"\n6818.348145\n28453.394531\n16183.631836\n10944.458008\n18208.404297\n63698.070312\n\"AutoARIMA\"\n\n\n\"H102\"\n65489.964844\n232924.84375\n132655.3125\n12699.896484\n309110.46875\n31393.519531\n\"SeasonalNaive\"\n\n\n\n\n\n\nCreate a summary table with a model column and the number of series where that model performs best. In this case, the Arima and Seasonal Naive are the best models for 10 series and the Theta model should be used for two.\n\nselect_cols = ['best_model', 'unique_id']\n\nsummary_df = (\n    evaluation_df\n    .groupby('best_model')\n    .n_unique()\n    [select_cols]\n    .sort(by='unique_id')\n    .rename(dict(zip(select_cols, [\"Model\", \"Nr. of unique_ids\"])))\n)\n\nsummary_df\n\n\nshape: (3, 2)\n\n\n\nModel\nNr. of unique_ids\n\n\nstr\nu32\n\n\n\n\n\"DynamicOptimiz‚Ä¶\n2\n\n\n\"SeasonalNaive\"\n4\n\n\n\"AutoARIMA\"\n4\n\n\n\n\n\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nseasonal_ids = evaluation_df.filter(pl.col('best_model') == 'SeasonalNaive')['unique_id']\n\nsf.plot(Y_df,forecasts_df, unique_ids=seasonal_ids, models=[\"SeasonalNaive\",\"DynamicOptimizedTheta\"], engine='plotly')"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#select-the-best-model-for-every-unique-series",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#select-the-best-model-for-every-unique-series",
    "title": "End to End Walkthrough with Polars",
    "section": "Select the best model for every unique series",
    "text": "Select the best model for every unique series\nDefine a utility function that takes your forecast‚Äôs data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df):\n    # Melt the 'forecasts_df' dataframe to long format, where each row represents\n    # a unique ID, a timestamp, a model, and that model's forecast.\n    df = (\n        forecasts_df\n        .melt(\n            id_vars=[\"unique_id\", \"ds\"], \n            value_vars=forecasts_df.columns[2:], \n            variable_name=\"model\", \n            value_name=\"best_model_forecast\"\n        )\n        # Join this dataframe with 'evaluation_df' on 'unique_id', attaching \n        # the 'best_model' for each unique ID.\n        .join(\n            evaluation_df[['unique_id', 'best_model']],\n            on='unique_id',\n            how=\"left\",\n        )\n    )\n\n    # Clean up the 'model' names by removing \"-lo-90\" and \"-hi-90\" from them,\n    # and store the cleaned names in a new column called 'clean_model'.\n    # Filter the dataframe to keep only the rows where 'clean_model' matches 'best_model'.\n    # After that, drop the 'clean_model' and 'best_model' columns, as they are no longer needed.\n    df = (\n        df\n        .with_columns(\n            pl.col('model').str.replace(\"-lo-90|-hi-90\", \"\").alias(\"clean_model\")\n        )\n        .filter(pl.col('clean_model') == pl.col('best_model'))\n        .drop('clean_model', 'best_model')\n    )\n\n    # Rename all the 'model' names to \"best_model\" for clarity, \n    # because at this point the dataframe only contains forecasts from the best model for each unique ID.\n    # Then, reshape the dataframe back to wide format using the 'pivot()' method.\n    # The pivoted dataframe has one row per unique ID and timestamp, with a column for each 'model' \n    # (in this case, all models are renamed to 'best_model'), and the value in each cell is the 'best_model_forecast'.\n    # The 'pivot()' method requires an aggregate function to apply if there are multiple values for the same index and column.\n    # Here, it uses 'first', meaning it keeps the first value if there are multiple.\n    # Finally, sort the dataframe by 'unique_id' and 'ds'.\n    return (\n        df\n        .with_columns(\n            pl.col('model').str.replace(\"[A-Za-z0-9]+\", \"best_model\")\n        )\n        .pivot(\n            values='best_model_forecast',\n            index=['unique_id', 'ds'],\n            columns='model',\n            aggregate_function='first',\n        )\n        .sort(by=['unique_id', 'ds'])\n    )\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(forecasts_df, evaluation_df)\n\nprod_forecasts_df.head()\n\n\nshape: (5, 5)\n\n\n\nunique_id\nds\nbest_model\nbest_model-lo-90\nbest_model-hi-90\n\n\nstr\ni64\nf32\nf32\nf32\n\n\n\n\n\"H1\"\n749\n592.701843\n577.677307\n611.652649\n\n\n\"H1\"\n750\n525.589111\n505.449738\n546.621826\n\n\n\"H1\"\n751\n489.251801\n462.072876\n512.424133\n\n\n\"H1\"\n752\n456.195038\n430.554291\n478.260956\n\n\n\"H1\"\n753\n436.290527\n411.051239\n461.815948\n\n\n\n\n\n\nPlot the results.\n\nsf.plot(Y_df, prod_forecasts_df, level=[90], engine='plotly')"
  },
  {
    "objectID": "docs/how-to-guides/ets_ray_m5.html",
    "href": "docs/how-to-guides/ets_ray_m5.html",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "",
    "text": "In this notebook we show how to use StatsForecast and ray to forecast thounsands of time series in less than 6 minutes (M5 dataset). Also, we show that StatsForecast has better performance in time and accuracy compared to Prophet running on a Spark cluster using DataBricks.\nIn this example, we used a ray cluster (AWS) of 11 instances of type m5.2xlarge (8 cores, 32 GB RAM).\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/ets_ray_m5.html#installing-statsforecast-library",
    "href": "docs/how-to-guides/ets_ray_m5.html#installing-statsforecast-library",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install \"statsforecast[ray]\" neuralforecast s3fs pyarrow\n\n\nfrom time import time\n\nimport pandas as pd\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import ETS"
  },
  {
    "objectID": "docs/how-to-guides/ets_ray_m5.html#download-data",
    "href": "docs/how-to-guides/ets_ray_m5.html#download-data",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Download data",
    "text": "Download data\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\n\nY_df = pd.read_parquet('s3://m5-benchmarks/data/train/target.parquet')\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_1_001_CA_1\n2011-01-29\n3.0\n\n\n1\nFOODS_1_001_CA_1\n2011-01-30\n0.0\n\n\n2\nFOODS_1_001_CA_1\n2011-01-31\n0.0\n\n\n3\nFOODS_1_001_CA_1\n2011-02-01\n1.0\n\n\n4\nFOODS_1_001_CA_1\n2011-02-02\n4.0\n\n\n\n\n\n\n\nSince the M5 dataset contains intermittent time series, we add a constant to avoid problems during the training phase. Later, we will substract the constant from the forecasts.\n\nconstant = 10\nY_df['y'] += constant"
  },
  {
    "objectID": "docs/how-to-guides/ets_ray_m5.html#train-the-model",
    "href": "docs/how-to-guides/ets_ray_m5.html#train-the-model",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Train the model",
    "text": "Train the model\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality. Observe that we need to pass the ray address to the ray_address argument.\n\nfcst = StatsForecast(\n    df=Y_df, \n    models=[ETS(season_length=7, model='ZNA')], \n    freq='D', \n    #n_jobs=-1\n    ray_address='ray://ADDRESS:10001'\n)\n\n\ninit = time()\nY_hat = fcst.forecast(28)\nend = time()\nprint(f'Minutes taken by StatsForecast using: {(end - init) / 60}')\n\n/home/ubuntu/miniconda/envs/ray/lib/python3.7/site-packages/ray/util/client/worker.py:618: UserWarning: More than 10MB of messages have been created to schedule tasks on the server. This can be slow on Ray Client due to communication overhead over the network. If you're running many fine-grained tasks, consider running them inside a single remote function. See the section on \"Too fine-grained tasks\" in the Ray Design Patterns document for more details: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.f7ins22n6nyl. If your functions frequently use large objects, consider storing the objects remotely with ray.put. An example of this is shown in the \"Closure capture of large / unserializable object\" section of the Ray Design Patterns document, available here: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.1afmymq455wu\n  UserWarning,\n\n\nMinutes taken by StatsForecast using: 5.4817593971888225\n\n\nStatsForecast and ray took only 5.48 minutes to train 30,490 time series, compared to 18.23 minutes for Prophet and Spark.\nWe remove the constant.\n\nY_hat['ETS'] -= constant\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = Y_hat.reset_index().set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50.2M/50.2M [00:00&lt;00:00, 77.1MiB/s]\n\n\n\nM5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\n\n\n\n\n\n\nwrmsse\n\n\n\n\nTotal\n0.677233\n\n\nLevel1\n0.435558\n\n\nLevel2\n0.522863\n\n\nLevel3\n0.582109\n\n\nLevel4\n0.488484\n\n\nLevel5\n0.567825\n\n\nLevel6\n0.587605\n\n\nLevel7\n0.662774\n\n\nLevel8\n0.647712\n\n\nLevel9\n0.732107\n\n\nLevel10\n1.013124\n\n\nLevel11\n0.970465\n\n\nLevel12\n0.916175\n\n\n\n\n\n\n\nAlso, StatsForecast is more accurate than Prophet, since the overall WMRSSE is 0.68, against 0.77 obtained by prophet."
  },
  {
    "objectID": "docs/getting-started/installation.html",
    "href": "docs/getting-started/installation.html",
    "title": "Install",
    "section": "",
    "text": "You can install the released version of StatsForecast from the Python package index with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\n\n\n\n\n\n\nWarning\n\n\n\nWe are constantly updating StatsForecast, so we suggest fixing the version to avoid issues. pip install statsforecast==\"1.0.0\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe recommend installing your libraries inside a python virtual or conda environment.\n\n\n\nUser our env (optional)\nIf you don‚Äôt have a Conda environment and need tools like Numba, Pandas, NumPy, Jupyter, StatsModels, and Nbdev you can use ours by following these steps:\n\nClone the StatsForecast repo:\n\n$ git clone https://github.com/Nixtla/statsforecast.git && cd statsforecast\n\nCreate the environment using the environment.yml file:\n\n$ conda env create -f environment.yml\n\nActivate the environment:\n\n$ conda activate statsforecast\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html",
    "href": "docs/getting-started/getting_started_complete.html",
    "title": "End to End Walkthrough",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start.\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#install-libraries",
    "href": "docs/getting-started/getting_started_complete.html#install-libraries",
    "title": "End to End Walkthrough",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast."
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#read-the-data",
    "href": "docs/getting-started/getting_started_complete.html#read-the-data",
    "title": "End to End Walkthrough",
    "section": "Read the data",
    "text": "Read the data\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. The target column needs to be renamed to y if it has a different column name.\n\nThis data set already satisfies the requirements.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport pandas as pd\n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility‚Äôs sake, we will select only 10 unique IDs and keep only the last week. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\nuids = Y_df['unique_id'].unique()[:10] # Select 10 ids to make the example faster\n\nY_df = Y_df.query('unique_id in @uids') \n\nY_df = Y_df.groupby('unique_id').tail(7 * 24) #Select last 7 days of data to make example faster"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#explore-data-with-the-plot-method",
    "href": "docs/getting-started/getting_started_complete.html#explore-data-with-the-plot-method",
    "title": "End to End Walkthrough",
    "section": "Explore Data with the plot method",
    "text": "Explore Data with the plot method\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a defaul engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(Y_df, engine='plotly')"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#train-multiple-models-for-many-series",
    "href": "docs/getting-started/getting_started_complete.html#train-multiple-models-for-many-series",
    "title": "End to End Walkthrough",
    "section": "Train multiple models for many series",
    "text": "Train multiple models for many series\nStatsForecast can train many models on many time series efficiently.\nStart by importing and instantiating the desired models. StatsForecast offers a wide variety of models grouped in the following categories:\n\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt‚Äôs Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nHere you can check the complete list of models .\nFor this example we will use:\n\nAutoARIMA: Automatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Ref: AutoARIMA.\nHoltWinters: triple exponential smoothing, Holt-Winters‚Äô method is an extension of exponential smoothing for series that contain both trend and seasonality. Ref: HoltWinters\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive\nHistoricAverage: arthimetic mean. Ref: HistoricAverage.\nDynamicOptimizedTheta: The theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series. Ref: DynamicOptimizedTheta.\n\nImport and instantiate the models. Setting the season_length argument is sometimes tricky. This article on Seasonal periods) by the master, Rob Hyndmann, can be useful.\n\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive\n)\n\n\n# Create a list of models and instantiation parameters\nmodels = [\n    AutoARIMA(season_length=24),\n    HoltWinters(),\n    Croston(),\n    SeasonalNaive(season_length=24),\n    HistoricAverage(),\n    DOT(season_length=24)\n]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See pandas available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n    n_jobs=-1,\n    fallback_model = SeasonalNaive(season_length=7)\n)\n\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nThe forecast method is compatible with distributed clusters, so it does not store any model parameters. If you want to store parameters for every model you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nforecasts_df = sf.forecast(h=48, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\nHoltWinters\nHoltWinters-lo-90\nHoltWinters-hi-90\nCrostonClassic\nCrostonClassic-lo-90\nCrostonClassic-hi-90\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nDynamicOptimizedTheta\nDynamicOptimizedTheta-lo-90\nDynamicOptimizedTheta-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n749\n592.461792\n572.325623\n612.597961\n829.0\n-246.367554\n1904.367554\n829.0\n-246.367554\n1904.367554\n635.0\n537.471191\n732.528809\n660.982117\n398.03775\n923.926514\n592.701843\n577.677307\n611.652649\n\n\nH1\n750\n527.174316\n495.321777\n559.026855\n807.0\n-268.367554\n1882.367554\n807.0\n-268.367554\n1882.367554\n572.0\n474.471222\n669.528809\n660.982117\n398.03775\n923.926514\n525.589111\n505.449738\n546.621826\n\n\nH1\n751\n488.418549\n445.535583\n531.301514\n785.0\n-290.367554\n1860.367554\n785.0\n-290.367554\n1860.367554\n532.0\n434.471222\n629.528809\n660.982117\n398.03775\n923.926514\n489.251801\n462.072876\n512.424133\n\n\nH1\n752\n452.284454\n400.677155\n503.891785\n756.0\n-319.367554\n1831.367554\n756.0\n-319.367554\n1831.367554\n493.0\n395.471222\n590.528809\n660.982117\n398.03775\n923.926514\n456.195038\n430.554291\n478.260956\n\n\nH1\n753\n433.127563\n374.070984\n492.184143\n719.0\n-356.367554\n1794.367554\n719.0\n-356.367554\n1794.367554\n477.0\n379.471222\n574.528809\n660.982117\n398.03775\n923.926514\n436.290527\n411.051239\n461.815948\n\n\n\n\n\n\n\nPlot the results of 8 random series using the StatsForecast.plot method.\n\nsf.plot(Y_df,forecasts_df, engine='plotly')\n\n\n                                                \n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nsf.plot(Y_df, forecasts_df, models=[\"HoltWinters\",\"DynamicOptimizedTheta\"], unique_ids=[\"H10\", \"H105\"], level=[90], engine='plotly')\n\n\n                                                \n\n\n\n# Explore other models \nsf.plot(Y_df, forecasts_df, models=[\"AutoARIMA\"], unique_ids=[\"H10\", \"H105\"], level=[90], engine='plotly')"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#evaluate-the-models-performance",
    "href": "docs/getting-started/getting_started_complete.html#evaluate-the-models-performance",
    "title": "End to End Walkthrough",
    "section": "Evaluate the model‚Äôs performance",
    "text": "Evaluate the model‚Äôs performance\nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 2 days (n_windows=2), forecasting every second day (step_size=48). Depending on your computer, this step should take around 1 min.\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvaldation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=2\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvaldation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n701\n700\n619.0\n603.925415\n847.0\n742.668762\n691.0\n661.674988\n612.767517\n\n\nH1\n702\n700\n565.0\n507.591736\n820.0\n742.668762\n618.0\n661.674988\n536.846252\n\n\nH1\n703\n700\n532.0\n481.281677\n790.0\n742.668762\n563.0\n661.674988\n497.824280\n\n\nH1\n704\n700\n495.0\n444.410248\n784.0\n742.668762\n529.0\n661.674988\n464.723236\n\n\nH1\n705\n700\n481.0\n421.168762\n752.0\n742.668762\n504.0\n661.674988\n440.972351\n\n\n\n\n\n\n\nNext, we will evaluate the performance of every model for every series using common error metrics like Mean Absolute Error (MAE) or Mean Square Error (MSE) Define a utility function to evaluate different error metrics for the cross validation data frame.\nFirst import the desired error metrics from mlforecast.losses. Then define a utility function that takes a cross-validation data frame as a metric and returns an evaluation data frame with the average of the error metric for every unique id and fitted model and all cutoffs.\n\nfrom utilsforecast.losses import mse\nfrom utilsforecast.evaluation import evaluate\n\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['unique_id', 'ds', 'cutoff', 'y']).columns.tolist()\n    evals = []\n    # Calculate loss for every unique_id and cutoff.    \n    for cutoff in df['cutoff'].unique():\n        eval_ = evaluate(df[df['cutoff'] == cutoff], metrics=[metric], models=models)\n        evals.append(eval_)\n    evals = pd.concat(evals)\n    evals = evals.groupby('unique_id').mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n    evals['best_model'] = evals.idxmin(axis=1)\n    return evals\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = evaluate_cross_validation(crossvaldation_df.reset_index(), mse)\n\nevaluation_df.head()\n\n\n\n\n\n\n\n\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\nbest_model\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\nH1\n1979.302124\n44888.019531\n28038.736328\n1422.666748\n20927.664062\n1296.333984\nDynamicOptimizedTheta\n\n\nH10\n458.892700\n2812.916504\n1483.484131\n96.895828\n1980.367432\n379.621124\nSeasonalNaive\n\n\nH100\n8629.948242\n121625.375000\n91945.140625\n12019.000000\n78491.187500\n21699.648438\nAutoARIMA\n\n\nH101\n6818.348633\n28453.394531\n16183.634766\n10944.458008\n18208.404297\n63698.082031\nAutoARIMA\n\n\nH102\n65489.964844\n232924.843750\n132655.296875\n12699.896484\n309110.468750\n31393.519531\nSeasonalNaive\n\n\n\n\n\n\n\nCreate a summary table with a model column and the number of series where that model performs best. In this case, the Arima and Seasonal Naive are the best models for 10 series and the Theta model should be used for two.\n\nsummary_df = evaluation_df.groupby('best_model').size().sort_values().to_frame()\n\nsummary_df.reset_index().columns = [\"Model\", \"Nr. of unique_ids\"]\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nseasonal_ids = evaluation_df.query('best_model == \"SeasonalNaive\"').index\n\nsf.plot(Y_df,forecasts_df, unique_ids=seasonal_ids, models=[\"SeasonalNaive\",\"DynamicOptimizedTheta\"], engine='plotly')"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "href": "docs/getting-started/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "title": "End to End Walkthrough",
    "section": "Select the best model for every unique series",
    "text": "Select the best model for every unique series\nDefine a utility function that takes your forecast‚Äôs data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df):\n    df = forecasts_df.set_index('ds', append=True).stack().to_frame().reset_index(level=2) # Wide to long \n    df.columns = ['model', 'best_model_forecast'] \n    df = df.join(evaluation_df[['best_model']])\n    df = df.query('model.str.replace(\"-lo-90|-hi-90\", \"\", regex=True) == best_model').copy()\n    df.loc[:, 'model'] = [model.replace(bm, 'best_model') for model, bm in zip(df['model'], df['best_model'])]\n    df = df.drop(columns='best_model').set_index('model', append=True).unstack()\n    df.columns = df.columns.droplevel()\n    df = df.reset_index(level=1)\n    return df\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(forecasts_df, evaluation_df)\n\nprod_forecasts_df.head()\n\n\n\n\n\n\n\nmodel\nds\nbest_model\nbest_model-hi-90\nbest_model-lo-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nH1\n749\n592.701843\n611.652649\n577.677307\n\n\nH1\n750\n525.589111\n546.621826\n505.449738\n\n\nH1\n751\n489.251801\n512.424133\n462.072876\n\n\nH1\n752\n456.195038\n478.260956\n430.554291\n\n\nH1\n753\n436.290527\n461.815948\n411.051239\n\n\n\n\n\n\n\nPlot the results.\n\nsf.plot(Y_df, prod_forecasts_df, level=[90], engine='plotly')"
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html",
    "href": "docs/tutorials/electricitypeakforecasting.html",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers‚Äô combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an MSTL (Multiple Seasonal-Trend decomposition using LOESS) model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the StatsForecast.cross_validation method to fit the MSTL model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit MSTL model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#introduction",
    "href": "docs/tutorials/electricitypeakforecasting.html#introduction",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers‚Äô combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an MSTL (Multiple Seasonal-Trend decomposition using LOESS) model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the StatsForecast.cross_validation method to fit the MSTL model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit MSTL model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#libraries",
    "href": "docs/tutorials/electricitypeakforecasting.html#libraries",
    "title": "Detect Demand Peaks",
    "section": "Libraries",
    "text": "Libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast"
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#load-data",
    "href": "docs/tutorials/electricitypeakforecasting.html#load-data",
    "title": "Detect Demand Peaks",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nFirst, download and read the 2022 historic total demand of the ERCOT market, available here. The data processing includes adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest. This step should take around 2s.\n\nimport numpy as np\nimport pandas as pd\n\n\n# Load data\nhistoric = pd.read_csv('./Native_Load_2022.csv')\n# Add missing hour due to daylight saving time\nhistoric = pd.concat([historic, pd.DataFrame({'Hour Ending':['03/13/2022 03:00'], 'ERCOT':['43980.57']})])\nhistoric = historic.sort_values('Hour Ending').reset_index(drop=True)\n# Convert to datetime\nhistoric['ERCOT'] = historic['ERCOT'].str.replace(',','').astype(float)\nhistoric = historic[~pd.isna(historic['ERCOT'])]\nhistoric['ds'] = pd.to_datetime(historic['Hour Ending'].str[:10]) + pd.to_timedelta(np.tile(range(24), len(historic)//24),'h')\nhistoric['unique_id'] = 'ERCOT'\nhistoric['y'] = historic['ERCOT']\n# Select relevant columns and dates\nY_df = historic[['unique_id', 'ds', 'y']]\nY_df = Y_df[Y_df['ds']&lt;='2022-10-01']\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)\n\n/Users/cchallu/NIXTLA/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n                                                \n\n\nWe observe that the time series exhibits seasonal patterns. Moreover, the time series contains 6,552 observations, so it is necessary to use computationally efficient methods to deploy them in production."
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "href": "docs/tutorials/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "title": "Detect Demand Peaks",
    "section": "Fit and Forecast MSTL model",
    "text": "Fit and Forecast MSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\n\n\n\n\n\n\nTip\n\n\n\nCheck our detailed explanation and tutorial on MSTL here\n\n\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, instantiate the model and define the parameters. The electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities. See this link for a detailed explanation on how to set seasonal lengths. In this example we use the AutoARIMA model for the trend component, however, any StatsForecast model can be used. The complete list of models is available here.\n\nmodels = [MSTL(\n            season_length=[24, 24 * 7], # seasonalities of the time series \n            trend_forecaster=AutoARIMA(nmodels=10) # model used to forecast trend\n            )\n          ]\n\n\n\n\n\n\n\nTip\n\n\n\nThe parameter nmodels of the AutoARIMA controls the number of models considered in stepwise search. The default is 94, reduce it to decrease training times!\n\n\nWe fit the model by instantiating a StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\n\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nThe cross_validation method allows the user to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods. This method re-trains the model and forecast each window. See this tutorial for an animation of how the windows are defined.\nUse the cross_validation method to produce all the daily forecasts for September. To produce daily forecasts set the forecasting horizon h as 24. In this example we are simulating deploying the pipeline during September, so set the number of windows as 30 (one for each day). Finally, set the step size between windows as 24, to only produce one forecast per day.\n\ncrossvalidation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=30\n  )\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nMSTL\n\n\nunique_id\n\n\n\n\n\n\n\n\nERCOT\n2022-09-01 00:00:00\n2022-08-31 23:00:00\n45482.468750\n47126.179688\n\n\nERCOT\n2022-09-01 01:00:00\n2022-08-31 23:00:00\n43602.660156\n45088.542969\n\n\nERCOT\n2022-09-01 02:00:00\n2022-08-31 23:00:00\n42284.820312\n43897.175781\n\n\nERCOT\n2022-09-01 03:00:00\n2022-08-31 23:00:00\n41663.160156\n43187.812500\n\n\nERCOT\n2022-09-01 04:00:00\n2022-08-31 23:00:00\n41710.621094\n43369.859375\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using cross_validation make sure the forecasts are produced at the desired timestamps. Check the cutoff column which specifices the last timestamp before the forecasting window."
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#peak-detection",
    "href": "docs/tutorials/electricitypeakforecasting.html#peak-detection",
    "title": "Detect Demand Peaks",
    "section": "Peak Detection",
    "text": "Peak Detection\nFinally, we use the forecasts in crossvaldation_df to detect the daily hourly demand peaks. For each day, we set the detected peaks as the highest forecasts. In this case, we want to predict one peak (npeaks); depending on your setting and goals, this parameter might change. For example, the number of peaks can correspond to how many hours a battery can be discharged to reduce demand.\n\nnpeaks = 1 # Number of peaks\n\nFor the ERCOT 4CP detection task we are interested in correctly predicting the highest monthly load. Next, we filter the day in September with the highest hourly demand and predict the peak.\n\ncrossvalidation_df = crossvalidation_df.reset_index()[['ds','y','MSTL']]\nmax_day = crossvalidation_df.iloc[crossvalidation_df['y'].argmax()].ds.day # Day with maximum load\ncv_df_day = crossvalidation_df.query('ds.dt.day == @max_day')\nmax_hour = cv_df_day['y'].argmax()\npeaks = cv_df_day['MSTL'].argsort().iloc[-npeaks:].values # Predicted peaks\n\nIn the following plot we see how the MSTL model is able to correctly detect the coincident peak for September 2022.\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(10, 5))\nplt.axvline(cv_df_day.iloc[max_hour]['ds'], color='black', label='True Peak')\nplt.scatter(cv_df_day.iloc[peaks]['ds'], cv_df_day.iloc[peaks]['MSTL'], color='green', label=f'Predicted Top-{npeaks}')\nplt.plot(cv_df_day['ds'], cv_df_day['y'], label='y', color='blue')\nplt.plot(cv_df_day['ds'], cv_df_day['MSTL'], label='Forecast', color='red')\nplt.xlabel('Time')\nplt.ylabel('Load (MW)')\nplt.grid()\nplt.legend()\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this example we only include September. However, MSTL can correctly predict the peaks for the 4 months of 2022. You can try this by increasing the nwindows parameter of cross_validation or filtering the Y_df dataset. The complete run for all months take only 10 minutes."
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#next-steps",
    "href": "docs/tutorials/electricitypeakforecasting.html#next-steps",
    "title": "Detect Demand Peaks",
    "section": "Next steps",
    "text": "Next steps\nStatsForecast and MSTL in particular are good benchmarking models for peak detection. However, it might be useful to explore further and newer forecasting algorithms. We have seen particularly good results with the N-HiTS, a deep-learning model from Nixtla‚Äôs NeuralForecast library.\nLearn how to predict ERCOT demand peaks with our deep-learning N-HiTS model and the NeuralForecast library in this tutorial."
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#references",
    "href": "docs/tutorials/electricitypeakforecasting.html#references",
    "title": "Detect Demand Peaks",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). ‚ÄúMSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns‚Äù.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). ‚ÄúN-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting‚Äù. Accepted at AAAI 2023."
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html",
    "href": "docs/tutorials/uncertaintyintervals.html",
    "title": "Probabilistic Forecasting",
    "section": "",
    "text": "Prerequisites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#introduction",
    "href": "docs/tutorials/uncertaintyintervals.html#introduction",
    "title": "Probabilistic Forecasting",
    "section": "Introduction",
    "text": "Introduction\nWhen we generate a forecast, we usually produce a single value known as the point forecast. This value, however, doesn‚Äôt tell us anything about the uncertainty associated with the forecast. To have a measure of this uncertainty, we need prediction intervals.\nA prediction interval is a range of values that the forecast can take with a given probability. Hence, a 95% prediction interval should contain a range of values that include the actual future value with probability 95%. Probabilistic forecasting aims to generate the full forecast distribution. Point forecasting, on the other hand, usually returns the mean or the median or said distribution. However, in real-world scenarios, it is better to forecast not only the most probable future outcome, but many alternative outcomes as well.\nStatsForecast has many models that can generate point forecasts. It also has probabilistic models than generate the same point forecasts and their prediction intervals. These models are stochastic data generating processes that can produce entire forecast distributions. By the end of this tutorial, you‚Äôll have a good understanding of the probabilistic models available in StatsForecast and will be able to use them to generate point forecasts and prediction intervals. Furthermore, you‚Äôll also learn how to generate plots with the historical data, the point forecasts, and the prediction intervals.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the terms are often confused, prediction intervals are not the same as confidence intervals.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn practice, most prediction intervals are too narrow since models do not account for all sources of uncertainty. A discussion about this can be found here.\n\n\nOutline:\n\nInstall libraries\nLoad and explore the data\nTrain models\nPlot prediction intervals\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#install-libraries",
    "href": "docs/tutorials/uncertaintyintervals.html#install-libraries",
    "title": "Probabilistic Forecasting",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#load-and-explore-the-data",
    "href": "docs/tutorials/uncertaintyintervals.html#load-and-explore-the-data",
    "title": "Probabilistic Forecasting",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we‚Äôll use the hourly dataset from the M4 Competition. We first need to download the data from a URL and then load it as a pandas dataframe. Notice that we‚Äôll load the train and the test data separately. We‚Äôll also rename the y column of the test data as y_test.\n\nimport pandas as pd \n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny_test\n\n\n\n\n0\nH1\n701\n619.0\n\n\n1\nH1\n702\n565.0\n\n\n2\nH1\n703\n532.0\n\n\n3\nH1\n704\n495.0\n\n\n4\nH1\n705\n481.0\n\n\n\n\n\n\n\nSince the goal of this notebook is to generate prediction intervals, we‚Äôll only use the first 8 series of the dataset to reduce the total computational time.\n\nn_series = 8 \nuids = train['unique_id'].unique()[:n_series] # select first n_series of the dataset\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\nWe can plot these series using the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: bool = True. Plots the time series randomly.\nmodels: List[str]. A list with the models we want to plot.\nlevel: List[float]. A list with the prediction intervals we want to plot.\nengine: str = plotly. It can also be matplotlib. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(train, test, plot_random = False)\n\n/Users/fedex/projects/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#train-models",
    "href": "docs/tutorials/uncertaintyintervals.html#train-models",
    "title": "Probabilistic Forecasting",
    "section": "Train models",
    "text": "Train models\nStatsForecast can train multiple models on different time series efficiently. Most of these models can generate a probabilistic forecast, which means that they can produce both point forecasts and prediction intervals.\nFor this example, we‚Äôll use AutoETS and the following baseline models:\n\nHistoricAverage\nNaive\nRandomWalkWithDrift\nSeasonalNaive\n\nTo use these models, we first need to import them from statsforecast.models and then we need to instantiate them. Given that we‚Äôre working with hourly data, we need to set seasonal_length=24 in the models that requiere this parameter.\n\nfrom statsforecast.models import (\n    AutoETS, \n    HistoricAverage, \n    Naive, \n    RandomWalkWithDrift, \n    SeasonalNaive\n)\n\n# Create a list of models and instantiation parameters \nmodels = [\n    AutoETS(season_length=24),\n    HistoricAverage(), \n    Naive(), \n    RandomWalkWithDrift(), \n    SeasonalNaive(season_length=24)\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\n\nfreq: A string indicating the frequency of the data. See pandas‚Äô available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df=train, \n    models=models, \n    freq='H', \n    n_jobs=-1\n)\n\nNow we‚Äôre ready to generate the point forecasts and the prediction intervals. To do this, we‚Äôll use the forecast method, which takes two arguments:\n\nh: An integer that represent the forecasting horizon. In this case, we‚Äôll forecast the next 48 hours.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nlevels = [80, 90, 95, 99] # confidence levels of the prediction intervals \n\nforecasts = sf.forecast(h=48, level=levels)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoETS\nAutoETS-lo-99\nAutoETS-lo-95\nAutoETS-lo-90\nAutoETS-lo-80\nAutoETS-hi-80\nAutoETS-hi-90\nAutoETS-hi-95\n...\nRWD-hi-99\nSeasonalNaive\nSeasonalNaive-lo-80\nSeasonalNaive-lo-90\nSeasonalNaive-lo-95\nSeasonalNaive-lo-99\nSeasonalNaive-hi-80\nSeasonalNaive-hi-90\nSeasonalNaive-hi-95\nSeasonalNaive-hi-99\n\n\n\n\n0\nH1\n701\n631.889587\n533.371826\n556.926819\n568.978882\n582.874084\n680.905090\n694.800354\n706.852356\n...\n789.416626\n691.0\n582.823792\n552.157349\n525.558777\n473.573395\n799.176208\n829.842651\n856.441223\n908.426575\n\n\n1\nH1\n702\n559.750854\n460.738586\n484.411835\n496.524353\n510.489288\n609.012329\n622.977295\n635.089844\n...\n833.254150\n618.0\n509.823822\n479.157379\n452.558807\n400.573395\n726.176208\n756.842651\n783.441223\n835.426575\n\n\n2\nH1\n703\n519.235474\n419.731232\n443.522095\n455.694794\n469.729156\n568.741821\n582.776123\n594.948853\n...\n866.990601\n563.0\n454.823822\n424.157379\n397.558807\n345.573395\n671.176208\n701.842651\n728.441223\n780.426575\n\n\n3\nH1\n704\n486.973358\n386.979523\n410.887451\n423.120056\n437.223480\n536.723267\n550.826660\n563.059265\n...\n895.510132\n529.0\n420.823822\n390.157379\n363.558807\n311.573395\n637.176208\n667.842651\n694.441223\n746.426575\n\n\n4\nH1\n705\n464.697357\n364.216339\n388.240753\n400.532959\n414.705078\n514.689636\n528.861755\n541.153992\n...\n920.702881\n504.0\n395.823822\n365.157379\n338.558807\n286.573395\n612.176208\n642.842651\n669.441223\n721.426575\n\n\n\n\n5 rows √ó 47 columns\n\n\n\nWe‚Äôll now merge the forecasts and their prediction intervals with the test set. This will allow us generate the plots of each probabilistic model.\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#plot-prediction-intervals",
    "href": "docs/tutorials/uncertaintyintervals.html#plot-prediction-intervals",
    "title": "Probabilistic Forecasting",
    "section": "Plot prediction intervals",
    "text": "Plot prediction intervals\nTo plot the point and the prediction intervals, we‚Äôll use the statsforecast.plot method again. Notice that now we also need to specify the model and the levels that we want to plot.\n\nAutoETS\n\nsf.plot(train, test, plot_random = False, models=['AutoETS'], level=levels)\n\n\n                                                \n\n\n\n\nHistoric Average\n\nsf.plot(train, test, plot_random = False, models=['HistoricAverage'], level=levels)\n\n\n                                                \n\n\n\n\nNaive\n\nsf.plot(train, test, plot_random = False, models=['Naive'], level=levels)\n\n\n                                                \n\n\n\n\nRandom Walk with Drift\n\nsf.plot(train, test, plot_random = False, models=['RWD'], level=levels)\n\n\n                                                \n\n\n\n\nSeasonal Naive\n\nsf.plot(train, test, plot_random = False, models=['SeasonalNaive'], level=levels)\n\n\n                                                \n\n\nFrom these plots, we can conclude that the uncertainty around each forecast varies according to the model that is being used. For the same time series, one model can predict a wider range of possible future values than others."
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#references",
    "href": "docs/tutorials/uncertaintyintervals.html#references",
    "title": "Probabilistic Forecasting",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, The Statistical Forecasting Perspective‚Äù."
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html",
    "href": "docs/tutorials/anomalydetection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#introduction",
    "href": "docs/tutorials/anomalydetection.html#introduction",
    "title": "Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection is a crucial task in time series forecasting. It involves identifying unusual observations that don‚Äôt follow the expected dataset patterns. Anomalies, also known as outliers, can be caused by a variety of factors, such as errors in the data collection process, sudden changes in the underlying patterns of the data, or unexpected events. They can pose problems for many forecasting models since they can distort trends, seasonal patterns, or autocorrelation estimates. As a result, anomalies can have a significant impact on the accuracy of the forecasts, and for this reason, it is essential to be able to identify them. Furthermore, anomaly detection has many applications across different industries, such as detecting fraud in financial data, monitoring the performance of online services, or identifying usual patterns in energy usage.\nBy the end of this tutorial, you‚Äôll have a good understanding of how to detect anomalies in time series data using StatsForecast‚Äôs probabilistic models.\nOutline:\n\nInstall libraries\nLoad and explore data\nTrain model\nRecover insample forecasts and identify anomalies\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce an anomaly has been identified, we must decide what to do with it. For example, we could remove it or replace it with another value. The correct course of action is context-dependent and beyond this notebook‚Äôs scope. Removing an anomaly will likely improve the accuracy of the forecast, but it can also underestimate the amount of randomness in the data.\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#install-libraries",
    "href": "docs/tutorials/anomalydetection.html#install-libraries",
    "title": "Anomaly Detection",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#load-and-explore-the-data",
    "href": "docs/tutorials/anomalydetection.html#load-and-explore-the-data",
    "title": "Anomaly Detection",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we‚Äôll use the hourly dataset of the M4 Competition. We‚Äôll first import the data from datasetsforecast, which you can install using pip install datasetsforecast\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.m4 import M4\n\nThe function to load the data is M4.load. It requieres the following two arguments:\n\ndirectory: (str) The directory where the data will be downloaded.\ngroup: (str). The group name, which can be Yearly, Quarterly, Monthly, Weekly, Daily or Hourly.\n\nThis function returns multiple outputs, but only the first one with the target series is needed.\n\ndf_total, *_ = M4.load('./data', 'Hourly')\ndf_total.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, df and y.\n\nunique_id: (string, int or category) A unique identifier for the series.\nds: (datestamp or int) A datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS or an integer indexing time.\ny: (numeric) The measurement we wish to forecast.\n\nIn this case, the unique_id and y columns already have the requiered format, but we need to change the data type of the ds column.\n\ndf_total['ds'] = df_total['ds'].astype(int)\n\nFrom this dataset, we‚Äôll select the first 8 time series to reduce the total execution time. You can select any number you want by changing the value of n_series.\n\nn_series = 8 \nuids = df_total['unique_id'].unique()[:n_series]\ndf = df_total.query('unique_id in @uids')\n\nWe can plot these series using the plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nunique_ids: (list[str]) A list with the ids of the time series we want to plot.\nplot_random: (bool = True) Plots the time series randomly.\nplot_anomalies: (bool = False) Plots anomalies for each prediction interval.\nengine: (str = plotly). The library used to generate the plots. It can also be matplotlib for static plots.\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(df, plot_random = False)"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#train-model",
    "href": "docs/tutorials/anomalydetection.html#train-model",
    "title": "Anomaly Detection",
    "section": "Train model",
    "text": "Train model\nTo generate the forecast, we‚Äôll use the MSTL model, which is well-suited for low-frequency data like the one used here. We first need to import it from statsforecast.models and then we need to instantiate it. Since we‚Äôre using hourly data, we have two seasonal periods: one every 24 hours (hourly) and one every 24*7 hours (daily). Hence, we need to set season_length = [24, 24*7].\n\nfrom statsforecast.models import MSTL\n\n# Create a list of models and instantiation parameters \nmodels = [MSTL(season_length = [24, 24*7])]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\nfreq: A string indicating the frequency of the data. See pandas‚Äô available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df = df, \n    models = models, \n    freq = 'H', \n    n_jobs = -1\n)\n\nWe‚Äôll now predict the next 48 hours. To do this, we‚Äôll use the forecast method, which requieres the following arguments:\n\nh: (int) The forecasting horizon.\nlevel: (list[float]) The confidence levels of the prediction intervals\nfitted: (bool = False) Returns insample predictions.\n\nIt is important that we select a level and set fitted = True since we‚Äôll need the insample forecasts and their prediction intervals to detect the anomalies.\n\nhorizon = 48\nlevels = [99] \n\nfcst = sf.forecast(h = 48, level = levels, fitted = True)\nfcst = fcst.reset_index()\nfcst.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nMSTL\nMSTL-lo-99\nMSTL-hi-99\n\n\n\n\n0\nH1\n749\n615.943970\n597.662170\n634.225708\n\n\n1\nH1\n750\n559.297791\n531.316650\n587.278931\n\n\n2\nH1\n751\n515.693542\n479.151337\n552.235718\n\n\n3\nH1\n752\n480.719269\n436.241547\n525.197021\n\n\n4\nH1\n753\n467.146484\n415.199738\n519.093262\n\n\n\n\n\n\n\nWe can plot the forecasts using the plot method from before.\n\nStatsForecast.plot(df, fcst, plot_random = False)"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#recover-insample-forecasts-and-identify-anomalies",
    "href": "docs/tutorials/anomalydetection.html#recover-insample-forecasts-and-identify-anomalies",
    "title": "Anomaly Detection",
    "section": "Recover insample forecasts and identify anomalies",
    "text": "Recover insample forecasts and identify anomalies\nIn this example, an anomaly will be any observation outside the prediction interval of the insample forecasts for a given confidence level (here we selected 99%). Hence, we first need to recover the insample forecasts using the forecast_fitted_values method.\n\ninsample_forecasts = sf.forecast_fitted_values().reset_index()\ninsample_forecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nMSTL\nMSTL-lo-99\nMSTL-hi-99\n\n\n\n\n0\nH1\n1\n605.0\n604.924500\n588.010376\n621.838623\n\n\n1\nH1\n2\n586.0\n585.221802\n568.307678\n602.135925\n\n\n2\nH1\n3\n586.0\n589.740723\n572.826599\n606.654846\n\n\n3\nH1\n4\n559.0\n557.778076\n540.863953\n574.692200\n\n\n4\nH1\n5\n511.0\n506.747009\n489.832886\n523.661133\n\n\n\n\n\n\n\nWe can now find all the observations above or below the 99% prediction interval for the insample forecasts.\n\nanomalies = insample_forecasts.loc[(insample_forecasts['y'] &gt;= insample_forecasts['MSTL-hi-99']) | (insample_forecasts['y'] &lt;= insample_forecasts['MSTL-lo-99'])]\nanomalies.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nMSTL\nMSTL-lo-99\nMSTL-hi-99\n\n\n\n\n168\nH1\n169\n813.0\n779.849792\n762.935669\n796.763916\n\n\n279\nH1\n280\n692.0\n672.638123\n655.723999\n689.552246\n\n\n289\nH1\n290\n770.0\n792.015442\n775.101318\n808.929565\n\n\n308\nH1\n309\n844.0\n867.809387\n850.895203\n884.723511\n\n\n336\nH1\n337\n853.0\n822.427002\n805.512878\n839.341187\n\n\n\n\n\n\n\nWe can plot the anomalies by adding the plot_anomalies = True argument to the plot method.\n\nStatsForecast.plot(insample_forecasts, plot_random = False, plot_anomalies = True)\n\n\n                                                \n\n\nIf we want to take a closer look, we can use the unique_ids argument to select one particular time series, for example, H10.\n\nStatsForecast.plot(insample_forecasts, unique_ids = ['H10'], plot_anomalies = True)\n\n\n                                                \n\n\nHere we identified the anomalies in the data using the MSTL model, but any probabilistic model from StatsForecast can be used. We also selected the 99% prediction interval of the insample forecasts, but other confidence levels can be used as well."
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html",
    "href": "docs/tutorials/electricityloadforecasting.html",
    "title": "Electricity Load Forecast",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#introduction",
    "href": "docs/tutorials/electricityloadforecasting.html#introduction",
    "title": "Electricity Load Forecast",
    "section": "Introduction",
    "text": "Introduction\nSome time series are generated from very low frequency data. These data generally exhibit multiple seasonalities. For example, hourly data may exhibit repeated patterns every hour (every 24 observations) or every day (every 24 * 7, hours per day, observations). This is the case for electricity load. Electricity load may vary hourly, e.g., during the evenings electricity consumption may be expected to increase. But also, the electricity load varies by week. Perhaps on weekends there is an increase in electrical activity.\nIn this example we will show how to model the two seasonalities of the time series to generate accurate forecasts in a short time. We will use hourly PJM electricity load data. The original data can be found here."
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#libraries",
    "href": "docs/tutorials/electricityloadforecasting.html#libraries",
    "title": "Electricity Load Forecast",
    "section": "Libraries",
    "text": "Libraries\nIn this example we will use the following libraries:\n\nStatsForecast. Lightning ‚ö°Ô∏è fast forecasting with statistical and econometric models. Includes the MSTL model for multiple seasonalities.\nDatasetsForecast. Used to evaluate the performance of the forecasts.\nProphet. Benchmark model developed by Facebook.\nNeuralProphet. Deep Learning version of Prophet. Used as benchark.\n\n\n!pip install statsforecast\n!pip install datasetsforecast\n!pip install prophet\n!pip install \"neuralprophet[live]\""
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "href": "docs/tutorials/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "title": "Electricity Load Forecast",
    "section": "Forecast using Multiple Seasonalities",
    "text": "Forecast using Multiple Seasonalities\n\nElectricity Load Data\nAccording to the dataset‚Äôs page,\n\nPJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia. The hourly power consumption data comes from PJM‚Äôs website and are in megawatts (MW).\n\nLet‚Äôs take a look to the data.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\npd.plotting.register_matplotlib_converters()\nplt.rc(\"figure\", figsize=(10, 8))\nplt.rc(\"font\", size=10)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/master/data/PJM_Load_hourly.csv')\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf['ds'] = pd.to_datetime(df['ds'])\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\ndf.tail()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n32891\nPJM_Load_hourly\n2001-12-31 20:00:00\n36392.0\n\n\n32892\nPJM_Load_hourly\n2001-12-31 21:00:00\n35082.0\n\n\n32893\nPJM_Load_hourly\n2001-12-31 22:00:00\n33890.0\n\n\n32894\nPJM_Load_hourly\n2001-12-31 23:00:00\n32590.0\n\n\n32895\nPJM_Load_hourly\n2002-01-01 00:00:00\n31569.0\n\n\n\n\n\n\n\n\ndf.plot(x='ds', y='y')\n\n&lt;Axes: xlabel='ds'&gt;\n\n\n\n\n\nWe clearly observe that the time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods to display them in production.\n\n\nMSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\nStatsForecast contains a fast implementation of the MSTL model. Also, the decomposition of the time series can be calculated.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA, SeasonalNaive\nfrom statsforecast.utils import AirPassengers as ap\n\nFirst we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities that the MSTL model receives. We must also specify the manner in which the trend will be forecasted. In this case we will use the AutoARIMA model.\n\nmstl = MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA() # model used to forecast trend\n)\n\nOnce the model is instantiated, we have to instantiate the StatsForecast class to create forecasts.\n\nsf = StatsForecast(\n    models=[mstl], # model used to fit each time series \n    freq='H', # frequency of the data\n)\n\n\nFit the model\nAfer that, we just have to use the fit method to fit each model to each time series.\n\nsf = sf.fit(df=df)\n\n\n\nDecompose the time series in multiple seasonalities\nOnce the model is fitted, we can access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n\n\n\n\n\ndata\ntrend\nseasonal24\nseasonal168\nremainder\n\n\n\n\n0\n22259.0\n26183.898892\n-5215.124554\n609.000432\n681.225229\n\n\n1\n21244.0\n26181.599305\n-6255.673234\n603.823918\n714.250011\n\n\n2\n20651.0\n26179.294886\n-6905.329895\n636.820423\n740.214587\n\n\n3\n20421.0\n26176.985472\n-7073.420118\n615.825999\n701.608647\n\n\n4\n20713.0\n26174.670877\n-7062.395760\n991.521912\n609.202971\n\n\n...\n...\n...\n...\n...\n...\n\n\n32891\n36392.0\n33123.552727\n4387.149171\n-488.177882\n-630.524015\n\n\n32892\n35082.0\n33148.242575\n3479.852929\n-682.928737\n-863.166767\n\n\n32893\n33890.0\n33172.926165\n2307.808829\n-650.566775\n-940.168219\n\n\n32894\n32590.0\n33197.603322\n748.587723\n-555.177849\n-801.013195\n\n\n32895\n31569.0\n33222.273902\n-967.124123\n-265.895357\n-420.254422\n\n\n\n\n32896 rows √ó 5 columns\n\n\n\nLet‚Äôs look graphically at the different components of the time series.\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe observe that there is a clear trend towards the high (orange line). This component would be predicted with the AutoARIMA model. We can also observe that every 24 hours and every 24 * 7 hours there is a very well defined pattern. These two components will be forecast separately using a SeasonalNaive model.\n\n\nProduce forecasts\nTo generate forecasts we only have to use the predict method specifying the forecast horizon (h). In addition, to calculate prediction intervals associated to the forecasts, we can include the parameter level that receives a list of levels of the prediction intervals we want to build. In this case we will only calculate the 90% forecast interval (level=[90]).\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2002-01-01 01:00:00\n29956.744141\n29585.187500\n30328.298828\n\n\nPJM_Load_hourly\n2002-01-01 02:00:00\n29057.691406\n28407.498047\n29707.884766\n\n\nPJM_Load_hourly\n2002-01-01 03:00:00\n28654.699219\n27767.101562\n29542.298828\n\n\nPJM_Load_hourly\n2002-01-01 04:00:00\n28499.009766\n27407.640625\n29590.378906\n\n\nPJM_Load_hourly\n2002-01-01 05:00:00\n28821.716797\n27552.236328\n30091.197266\n\n\n\n\n\n\n\nLet‚Äôs look at our forecasts graphically.\n\n_, ax = plt.subplots(1, 1, figsize = (20, 7))\ndf_plot = pd.concat([df, forecasts]).set_index('ds').tail(24 * 7)\ndf_plot[['y', 'MSTL']].plot(ax=ax, linewidth=2)\nax.fill_between(df_plot.index, \n                df_plot['MSTL-lo-90'], \n                df_plot['MSTL-hi-90'],\n                alpha=.35,\n                color='orange',\n                label='MSTL-level-90')\nax.set_title('PJM Load Hourly', fontsize=22)\nax.set_ylabel('Electricity Load', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\nIn the next section we will plot different models so it is convenient to reuse the previous code with the following function.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(24 * 7)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2)\n    colors = ['orange', 'green', 'red']\n    for model, color in zip(models, colors):\n        ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-90'], \n                        df_plot[f'{model}-hi-90'],\n                        alpha=.35,\n                        color=color,\n                        label=f'{model}-level-90')\n    ax.set_title('PJM Load Hourly', fontsize=22)\n    ax.set_ylabel('Electricity Load', fontsize=20)\n    ax.set_xlabel('Timestamp [t]', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid()\n\n\n\n\nPerformance of the MSTL model\n\nSplit Train/Test sets\nTo validate the accuracy of the MSTL model, we will show its performance on unseen data. We will use a classical time series technique that consists of dividing the data into a training set and a test set. We will leave the last 24 observations (the last day) as the test set. So the model will train on 32,872 observations.\n\ndf_test = df.tail(24)\ndf_train = df.drop(df_test.index)\n\n\n\nMSTL model\nIn addition to the MSTL model, we will include the SeasonalNaive model as a benchmark to validate the added value of the MSTL model. Including StatsForecast models is as simple as adding them to the list of models to be fitted.\n\nsf = StatsForecast(\n    models=[mstl, SeasonalNaive(season_length=24)], # add SeasonalNaive model to the list\n    freq='H'\n)\n\nTo measure the fitting time we will use the time module.\n\nfrom time import time\n\nTo retrieve the forecasts of the test set we only have to do fit and predict as before.\n\ninit = time()\nsf = sf.fit(df=df_train)\nforecasts_test = sf.predict(h=len(df_test), level=[90])\nend = time()\nforecasts_test.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2001-12-31 01:00:00\n28345.212891\n27973.572266\n28716.853516\n28326.0\n23468.693359\n33183.304688\n\n\nPJM_Load_hourly\n2001-12-31 02:00:00\n27567.455078\n26917.085938\n28217.824219\n27362.0\n22504.693359\n32219.306641\n\n\nPJM_Load_hourly\n2001-12-31 03:00:00\n27260.001953\n26372.138672\n28147.865234\n27108.0\n22250.693359\n31965.306641\n\n\nPJM_Load_hourly\n2001-12-31 04:00:00\n27328.125000\n26236.410156\n28419.839844\n26865.0\n22007.693359\n31722.306641\n\n\nPJM_Load_hourly\n2001-12-31 05:00:00\n27640.673828\n26370.773438\n28910.572266\n26808.0\n21950.693359\n31665.306641\n\n\n\n\n\n\n\n\ntime_mstl = (end - init) / 60\nprint(f'MSTL Time: {time_mstl:.2f} minutes')\n\nMSTL Time: 0.22 minutes\n\n\nThen we were able to generate forecasts for the next 24 hours. Now let‚Äôs look at the graphical comparison of the forecasts with the actual values.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n\nLet‚Äôs look at those produced only by MSTL.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL'])\n\n\n\n\nWe note that MSTL produces very accurate forecasts that follow the behavior of the time series. Now let us calculate numerically the accuracy of the model. We will use the following metrics: MAE, MAPE, MASE, RMSE, SMAPE.\n\nfrom datasetsforecast.losses import (\n    mae, mape, mase, rmse, smape\n)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, models):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    for model in models:\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                 y_true[model].values, \n                                                 y_hist['y'].values, seasonality=24)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nmase\nmae\nmape\nrmse\nsmape\n\n\n\n\nMSTL\n0.341926\n709.932048\n2.182804\n892.888012\n2.162832\n\n\nSeasonalNaive\n0.894653\n1857.541667\n5.648190\n2201.384101\n5.868604\n\n\n\n\n\n\n\nWe observe that MSTL has an improvement of about 60% over the SeasonalNaive method in the test set measured in MASE.\n\n\nComparison with Prophet\nOne of the most widely used models for time series forecasting is Prophet. This model is known for its ability to model different seasonalities (weekly, daily yearly). We will use this model as a benchmark to see if the MSTL adds value for this time series.\n\nfrom prophet import Prophet\n\n# create prophet model\nprophet = Prophet(interval_width=0.9)\ninit = time()\nprophet.fit(df_train)\n# produce forecasts\nfuture = prophet.make_future_dataframe(periods=len(df_test), freq='H', include_history=False)\nforecast_prophet = prophet.predict(future)\nend = time()\n# data wrangling\nforecast_prophet = forecast_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\nforecast_prophet.columns = ['ds', 'Prophet', 'Prophet-lo-90', 'Prophet-hi-90']\nforecast_prophet.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_prophet.head()\n\n23:41:40 - cmdstanpy - INFO - Chain [1] start processing\n23:41:56 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nunique_id\nds\nProphet\nProphet-lo-90\nProphet-hi-90\n\n\n\n\n0\nPJM_Load_hourly\n2001-12-31 01:00:00\n25317.658386\n20757.919539\n30313.561582\n\n\n1\nPJM_Load_hourly\n2001-12-31 02:00:00\n24024.188077\n19304.093939\n28667.495805\n\n\n2\nPJM_Load_hourly\n2001-12-31 03:00:00\n23348.306824\n18608.982825\n28497.334752\n\n\n3\nPJM_Load_hourly\n2001-12-31 04:00:00\n23356.150113\n18721.142270\n28136.888630\n\n\n4\nPJM_Load_hourly\n2001-12-31 05:00:00\n24130.861217\n19896.188455\n28970.202276\n\n\n\n\n\n\n\n\ntime_prophet = (end - init) / 60\nprint(f'Prophet Time: {time_prophet:.2f} minutes')\n\nProphet Time: 0.30 minutes\n\n\n\ntimes = pd.DataFrame({'model': ['MSTL', 'Prophet'], 'time (mins)': [time_mstl, time_prophet]})\ntimes\n\n\n\n\n\n\n\n\nmodel\ntime (mins)\n\n\n\n\n0\nMSTL\n0.217266\n\n\n1\nProphet\n0.301172\n\n\n\n\n\n\n\nWe observe that the time required for Prophet to perform the fit and predict pipeline is greater than MSTL. Let‚Äôs look at the forecasts produced by Prophet.\n\nforecasts_test = forecasts_test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive', 'Prophet'])\n\n\n\n\nWe note that Prophet is able to capture the overall behavior of the time series. However, in some cases it produces forecasts well below the actual value. It also does not correctly adjust the valleys.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'Prophet', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nmase\nmae\nmape\nrmse\nsmape\n\n\n\n\nMSTL\n0.341926\n709.932048\n2.182804\n892.888012\n2.162832\n\n\nProphet\n1.094768\n2273.036373\n7.343292\n2709.400341\n7.688665\n\n\nSeasonalNaive\n0.894653\n1857.541667\n5.648190\n2201.384101\n5.868604\n\n\n\n\n\n\n\nIn terms of accuracy, Prophet is not able to produce better forecasts than the SeasonalNaive model, however, the MSTL model improves Prophet‚Äôs forecasts by 69% (MASE).\n\n\nComparison with NeuralProphet\nNeuralProphet is the version of Prophet using deep learning. This model is also capable of handling different seasonalities so we will also use it as a benchmark.\n\nfrom neuralprophet import NeuralProphet\n\nneuralprophet = NeuralProphet(quantiles=[0.05, 0.95])\ninit = time()\nneuralprophet.fit(df_train.drop(columns='unique_id'))\nfuture = neuralprophet.make_future_dataframe(df=df_train.drop(columns='unique_id'), periods=len(df_test))\nforecast_np = neuralprophet.predict(future)\nend = time()\nforecast_np = forecast_np[['ds', 'yhat1', 'yhat1 5.0%', 'yhat1 95.0%']]\nforecast_np.columns = ['ds', 'NeuralProphet', 'NeuralProphet-lo-90', 'NeuralProphet-hi-90']\nforecast_np.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_np.head()\n\nWARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\nINFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as H\nINFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\nINFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\nINFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 76\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 95.833% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 95.833% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nNeuralProphet\nNeuralProphet-lo-90\nNeuralProphet-hi-90\n\n\n\n\n0\nPJM_Load_hourly\n2001-12-31 01:00:00\n25019.892578\n22296.675781\n27408.724609\n\n\n1\nPJM_Load_hourly\n2001-12-31 02:00:00\n24128.816406\n21439.851562\n26551.615234\n\n\n2\nPJM_Load_hourly\n2001-12-31 03:00:00\n23736.679688\n20961.978516\n26289.349609\n\n\n3\nPJM_Load_hourly\n2001-12-31 04:00:00\n23476.744141\n20731.619141\n26050.443359\n\n\n4\nPJM_Load_hourly\n2001-12-31 05:00:00\n23899.162109\n21217.503906\n26449.603516\n\n\n\n\n\n\n\n\ntime_np = (end - init) / 60\nprint(f'Prophet Time: {time_np:.2f} minutes')\n\nProphet Time: 2.95 minutes\n\n\n\ntimes = times.append({'model': 'NeuralProphet', 'time (mins)': time_np}, ignore_index=True)\ntimes\n\n\n\n\n\n\n\n\nmodel\ntime (mins)\n\n\n\n\n0\nMSTL\n0.217266\n\n\n1\nProphet\n0.301172\n\n\n2\nNeuralProphet\n2.946358\n\n\n\n\n\n\n\nWe observe that NeuralProphet requires a longer processing time than Prophet and MSTL.\n\nforecasts_test = forecasts_test.merge(forecast_np, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'NeuralProphet', 'Prophet'])\n\n\n\n\nThe forecasts graph shows that NeuralProphet generates very similar results to Prophet, as expected.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'NeuralProphet', 'Prophet', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nmase\nmae\nmape\nrmse\nsmape\n\n\n\n\nMSTL\n0.341926\n709.932048\n2.182804\n892.888012\n2.162832\n\n\nNeuralProphet\n1.084915\n2252.578613\n7.280202\n2671.145730\n7.615492\n\n\nProphet\n1.094768\n2273.036373\n7.343292\n2709.400341\n7.688665\n\n\nSeasonalNaive\n0.894653\n1857.541667\n5.648190\n2201.384101\n5.868604\n\n\n\n\n\n\n\nWith respect to numerical evaluation, NeuralProphet improves the results of Prophet, as expected, however, MSTL improves over NeuralProphet‚Äôs foreacasts by 68% (MASE).\n\n\n\n\n\n\nImportant\n\n\n\nThe performance of NeuralProphet can be improved using hyperparameter optimization, which can increase the fitting time significantly. In this example we show its performance with the default version."
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#conclusion",
    "href": "docs/tutorials/electricityloadforecasting.html#conclusion",
    "title": "Electricity Load Forecast",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we introduced MSTL, a model originally developed by Kasun Bandara, Rob Hyndman and Christoph Bergmeir capable of handling time series with multiple seasonalities. We also showed that for the PJM electricity load time series offers better performance in time and accuracy than the Prophet and NeuralProphet models."
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#references",
    "href": "docs/tutorials/electricityloadforecasting.html#references",
    "title": "Electricity Load Forecast",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). ‚ÄúMSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns‚Äù."
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html",
    "href": "docs/tutorials/garch_tutorial.html",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#introduction",
    "href": "docs/tutorials/garch_tutorial.html#introduction",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Introduction",
    "text": "Introduction\nThe Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model is used for time series that exhibit non-constant volatility over time. Here volatility refers to the conditional standard deviation. The GARCH(p,q) model is given by\n\\[\\begin{equation}\ny_t = v_t \\sigma_t\n\\end{equation}\\]\nwhere \\(v_t\\) is independent and identically distributed with zero mean and unit variance, and \\(\\sigma_t\\) evolves according to\n\\[\\begin{equation}\n\\sigma_t^2 = w + \\sum_{i=1}^p \\alpha_i y^2_{t-i} + \\sum_{j=1}^q \\beta_j \\sigma_{t-j}^2\n\\end{equation}\\]\nThe coefficients in the equation above must satisfy the following conditions:\n\n\\(w&gt;0\\), \\(\\alpha_i \\geq 0\\) for all \\(i\\), and \\(\\beta_j \\geq 0\\) for all \\(j\\)\n\\(\\sum_{k=1}^{max(p,q)} \\alpha_k + \\beta_k &lt; 1\\). Here it is assumed that \\(\\alpha_i=0\\) for \\(i&gt;p\\) and \\(\\beta_j=0\\) for \\(j&gt;q\\).\n\nA particular case of the GARCH model is the ARCH model, in which \\(q=0\\). Both models are commonly used in finance to model the volatility of stock prices, exchange rates, interest rates, and other financial instruments. They‚Äôre also used in risk management to estimate the probability of large variations in the price of financial assets.\nBy the end of this tutorial, you‚Äôll have a good understanding of how to implement a GARCH or an ARCH model in StatsForecast and how they can be used to analyze and predict financial time series data.\nOutline:\n\nInstall libraries\nLoad and explore the data\nTrain models\nPerform time series cross-validation\nEvaluate results\nForecast volatility\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#install-libraries",
    "href": "docs/tutorials/garch_tutorial.html#install-libraries",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#load-and-explore-the-data",
    "href": "docs/tutorials/garch_tutorial.html#load-and-explore-the-data",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nIn this tutorial, we‚Äôll use the last 5 years of prices from the S&P 500 and several publicly traded companies. The data can be downloaded from Yahoo! Finance using yfinance. To install it, use pip install yfinance.\n\npip install yfinance\n\nWe‚Äôll also need pandas to deal with the dataframes.\n\nimport yfinance as yf\nimport pandas as pd \n\ntickers = ['SPY', 'MSFT', 'AAPL', 'GOOG', 'AMZN', 'TSLA', 'NVDA', 'META', 'NKE', 'NFLX'] \ndf = yf.download(tickers, start = '2018-01-01', end = '2022-12-31', interval='1mo') # use monthly prices\ndf.head()\n\n[*********************100%***********************]  10 of 10 completed\n\n\n\n\n\n\n\n\n\nAdj Close\n...\nVolume\n\n\n\nAAPL\nAMZN\nGOOG\nMETA\nMSFT\nNFLX\nNKE\nNVDA\nSPY\nTSLA\n...\nAAPL\nAMZN\nGOOG\nMETA\nMSFT\nNFLX\nNKE\nNVDA\nSPY\nTSLA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n39.741604\n72.544502\n58.497002\n186.889999\n89.248772\n270.299988\n64.929787\n60.830006\n258.821686\n23.620667\n...\n2638717600\n1927424000\n574768000\n495655700\n574258400\n238377600\n157812200\n1145621600\n1985506700\n1864072500\n\n\n2018-02-01\n42.279007\n75.622498\n55.236500\n178.320007\n88.083969\n291.380005\n63.797192\n59.889591\n249.410812\n22.870667\n...\n3711577200\n2755680000\n847640000\n516251600\n725663300\n184585800\n160317000\n1491552800\n2923722000\n1637850000\n\n\n2018-03-01\n39.987053\n72.366997\n51.589500\n159.789993\n86.138298\n295.350006\n63.235649\n57.348976\n241.606750\n17.742001\n...\n2854910800\n2608002000\n907066000\n996201700\n750754800\n263449400\n174066700\n1411844000\n2323561800\n2359027500\n\n\n2018-04-01\n39.386456\n78.306503\n50.866501\n172.000000\n88.261810\n312.459991\n65.288467\n55.692314\n243.828018\n19.593332\n...\n2664617200\n2598392000\n834318000\n750072700\n668130700\n262006000\n158981900\n1114400800\n1998466500\n2854662000\n\n\n2018-05-01\n44.536777\n81.481003\n54.249500\n191.779999\n93.282692\n351.600006\n68.543846\n62.450180\n249.755264\n18.982000\n...\n2483905200\n1432310000\n636988000\n401144100\n509417900\n142050800\n129566300\n1197824000\n1606397200\n2333671500\n\n\n\n\n5 rows √ó 60 columns\n\n\n\nThe data downloaded includes different prices. We‚Äôll use the adjusted closing price, which is the closing price after accounting for any corporate actions like stock splits or dividend distributions. It is also the price that is used to examine historical returns.\nNotice that the dataframe that yfinance returns has a MultiIndex, so we need to select both the adjusted price and the tickers.\n\ndf = df.loc[:, (['Adj Close'], tickers)]\ndf.columns = df.columns.droplevel() # drop MultiIndex\ndf = df.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\nDate\nSPY\nMSFT\nAAPL\nGOOG\nAMZN\nTSLA\nNVDA\nMETA\nNKE\nNFLX\n\n\n\n\n0\n2018-01-01\n258.821686\n89.248772\n39.741604\n58.497002\n72.544502\n23.620667\n60.830006\n186.889999\n64.929787\n270.299988\n\n\n1\n2018-02-01\n249.410812\n88.083969\n42.279007\n55.236500\n75.622498\n22.870667\n59.889591\n178.320007\n63.797192\n291.380005\n\n\n2\n2018-03-01\n241.606750\n86.138298\n39.987053\n51.589500\n72.366997\n17.742001\n57.348976\n159.789993\n63.235649\n295.350006\n\n\n3\n2018-04-01\n243.828018\n88.261810\n39.386456\n50.866501\n78.306503\n19.593332\n55.692314\n172.000000\n65.288467\n312.459991\n\n\n4\n2018-05-01\n249.755264\n93.282692\n44.536777\n54.249500\n81.481003\n18.982000\n62.450180\n191.779999\n68.543846\n351.600006\n\n\n\n\n\n\n\nThe input to StatsForecast is a dataframe in long format with three columns: unique_id, ds and y:\n\nunique_id: (string, int or category) A unique identifier for the series.\nds: (datestamp or int) A datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS or an integer indexing time.\ny: (numeric) The measurement we wish to forecast.\n\nHence, we need to reshape the data. We‚Äôll do this by creating a new dataframe called price.\n\nprices = df.melt(id_vars = 'Date')\nprices = prices.rename(columns={'Date': 'ds', 'variable': 'unique_id', 'value': 'y'})\nprices = prices[['unique_id', 'ds', 'y']]\nprices\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nSPY\n2018-01-01\n258.821686\n\n\n1\nSPY\n2018-02-01\n249.410812\n\n\n2\nSPY\n2018-03-01\n241.606750\n\n\n3\nSPY\n2018-04-01\n243.828018\n\n\n4\nSPY\n2018-05-01\n249.755264\n\n\n...\n...\n...\n...\n\n\n595\nNFLX\n2022-08-01\n223.559998\n\n\n596\nNFLX\n2022-09-01\n235.440002\n\n\n597\nNFLX\n2022-10-01\n291.880005\n\n\n598\nNFLX\n2022-11-01\n305.529999\n\n\n599\nNFLX\n2022-12-01\n294.880005\n\n\n\n\n600 rows √ó 3 columns\n\n\n\nWe can plot this series using the plot method of the StatsForecast class.\n\nfrom statsforecast import StatsForecast\n\n/home/ubuntu/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\nStatsForecast.plot(prices)\n\n\n                                                \n\n\nWith the prices, we can compute the logarithmic returns of the S&P 500 and the publicly traded companies. This is the variable we‚Äôre interested in since it‚Äôs likely to work well with the GARCH framework. The logarithmic return is given by\n\\(return_t = log \\big( \\frac{price_t}{price_{t-1}} \\big)\\)\nWe‚Äôll compute the returns on the price dataframe and then we‚Äôll create a return dataframe with StatsForecast‚Äôs format. To do this, we‚Äôll need numpy.\n\nimport numpy as np \nprices['rt'] = prices['y'].div(prices.groupby('unique_id')['y'].shift(1))\nprices['rt'] = np.log(prices['rt'])\n\nreturns = prices[['unique_id', 'ds', 'rt']]\nreturns = returns.rename(columns={'rt':'y'})\nreturns\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nSPY\n2018-01-01\nNaN\n\n\n1\nSPY\n2018-02-01\n-0.037038\n\n\n2\nSPY\n2018-03-01\n-0.031790\n\n\n3\nSPY\n2018-04-01\n0.009152\n\n\n4\nSPY\n2018-05-01\n0.024018\n\n\n...\n...\n...\n...\n\n\n595\nNFLX\n2022-08-01\n-0.005976\n\n\n596\nNFLX\n2022-09-01\n0.051776\n\n\n597\nNFLX\n2022-10-01\n0.214887\n\n\n598\nNFLX\n2022-11-01\n0.045705\n\n\n599\nNFLX\n2022-12-01\n-0.035479\n\n\n\n\n600 rows √ó 3 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the order of the data is very small (say \\(&lt;1e-5\\)), scipy.optimize.minimize might not terminate successfully. In this case, rescale the data and then generate the GARCH or ARCH model.\n\n\n\nStatsForecast.plot(returns)\n\n\n                                                \n\n\nFrom this plot, we can see that the returns seem suited for the GARCH framework, since large shocks tend to be followed by other large shocks. This doesn‚Äôt mean that after every large shock we should expect another one; merely that the probability of a large variance is greater than the probability of a small one."
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#train-models",
    "href": "docs/tutorials/garch_tutorial.html#train-models",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Train models",
    "text": "Train models\nWe first need to import the GARCH and the ARCH models from statsforecast.models, and then we need to fit them by instantiating a new StatsForecast object. Notice that we‚Äôll be using different values of \\(p\\) and \\(q\\). In the next section, we‚Äôll determine which ones produce the most accurate model using cross-validation. We‚Äôll also import the Naive model since we‚Äôll use it as a baseline.\n\nfrom statsforecast.models import (\n    GARCH, \n    ARCH, \n    Naive\n)\n\nmodels = [ARCH(1), \n          ARCH(2), \n          GARCH(1,1),\n          GARCH(1,2),\n          GARCH(2,2),\n          GARCH(2,1),\n          Naive()\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\nfreq: A string indicating the frequency of the data. Here we‚Äôll use MS, which correspond to the start of the month. You can see the list of panda‚Äôs available frequencies here.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df = returns, \n    models = models, \n    freq = 'MS',\n    n_jobs = -1\n)"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#perform-time-series-cross-validation",
    "href": "docs/tutorials/garch_tutorial.html#perform-time-series-cross-validation",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Perform time series cross-validation",
    "text": "Perform time series cross-validation\nTime series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it. Here we‚Äôll use StatsForercast‚Äôs cross-validation method to determine the most accurate model for the S&P 500 and the companies selected.\nThis method takes the following arguments:\n\ndf: The dataframe with the training data.\nh (int): represents the h steps into the future that will be forecasted.\nstep_size (int): step size between each window, meaning how often do you want to run the forecasting process.\nn_windows (int): number of windows used for cross-validation, meaning the number of forecasting processes in the past you want to evaluate.\n\nFor this particular example, we‚Äôll use 4 windows of 3 months, or all the quarters in a year.\n\ncrossvalidation_df = sf.cross_validation(\n    df = returns,\n    h = 3,\n    step_size = 3,\n    n_windows = 4\n  )\n\nThe crossvalidation_df object ia a dataframe with the following columns:\n\nunique_id: index.\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df = crossvalidation_df.reset_index()\ncrossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True)\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\nactual\nARCH(1)\nARCH(2)\nGARCH(1,1)\nGARCH(1,2)\nGARCH(2,2)\nGARCH(2,1)\nNaive\n\n\n\n\n0\nAAPL\n2022-01-01\n2021-12-01\n-0.015837\n0.142416\n0.144013\n0.142951\n0.226098\n0.141690\n0.144018\n0.073061\n\n\n1\nAAPL\n2022-02-01\n2021-12-01\n-0.056855\n-0.056896\n-0.057158\n-0.056387\n-0.087001\n-0.058787\n-0.057161\n0.073061\n\n\n2\nAAPL\n2022-03-01\n2021-12-01\n0.057156\n-0.045899\n-0.046478\n-0.047512\n-0.073625\n-0.045714\n-0.046479\n0.073061\n\n\n3\nAAPL\n2022-04-01\n2022-03-01\n-0.102178\n0.138661\n0.140211\n0.136213\n0.136124\n0.136127\n0.136546\n0.057156\n\n\n4\nAAPL\n2022-05-01\n2022-03-01\n-0.057505\n-0.056013\n-0.056268\n-0.054599\n-0.057080\n-0.057085\n-0.053791\n0.057156\n\n\n\n\n\n\n\n\nStatsForecast.plot(returns, crossvalidation_df.drop(['cutoff', 'actual'], axis=1))\n\n\n                                                \n\n\nA tutorial on cross-validation can be found here."
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#evaluate-results",
    "href": "docs/tutorials/garch_tutorial.html#evaluate-results",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Evaluate results",
    "text": "Evaluate results\nTo compute the accuracy of the forecasts, we‚Äôll use the mean average error (mae), which is the sum of the absolute errors divided by the number of forecasts. There‚Äôs an implementation of MAE on datasetsforecast, so we‚Äôll install it and then import the mae function.\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.losses import mae\n\nThe MAE needs to be computed for every window and then it needs to be averaged across all of them. To do this, we‚Äôll create the following function.\n\ndef compute_cv_mae(crossvalidation_df):\n    \"\"\"Compute MAE for all models generated\"\"\"\n    res = {}\n    for mod in models: \n        res[mod] = mae(crossvalidation_df['actual'], crossvalidation_df[str(mod)])\n    return pd.Series(res)\n\n\nmae_cv = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(compute_cv_mae)\n\nmae = mae_cv.groupby('unique_id').mean()\nmae.style.highlight_min(color = 'lightblue', axis = 1)\n\n\n\n\n\n\n¬†\nARCH(1)\nARCH(2)\nGARCH(1,1)\nGARCH(1,2)\nGARCH(2,2)\nGARCH(2,1)\nNaive\n\n\nunique_id\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n\n\nAAPL\n0.068537\n0.068927\n0.068929\n0.085630\n0.072519\n0.068556\n0.110426\n\n\nAMZN\n0.118612\n0.126182\n0.118858\n0.125470\n0.109913\n0.109912\n0.115189\n\n\nGOOG\n0.093849\n0.093752\n0.099593\n0.115136\n0.094648\n0.113645\n0.083233\n\n\nMETA\n0.198333\n0.198891\n0.199617\n0.199712\n0.199708\n0.198890\n0.185346\n\n\nMSFT\n0.080022\n0.097301\n0.082183\n0.072765\n0.073006\n0.080494\n0.086951\n\n\nNFLX\n0.159384\n0.159523\n0.219658\n0.231798\n0.230077\n0.224103\n0.167421\n\n\nNKE\n0.107842\n0.114263\n0.103097\n0.107180\n0.107179\n0.107019\n0.160405\n\n\nNVDA\n0.189462\n0.207875\n0.199004\n0.196172\n0.211928\n0.211928\n0.215289\n\n\nSPY\n0.058513\n0.065498\n0.058700\n0.057051\n0.057051\n0.058526\n0.089012\n\n\nTSLA\n0.192003\n0.192620\n0.190225\n0.192353\n0.191620\n0.191418\n0.218857\n\n\n\n\n\nHence, the most accurate model to describe the logarithmic returns of Apple‚Äôs stock is an ARCH(1), for Amazon‚Äôs stock is a GARCH(2,1), and so on."
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#forecast-volatility",
    "href": "docs/tutorials/garch_tutorial.html#forecast-volatility",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Forecast volatility",
    "text": "Forecast volatility\nWe can now generate a forecast for the next quarter. To do this, we‚Äôll use the forecast method, which requieres the following arguments:\n\nh: (int) The forecasting horizon.\nlevel: (list[float]) The confidence levels of the prediction intervals\nfitted : (bool = False) Returns insample predictions.\n\n\nlevels = [80, 95] # confidence levels for the prediction intervals \n\nforecasts = sf.forecast(h=3, level=levels)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nARCH(1)\nARCH(1)-lo-95\nARCH(1)-lo-80\nARCH(1)-hi-80\nARCH(1)-hi-95\nARCH(2)\nARCH(2)-lo-95\nARCH(2)-lo-80\n...\nGARCH(2,1)\nGARCH(2,1)-lo-95\nGARCH(2,1)-lo-80\nGARCH(2,1)-hi-80\nGARCH(2,1)-hi-95\nNaive\nNaive-lo-80\nNaive-lo-95\nNaive-hi-80\nNaive-hi-95\n\n\n\n\n0\nAAPL\n2023-01-01\n0.150457\n0.133641\n0.139462\n0.161452\n0.167273\n0.150166\n0.133415\n0.139213\n...\n0.147610\n0.131424\n0.137027\n0.158193\n0.163795\n-0.128762\n-0.284463\n-0.366886\n0.026939\n0.109362\n\n\n1\nAAPL\n2023-02-01\n-0.056942\n-0.073923\n-0.068046\n-0.045839\n-0.039961\n-0.057209\n-0.074349\n-0.068417\n...\n-0.059511\n-0.078059\n-0.071639\n-0.047384\n-0.040964\n-0.128762\n-0.348956\n-0.465520\n0.091433\n0.207997\n\n\n2\nAAPL\n2023-03-01\n-0.048390\n-0.064842\n-0.059148\n-0.037633\n-0.031939\n-0.049279\n-0.066340\n-0.060435\n...\n-0.054537\n-0.075435\n-0.068201\n-0.040874\n-0.033640\n-0.128762\n-0.398444\n-0.541205\n0.140920\n0.283681\n\n\n3\nAMZN\n2023-01-01\n0.152158\n0.134960\n0.140913\n0.163404\n0.169357\n0.148659\n0.132243\n0.137925\n...\n0.148597\n0.132195\n0.137872\n0.159322\n0.165000\n-0.139141\n-0.315716\n-0.409190\n0.037435\n0.130909\n\n\n4\nAMZN\n2023-02-01\n-0.057306\n-0.074504\n-0.068551\n-0.046060\n-0.040107\n-0.061187\n-0.080794\n-0.074007\n...\n-0.069302\n-0.094455\n-0.085749\n-0.052856\n-0.044150\n-0.139141\n-0.388856\n-0.521048\n0.110575\n0.242767\n\n\n\n\n5 rows √ó 37 columns\n\n\n\nWith the results of the previous section, we can choose the best model for the S&P 500 and the companies selected. Some of the plots are shown below. Notice that we‚Äôre using somo additional arguments in the plot method:\n\nlevel: (list[int]) The confidence levels for the prediction intervals (this was already defined).\nunique_ids: (list[str, int or category]) The ids to plot.\nmodels: (list(str)). The model to plot. In this case, is the model selected by cross-validation.\n\n\nStatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['AAPL'], models = ['GARCH(2,1)'])\n\n\n                                                \n\n\n\nStatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['MSFT'], models = ['ARCH(2)'])\n\n\n                                                \n\n\n\nStatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['NFLX'], models = ['ARCH(1)'])"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#references",
    "href": "docs/tutorials/garch_tutorial.html#references",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "References",
    "text": "References\n\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.\nBollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of econometrics, 31(3), 307-327.\nHamilton, J. D. (1994). Time series analysis. Princeton university press.\nTsay, R. S. (2005). Analysis of financial time series. John wiley & sons."
  },
  {
    "objectID": "docs/contribute/docs.html",
    "href": "docs/contribute/docs.html",
    "title": "Nixtla Documentation",
    "section": "",
    "text": "TBD\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/contribute/contribute.html",
    "href": "docs/contribute/contribute.html",
    "title": "Contribute to Nixtla",
    "section": "",
    "text": "Thank you for your interest in contributing to Nixtla. Nixtla is free, open-source software and welcomes all types of contributions, including documentation changes, bug reports, bug fixes, or new source code changes.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/contribute/contribute.html#contribution-issues",
    "href": "docs/contribute/contribute.html#contribution-issues",
    "title": "Contribute to Nixtla",
    "section": "Contribution issues üîß",
    "text": "Contribution issues üîß\nMost of the issues that are open for contributions will be tagged with good first issue or help wanted. A great place to start looking will be our GitHub projects for:\n\nCommunity writers dashboard.\nCommunity code contributors dashboard.\n\nAlso, we are always open to suggestions so feel free to open new issues with your ideas and we can give you guidance!\nAfter you find the issue that you want to contribute to, follow the fork-and-pull workflow:\n\nFork the Nixtla repository you want to work on (e.g.¬†StatsForecast or NeuralForecast)\nClone the repository locally (git clone) and create a new branch (git checkout -b my-new-branch)\nMake changes and commit them\nPush your local branch to your fork\nSubmit a Pull Request so that we can review your changes\nWrite a commit message\nMake sure that the CI tests are GREEN (CI tests refer to automated tests that are run on code changes to ensure that new additions or modifications do not introduce new errors or break existing functionality.)\n\n\nBe sure to merge the latest from ‚Äúupstream‚Äù before making a Pull Request!\n\nYou can find a complete step-by-step guide on this fork-and-pull workflow here.\nPull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA."
  },
  {
    "objectID": "docs/contribute/contribute.html#documentation",
    "href": "docs/contribute/contribute.html#documentation",
    "title": "Contribute to Nixtla",
    "section": "Documentation üìñ",
    "text": "Documentation üìñ\nWe are committed to continuously improving our documentation. As such, we warmly welcome any Pull Requests that focus on improving our grammar, documentation structure, or fixing any typos.\n\nCheck the documentation tagged issues and help us."
  },
  {
    "objectID": "docs/contribute/contribute.html#write-for-us",
    "href": "docs/contribute/contribute.html#write-for-us",
    "title": "Contribute to Nixtla",
    "section": "Write for us üìù",
    "text": "Write for us üìù\nDo you find Nixtla useful and want to share your story or create some content? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you üíö\n\nThis document is based on the documentation from MindsDB"
  },
  {
    "objectID": "docs/contribute/issue-labels.html",
    "href": "docs/contribute/issue-labels.html",
    "title": "Understanding Issue Labels",
    "section": "",
    "text": "This segment delves into the variety of issue labels used within the Nixtla GitHub repository.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/contribute/issue-labels.html#labels-relevant-to-contributors",
    "href": "docs/contribute/issue-labels.html#labels-relevant-to-contributors",
    "title": "Understanding Issue Labels",
    "section": "Labels Relevant to Contributors",
    "text": "Labels Relevant to Contributors\nShould you be a contributor now or in the future, it‚Äôs important to take note of issues flagged with these labels.\n\nThe first-timers-only Label\nFor those who have not yet contributed to Nixtla, start by looking for issues tagged as first-timers-only.\nPlease note that before we can accept your contribution to Nixtla, you‚Äôll need to sign our Contributor License Agreement.\nYou can browse all first-timers-only issues here.\n\n\nThe good first issue Label\nIssues labeled as good first issue are ideal for newcomers.\nYou can browse all good first issue issues here.\n\n\nThe help wanted Label\nIssues tagged as help wanted are open to anyone who wishes to contribute to Nixtla.\nYou can browse all help wanted issues here.\n\n\nThe bug Label\nThe bug label flags issues that outline something that‚Äôs currently not functioning correctly.\nYou can report a bug by following the instructions here.\n\n\nThe discussion Label\nIf an issue is labeled as discussion, it signifies that more conversation is needed before it can be resolved.\n\n\nThe documentation Label\nThe documentation label identifies issues pertaining to our documentation.\nYou can contribute to improving our documentation by creating issues following the guidelines here.\n\n\nThe enhancement Label\nAs Nixtla continues to evolve, there are always areas that can be enhanced. All issues suggesting improvements to Nixtla are tagged with the enhancement label.\nYou can propose a feature by following the instructions here.\n\n\nThe discussion Label\nIf an issue is labeled as discussion, it needs more information before it can be resolved.\n\n\nThe requested Label\nOur users are welcomed to propose improvements, report bugs, request feature, etc. Any issue originating from them is flagged as requested."
  },
  {
    "objectID": "docs/contribute/step-by-step.html",
    "href": "docs/contribute/step-by-step.html",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "This document contains instructions for collaborating on the different libraries of Nixtla.\n\nSometimes, diving into a new technology can be challenging and overwhelming. We‚Äôve been there too, and we‚Äôre more than ready to assist you with any issues you may encounter while following these steps. Don‚Äôt hesitate to reach out to us on Slack. Just give fede a ping, and she‚Äôll be glad to assist you.\n\n\n\nPrerequisites\nGit fork-and-pull worklow\nSet Up a Conda Environment\nInstall required libraries for development\nStart editable mode\nSet Up your Notebook based development environment\nStart Coding\nExample with Screen-shots\n\n\n\n\n\nGitHub: You should already have a GitHub account and a basic understanding of its functionalities. Alternatively check this guide.\nPython: Python should be installed on your system. Alternatively check this guide.\nconda: You need to have conda installed, along with a good grasp of fundamental operations such as creating environments, and activating and deactivating them. Alternatively check this guide.\n\n\n\n\n1. Fork the Project: Start by forking the Nixtla repository to your own GitHub account. This creates a personal copy of the project where you can make changes without affecting the main repository.\n2. Clone the Forked Repository Clone the forked repository to your local machine using git clone https://github.com/&lt;your-username&gt;/nixtla.git. This allows you to work with the code directly on your system.\n3. Create a Branch:\nBranching in GitHub is a key strategy for effectively managing and isolating changes to your project. It allows you to segregate work on different features, fixes, and issues without interfering with the main, production-ready codebase.\n\nMain Branch: The default branch with production-ready code.\nFeature Branches: For new features, create branches prefixed with ‚Äòfeature/‚Äô, like git checkout -b feature/new-model.\nFix Branches: For bug fixes, use ‚Äòfix/‚Äô prefix, like git checkout -b fix/forecasting-bug.\nIssue Branches: For specific issues, use git checkout -b issue/issue-number or git checkout -b issue/issue-description.\n\nAfter testing, branches are merged back into the main branch via a pull request, and then typically deleted to maintain a clean repository. You can read more about github and branching here.\n\n\n\n\nIf you want to use Docker or Codespaces, let us know opening an issue and we will set you up.\n\nNext, you‚Äôll need to set up a Conda environment. Conda is an open-source package management and environment management system that runs on Windows, macOS, and Linux. It allows you to create separate environments containing files, packages, and dependencies that will not interact with each other.\nFirst, ensure you have Anaconda or Miniconda installed on your system. Alternatively checkout these guides: Anaconda, Miniconda, and Mamba.\nThen, you can create a new environment using conda create -n nixtla-env python=3.10.\nYou can also use mamba for creating the environment (mamba is faster than Conda) using mamba create -n nixtla-env python=3.10.\nYou can replace nixtla-env for something more meaningful to you. Eg. statsforecast-env or mlforecast-env. You can always check the list of environments in your system using conda env list.\nActivate your new environment with conda activate nixtla-env.\n\n\n\nThe environment.yml file contains all the dependencies required for the project. To install these dependencies, use the mamba package manager, which offers faster package installation and environment resolution than Conda. If you haven‚Äôt installed mamba yet, you can do so using conda install mamba -c conda-forge. Run the following command to install the dependencies:\nmamba env update -f environment.yml\nSometimes (e.g.¬†StatsForecast) the enviorment.yml is sometimes inside a folder called dev. In that case, you should run mamba env update -f dev/environment.yml.\n\n\n\nInstall the library in editable mode using pip install -e \".[dev]\".\nThis means the package is linked directly to the source code, allowing any changes made to the source code to be immediately reflected in your Python environment without the need to reinstall the package. This is useful for testing changes during package development.\n\n\n\nNotebook-based development refers to using interactive notebooks, such as Jupyter Notebooks, for coding, data analysis, and visualization. Here‚Äôs a brief description of its characteristics:\n\nInteractivity: Code in notebooks is written in cells which can be run independently. This allows for iterative development and testing of small code snippets.\nVisualization: Notebooks can render charts, tables, images, and other graphical outputs within the same interface, making it great for data exploration and analysis.\nDocumentation: Notebooks support Markdown and HTML, allowing for detailed inline documentation. Code, outputs, and documentation are in one place, which is ideal for tutorials, reports, or sharing work.\n\nFor notebook based development you‚Äôll need nbdev and a notebook editor (such as VS Code, Jupyter Notebook or Jupyter Lab). nbdev and jupyter have been installed in the previous step. If you use VS Code follow this tutorial.\nnbdev makes debugging and refactoring your code much easier than in traditional programming environments since you always have live objects at your fingertips. nbdev also promotes software engineering best practices because tests and documentation are first class.\nAll your changes must be written in the notebooks contained in the library (under the nbs directory). Once a specific notebook is open (more details to come), you can write your Python code in cells within the notebook, as you would do in a traditional Python development workflow. You can break down complex problems into smaller parts, visualizing data, and documenting your thought process. Along with your code, you can include markdown cells to add documentation directly in the notebook. This includes explanations of your logic, usage examples, and more. Also, nbdev allows you to write tests inline with your code in your notebook. After writing a function, you can immediately write tests for it in the following cells.\nOnce your code is ready, nbdev can automatically convert your notebook into Python scripts. Code cells are converted into Python code, and markdown cells into comments and docstrings.\n\n\n\nOpen a jupyter notebook using jupyter lab (or VS Code).\n\nMake Your Changes: Make changes to the codebase, ensuring your changes are self-contained and cohesive.\nCommit Your Changes: Add the changed files using git add [your_modified_file_0.ipynb] [your_modified_file_1.ipynb], then commit these changes using git commit -m \"&lt;type&gt;: &lt;Your descriptive commit message&gt;\". Please use Conventional Commits\nPush Your Changes: Push your changes to the remote repository on GitHub with git push origin feature/your-feature-name.\nOpen a Pull Request: Open a pull request from your new branch on the Nixtla repository on GitHub. Provide a thorough description of your changes when creating the pull request.\nWait for Review: The maintainers of the Nixtla project will review your changes. Be ready to iterate on your contributions based on their feedback.\n\nRemember, contributing to open-source projects is a collaborative effort. Respect the work of others, welcome feedback, and always strive to improve. Happy coding!\n\nNixtla offers the possibility of assisting with stipends for computing infrastructure for our contributors. If you are interested, please join our slack and write to fede or Max.\n\nYou can find a detailed step by step buide with screen-shots below.\n\n\n\n\n\nThe first thing you need to do is create a fork of the GitHub repository to your own account:\n\nYour fork on your account will look like this:\n\nIn that repository, you can make your changes and then request to have them added to the main repo.\n\n\n\nIn this tutorial, we are using Mac (also compatible with other Linux distributions). If you are a collaborator of Nixtla, you can request an AWS instance to collaborate from there. If this is the case, please reach out to Max or Fede on Slack to receive the appropriate access. We also use Visual Studio Code, which you can download from here.\nOnce the repository is created, you need to clone it to your own computer. Simply copy the repository URL from GitHub as shown below:\n\nThen open Visual Studio Code, click on ‚ÄúClone Git Repository,‚Äù and paste the line you just copied into the top part of the window, as shown below:\n\nSelect the folder where you want to copy the repository:\n\nAnd choose to open the cloned repository:\n\nYou will end up with something like this:\n\n\n\n\nOpen a terminal within Visual Studio Code, as shown in the image:\n\nYou can use conda but we highly recommend using Mamba to speed up the creation of the Conda environment. To install it, simply use conda install mamba -c conda-forge in the terminal you just opened:\n\nCreate an empty environment named mlforecast with the following command: mamba create -n mlforecast python=3.10:\n\nActivate the newly created environment using conda activate mlforecast:\n\nInstall the libraries within the environment file environment.yml using mamba env update -f environment.yml:\n\nNow install the library to make interactive changes and other additional dependencies using pip install -e \".[dev]\":\n\n\n\n\nIn this section, we assume that we want to increase the default number of windows used to create prediction intervals from 2 to 3. The first thing we need to do is create a specific branch for that change using git checkout -b [new_branch] like this:\n\nOnce created, open the notebook you want to modify. In this case, it‚Äôs nbs/utils.ipynb, which contains the metadata for the prediction intervals. After opening it, click on the environment you want to use (top right) and select the mlforecast environment:\n\nNext, execute the notebook and make the necessary changes. In this case, we want to modify the PredictionIntervals class:\n\nWe will change the default value of n_window from 2 to 3:\n\nOnce you have made the change and performed any necessary validations, it‚Äôs time to convert the notebook to Python modules. To do this, simply use nbdev_export in the terminal.\nYou will see that the mlforecast/utils.py file has been modified (the changes from nbs/utils.ipynb are reflected in that module). Before committing the changes, we need to clean the notebooks using the command ./action_files/clean_nbs and verify that the linters pass using ./action_files/lint:\n\nOnce you have done the above, simply add the changes using git add nbs/utils.ipynb mlforecast/utils.py:\n\nCreate a descriptive commit message for the changes using git commit -m \"[description of changes]\":\n\nFinally, push your changes using git push:\n\n\n\n\nIn GitHub, open your repository that contains your fork of the original repo. Once inside, you will see the changes you just pushed. Click on ‚ÄúCompare and pull request‚Äù:\n\nInclude an appropriate title for your pull request and fill in the necessary information. Once you‚Äôre done, click on ‚ÄúCreate pull request‚Äù.\n\nFinally, you will see something like this:\n\n\n\n\n\n\nThis file was generated using this file. Please change that file if you want to enhance the document.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/contribute/step-by-step.html#table-of-contents",
    "href": "docs/contribute/step-by-step.html#table-of-contents",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "Prerequisites\nGit fork-and-pull worklow\nSet Up a Conda Environment\nInstall required libraries for development\nStart editable mode\nSet Up your Notebook based development environment\nStart Coding\nExample with Screen-shots"
  },
  {
    "objectID": "docs/contribute/step-by-step.html#prerequisites",
    "href": "docs/contribute/step-by-step.html#prerequisites",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "GitHub: You should already have a GitHub account and a basic understanding of its functionalities. Alternatively check this guide.\nPython: Python should be installed on your system. Alternatively check this guide.\nconda: You need to have conda installed, along with a good grasp of fundamental operations such as creating environments, and activating and deactivating them. Alternatively check this guide."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#git-fork-and-pull-worklow",
    "href": "docs/contribute/step-by-step.html#git-fork-and-pull-worklow",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "1. Fork the Project: Start by forking the Nixtla repository to your own GitHub account. This creates a personal copy of the project where you can make changes without affecting the main repository.\n2. Clone the Forked Repository Clone the forked repository to your local machine using git clone https://github.com/&lt;your-username&gt;/nixtla.git. This allows you to work with the code directly on your system.\n3. Create a Branch:\nBranching in GitHub is a key strategy for effectively managing and isolating changes to your project. It allows you to segregate work on different features, fixes, and issues without interfering with the main, production-ready codebase.\n\nMain Branch: The default branch with production-ready code.\nFeature Branches: For new features, create branches prefixed with ‚Äòfeature/‚Äô, like git checkout -b feature/new-model.\nFix Branches: For bug fixes, use ‚Äòfix/‚Äô prefix, like git checkout -b fix/forecasting-bug.\nIssue Branches: For specific issues, use git checkout -b issue/issue-number or git checkout -b issue/issue-description.\n\nAfter testing, branches are merged back into the main branch via a pull request, and then typically deleted to maintain a clean repository. You can read more about github and branching here."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#set-up-a-conda-environment",
    "href": "docs/contribute/step-by-step.html#set-up-a-conda-environment",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "If you want to use Docker or Codespaces, let us know opening an issue and we will set you up.\n\nNext, you‚Äôll need to set up a Conda environment. Conda is an open-source package management and environment management system that runs on Windows, macOS, and Linux. It allows you to create separate environments containing files, packages, and dependencies that will not interact with each other.\nFirst, ensure you have Anaconda or Miniconda installed on your system. Alternatively checkout these guides: Anaconda, Miniconda, and Mamba.\nThen, you can create a new environment using conda create -n nixtla-env python=3.10.\nYou can also use mamba for creating the environment (mamba is faster than Conda) using mamba create -n nixtla-env python=3.10.\nYou can replace nixtla-env for something more meaningful to you. Eg. statsforecast-env or mlforecast-env. You can always check the list of environments in your system using conda env list.\nActivate your new environment with conda activate nixtla-env."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#install-required-libraries-for-development",
    "href": "docs/contribute/step-by-step.html#install-required-libraries-for-development",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "The environment.yml file contains all the dependencies required for the project. To install these dependencies, use the mamba package manager, which offers faster package installation and environment resolution than Conda. If you haven‚Äôt installed mamba yet, you can do so using conda install mamba -c conda-forge. Run the following command to install the dependencies:\nmamba env update -f environment.yml\nSometimes (e.g.¬†StatsForecast) the enviorment.yml is sometimes inside a folder called dev. In that case, you should run mamba env update -f dev/environment.yml."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#start-editable-mode",
    "href": "docs/contribute/step-by-step.html#start-editable-mode",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "Install the library in editable mode using pip install -e \".[dev]\".\nThis means the package is linked directly to the source code, allowing any changes made to the source code to be immediately reflected in your Python environment without the need to reinstall the package. This is useful for testing changes during package development."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#set-up-your-notebook-based-development-environment",
    "href": "docs/contribute/step-by-step.html#set-up-your-notebook-based-development-environment",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "Notebook-based development refers to using interactive notebooks, such as Jupyter Notebooks, for coding, data analysis, and visualization. Here‚Äôs a brief description of its characteristics:\n\nInteractivity: Code in notebooks is written in cells which can be run independently. This allows for iterative development and testing of small code snippets.\nVisualization: Notebooks can render charts, tables, images, and other graphical outputs within the same interface, making it great for data exploration and analysis.\nDocumentation: Notebooks support Markdown and HTML, allowing for detailed inline documentation. Code, outputs, and documentation are in one place, which is ideal for tutorials, reports, or sharing work.\n\nFor notebook based development you‚Äôll need nbdev and a notebook editor (such as VS Code, Jupyter Notebook or Jupyter Lab). nbdev and jupyter have been installed in the previous step. If you use VS Code follow this tutorial.\nnbdev makes debugging and refactoring your code much easier than in traditional programming environments since you always have live objects at your fingertips. nbdev also promotes software engineering best practices because tests and documentation are first class.\nAll your changes must be written in the notebooks contained in the library (under the nbs directory). Once a specific notebook is open (more details to come), you can write your Python code in cells within the notebook, as you would do in a traditional Python development workflow. You can break down complex problems into smaller parts, visualizing data, and documenting your thought process. Along with your code, you can include markdown cells to add documentation directly in the notebook. This includes explanations of your logic, usage examples, and more. Also, nbdev allows you to write tests inline with your code in your notebook. After writing a function, you can immediately write tests for it in the following cells.\nOnce your code is ready, nbdev can automatically convert your notebook into Python scripts. Code cells are converted into Python code, and markdown cells into comments and docstrings."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#start-coding",
    "href": "docs/contribute/step-by-step.html#start-coding",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "Open a jupyter notebook using jupyter lab (or VS Code).\n\nMake Your Changes: Make changes to the codebase, ensuring your changes are self-contained and cohesive.\nCommit Your Changes: Add the changed files using git add [your_modified_file_0.ipynb] [your_modified_file_1.ipynb], then commit these changes using git commit -m \"&lt;type&gt;: &lt;Your descriptive commit message&gt;\". Please use Conventional Commits\nPush Your Changes: Push your changes to the remote repository on GitHub with git push origin feature/your-feature-name.\nOpen a Pull Request: Open a pull request from your new branch on the Nixtla repository on GitHub. Provide a thorough description of your changes when creating the pull request.\nWait for Review: The maintainers of the Nixtla project will review your changes. Be ready to iterate on your contributions based on their feedback.\n\nRemember, contributing to open-source projects is a collaborative effort. Respect the work of others, welcome feedback, and always strive to improve. Happy coding!\n\nNixtla offers the possibility of assisting with stipends for computing infrastructure for our contributors. If you are interested, please join our slack and write to fede or Max.\n\nYou can find a detailed step by step buide with screen-shots below."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#example-with-screen-shots",
    "href": "docs/contribute/step-by-step.html#example-with-screen-shots",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "The first thing you need to do is create a fork of the GitHub repository to your own account:\n\nYour fork on your account will look like this:\n\nIn that repository, you can make your changes and then request to have them added to the main repo.\n\n\n\nIn this tutorial, we are using Mac (also compatible with other Linux distributions). If you are a collaborator of Nixtla, you can request an AWS instance to collaborate from there. If this is the case, please reach out to Max or Fede on Slack to receive the appropriate access. We also use Visual Studio Code, which you can download from here.\nOnce the repository is created, you need to clone it to your own computer. Simply copy the repository URL from GitHub as shown below:\n\nThen open Visual Studio Code, click on ‚ÄúClone Git Repository,‚Äù and paste the line you just copied into the top part of the window, as shown below:\n\nSelect the folder where you want to copy the repository:\n\nAnd choose to open the cloned repository:\n\nYou will end up with something like this:\n\n\n\n\nOpen a terminal within Visual Studio Code, as shown in the image:\n\nYou can use conda but we highly recommend using Mamba to speed up the creation of the Conda environment. To install it, simply use conda install mamba -c conda-forge in the terminal you just opened:\n\nCreate an empty environment named mlforecast with the following command: mamba create -n mlforecast python=3.10:\n\nActivate the newly created environment using conda activate mlforecast:\n\nInstall the libraries within the environment file environment.yml using mamba env update -f environment.yml:\n\nNow install the library to make interactive changes and other additional dependencies using pip install -e \".[dev]\":\n\n\n\n\nIn this section, we assume that we want to increase the default number of windows used to create prediction intervals from 2 to 3. The first thing we need to do is create a specific branch for that change using git checkout -b [new_branch] like this:\n\nOnce created, open the notebook you want to modify. In this case, it‚Äôs nbs/utils.ipynb, which contains the metadata for the prediction intervals. After opening it, click on the environment you want to use (top right) and select the mlforecast environment:\n\nNext, execute the notebook and make the necessary changes. In this case, we want to modify the PredictionIntervals class:\n\nWe will change the default value of n_window from 2 to 3:\n\nOnce you have made the change and performed any necessary validations, it‚Äôs time to convert the notebook to Python modules. To do this, simply use nbdev_export in the terminal.\nYou will see that the mlforecast/utils.py file has been modified (the changes from nbs/utils.ipynb are reflected in that module). Before committing the changes, we need to clean the notebooks using the command ./action_files/clean_nbs and verify that the linters pass using ./action_files/lint:\n\nOnce you have done the above, simply add the changes using git add nbs/utils.ipynb mlforecast/utils.py:\n\nCreate a descriptive commit message for the changes using git commit -m \"[description of changes]\":\n\nFinally, push your changes using git push:\n\n\n\n\nIn GitHub, open your repository that contains your fork of the original repo. Once inside, you will see the changes you just pushed. Click on ‚ÄúCompare and pull request‚Äù:\n\nInclude an appropriate title for your pull request and fill in the necessary information. Once you‚Äôre done, click on ‚ÄúCreate pull request‚Äù.\n\nFinally, you will see something like this:"
  },
  {
    "objectID": "docs/contribute/step-by-step.html#notes",
    "href": "docs/contribute/step-by-step.html#notes",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "This file was generated using this file. Please change that file if you want to enhance the document."
  },
  {
    "objectID": "docs/contribute/techstack.html",
    "href": "docs/contribute/techstack.html",
    "title": "Contributing Code to Nixtla Development",
    "section": "",
    "text": "Curious about the skills required to contribute to the Nixtla project?\n\n\n\n\nIf you‚Äôre interested in making code contributions, possessing any of the following skills can assist you in getting started:\n\nGitHub\nPython 3\nconda\nnbdev\n\n\n\n\n\nForecasting: Principles and Practice\nPython Adaptation of Forecasting: Principles and Practice\n\nHappy forecasting!\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/contribute/techstack.html#required-skills-for-contribution",
    "href": "docs/contribute/techstack.html#required-skills-for-contribution",
    "title": "Contributing Code to Nixtla Development",
    "section": "",
    "text": "If you‚Äôre interested in making code contributions, possessing any of the following skills can assist you in getting started:\n\nGitHub\nPython 3\nconda\nnbdev\n\n\n\n\n\nForecasting: Principles and Practice\nPython Adaptation of Forecasting: Principles and Practice\n\nHappy forecasting!"
  },
  {
    "objectID": "docs/contribute/issues.html",
    "href": "docs/contribute/issues.html",
    "title": "Submit an Issue üì¢",
    "section": "",
    "text": "To report a bug, request a feature, propose a new integration, or suggest documentation improvements, please visit the Nixtla GitHub issues page. Before submitting a new issue, kindly check if it has already been reported.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/contribute/issues.html#steps-to-submit-an-issue",
    "href": "docs/contribute/issues.html#steps-to-submit-an-issue",
    "title": "Submit an Issue üì¢",
    "section": "Steps to Submit an Issue",
    "text": "Steps to Submit an Issue\nHere‚Äôs a step-by-step guide on submitting an issue to the Nixtla repository.\nVisit our GitHub issues page and click on the New issue button.\nA list of available issue types will be displayed.\n\nReporting a Bug üêû\nSelect Report a bug and click on the Get started button.\nThe form to report the bug will appear.\n\nBegin by adding a concise, informative title.\nDescribe the bug you‚Äôve observed. This information is required. You can also attach relevant videos or screenshots.\nIf you‚Äôre aware of what the correct behavior should be, note it down here.\nDocumenting the steps leading to the bug will be of immense help to us.\nYou can also add links, references, logs, screenshots, and so on.\n\n\nPlease ensure that your contributions abide by the contributing guidelines and code of conduct.\n\nThank you for your contribution! Your report aids in refining Nixtla for current and future users.\n\n\nFeature Request üöÄ\nSelect Request a feature and click the Get started button.\nThe feature request form will appear.\n\nStart with a significant, clear title.\nProvide a detailed description of the feature you want to request, along with the reasoning behind the request. This field is mandatory. Feel free to attach related videos or screenshots.\nIf you have an idea of how the feature should work, include it.\nAdditional references, links, logs, and screenshots are welcome!\n\n\nPlease ensure that your contributions abide by the contributing guidelines and code of conduct.\n\nThank you for your feature request! It will help us enhance Nixtla for all users.\n\n\nSuggest Documentation Improvements ‚úçÔ∏è\nSelect Improve our docs and click the Get started button.\nA form for suggesting improvements will appear.\n\nA clear, concise title is important.\nDescribe the improvements you believe are needed. This field is mandatory. Attach any related videos or screenshots, if necessary.\nAny additional references, links, logs, screenshots are appreciated!\n\n\nPlease ensure that your contributions abide by the contributing guidelines and code of conduct.\n\nThank you for your valuable suggestions! Your input helps us refine Nixtla‚Äôs documentation.\n\n\nProposing a New Integration üßë‚Äçüîß\nIf you have a proposal for a new database integration or a new machine learning framework, here‚Äôs how to get started:\nSelect `\nPropose a new integration` and click the Get started button.\nA form for your proposal will appear.\n\nStart with a clear, concise title.\nDescribe your proposal and why it is needed. This field is mandatory. Feel free to attach any related videos or screenshots.\nIf you have an idea of how this integration should work, include it.\nAny additional references, links, logs, screenshots, and so on, are welcome!\n\n\nPlease ensure that your contributions abide by the contributing guidelines and code of conduct.\n\nThank you for your proposal! Your suggestion helps us extend the capabilities of Nixtla."
  },
  {
    "objectID": "docs/contribute/issues.html#reviewing-issues",
    "href": "docs/contribute/issues.html#reviewing-issues",
    "title": "Submit an Issue üì¢",
    "section": "Reviewing Issues",
    "text": "Reviewing Issues\n\nIssues are reviewed on a regular basis, usually every day.\nIssues will be labeled as Bug or enhancement based on their type.\nPlease be ready to respond to our feedback or questions regarding your issue.\n\n\nThis document is based on the documentation from MindsDB"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html",
    "href": "docs/tutorials/conformalprediction.html",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Prerequisites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#introduction",
    "href": "docs/tutorials/conformalprediction.html#introduction",
    "title": "Conformal Prediction",
    "section": "Introduction",
    "text": "Introduction\nWhen we generate a forecast, we usually produce a single value known as the point forecast. This value, however, doesn‚Äôt tell us anything about the uncertainty associated with the forecast. To have a measure of this uncertainty, we need prediction intervals.\nA prediction interval is a range of values that the forecast can take with a given probability. Hence, a 95% prediction interval should contain a range of values that include the actual future value with probability 95%. Probabilistic forecasting aims to generate the full forecast distribution. Point forecasting, on the other hand, usually returns the mean or the median or said distribution. However, in real-world scenarios, it is better to forecast not only the most probable future outcome, but many alternative outcomes as well.\nThe problem is that some timeseries models provide forecast distributions, but some other ones only provide point forecasts. How can we then estimate the uncertainty of predictions?\n\n\n\n\n\n\nPrediction Intervals\n\n\n\nFor models that already provide the forecast distribution, check Prediction Intervals.\n\n\n\nConformal Prediction\nFor a video introduction, see the PyData Seattle presentation.\nMulti-quantile losses and statistical models can provide provide prediction intervals, but the problem is that these are uncalibrated, meaning that the actual frequency of observations falling within the interval does not align with the confidence level associated with it. For example, a calibrated 95% prediction interval should contain the true value 95% of the time in repeated sampling. An uncalibrated 95% prediction interval, on the other hand, might contain the true value only 80% of the time, or perhaps 99% of the time. In the first case, the interval is too narrow and underestimates the uncertainty, while in the second case, it is too wide and overestimates the uncertainty.\nStatistical methods also assume normality. Here, we talk about another method called conformal prediction that doesn‚Äôt require any distributional assumptions. More information on the approach can be found in this repo owned by Valery Manokhin.\nConformal prediction intervals use cross-validation on a point forecaster model to generate the intervals. This means that no prior probabilities are needed, and the output is well-calibrated. No additional training is needed, and the model is treated as a black box. The approach is compatible with any model.\nStatsforecast now supports Conformal Prediction on all available models."
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#install-libraries",
    "href": "docs/tutorials/conformalprediction.html#install-libraries",
    "title": "Conformal Prediction",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#load-and-explore-the-data",
    "href": "docs/tutorials/conformalprediction.html#load-and-explore-the-data",
    "title": "Conformal Prediction",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we‚Äôll use the hourly dataset from the M4 Competition. We first need to download the data from a URL and then load it as a pandas dataframe. Notice that we‚Äôll load the train and the test data separately. We‚Äôll also rename the y column of the test data as y_test.\n\nimport pandas as pd \n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nSince the goal of this notebook is to generate prediction intervals, we‚Äôll only use the first 8 series of the dataset to reduce the total computational time.\n\nn_series = 8 \nuids = train['unique_id'].unique()[:n_series] # select first n_series of the dataset\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\nWe can plot these series using the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: bool = True. Plots the time series randomly.\nmodels: List[str]. A list with the models we want to plot.\nlevel: List[float]. A list with the prediction intervals we want to plot.\nengine: str = matplotlib. It can also be matplotlib. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(train, test, plot_random = False, engine = 'plotly')\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#train-models",
    "href": "docs/tutorials/conformalprediction.html#train-models",
    "title": "Conformal Prediction",
    "section": "Train models",
    "text": "Train models\nStatsForecast can train multiple models on different time series efficiently. Most of these models can generate a probabilistic forecast, which means that they can produce both point forecasts and prediction intervals.\nFor this example, we‚Äôll use SimpleExponentialSmoothing and ADIDA which do not provide a prediction interval natively. Thus, it makes sense to use Conformal Prediction to generate the prediction interval.\nWe‚Äôll also show using it with ARIMA to provide prediction intervals that don‚Äôt assume normality.\nTo use these models, we first need to import them from statsforecast.models and then we need to instantiate them.\n\nfrom statsforecast.models import SeasonalExponentialSmoothing, ADIDA, ARIMA\nfrom statsforecast.utils import ConformalIntervals\n\n# Create a list of models and instantiation parameters \nintervals = ConformalIntervals(h=24, n_windows=2)\n\nmodels = [\n    SeasonalExponentialSmoothing(season_length=24,alpha=0.1, prediction_intervals=intervals),\n    ADIDA(prediction_intervals=intervals),\n    ARIMA(order=(24,0,12), season_length=24, prediction_intervals=intervals),\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\n\nfreq: A string indicating the frequency of the data. See pandas‚Äô available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df=train, \n    models=models, \n    freq='H', \n)\n\nNow we‚Äôre ready to generate the forecasts and the prediction intervals. To do this, we‚Äôll use the forecast method, which takes two arguments:\n\nh: An integer that represent the forecasting horizon. In this case, we‚Äôll forecast the next 24 hours.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nlevels = [80, 90] # confidence levels of the prediction intervals \n\nforecasts = sf.forecast(h=24, level=levels)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nSeasonalES\nSeasonalES-lo-90\nSeasonalES-lo-80\nSeasonalES-hi-80\nSeasonalES-hi-90\nADIDA\nADIDA-lo-90\nADIDA-lo-80\nADIDA-hi-80\nADIDA-hi-90\nARIMA\nARIMA-lo-90\nARIMA-lo-80\nARIMA-hi-80\nARIMA-hi-90\n\n\n\n\n0\nH1\n701\n624.132690\n561.315369\n565.365356\n682.900024\n686.950012\n747.292542\n668.049988\n672.099976\n822.485107\n826.535095\n634.355164\n581.760376\n585.810364\n682.900024\n686.950012\n\n\n1\nH1\n702\n555.698181\n501.886902\n510.377441\n601.018921\n609.509460\n747.292542\n560.200012\n570.400024\n924.185059\n934.385071\n578.701355\n540.992310\n542.581909\n614.820801\n616.410400\n\n\n2\nH1\n703\n514.403015\n468.656036\n471.506042\n557.299988\n560.150024\n747.292542\n546.849976\n549.700012\n944.885071\n947.735107\n544.308960\n528.375244\n531.132568\n557.485352\n560.242676\n\n\n3\nH1\n704\n482.057892\n438.715790\n442.315796\n521.799988\n525.400024\n747.292542\n508.600006\n512.200012\n982.385071\n985.985107\n516.846619\n504.739288\n504.785309\n528.907959\n528.953979\n\n\n4\nH1\n705\n460.222534\n419.595062\n422.745056\n497.700012\n500.850006\n747.292542\n486.149994\n489.299988\n1005.285095\n1008.435059\n502.623077\n485.736938\n488.473846\n516.772339\n519.509277"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#plot-prediction-intervals",
    "href": "docs/tutorials/conformalprediction.html#plot-prediction-intervals",
    "title": "Conformal Prediction",
    "section": "Plot prediction intervals",
    "text": "Plot prediction intervals\nHere we‚Äôll plot the different intervals using matplotlib for one timeseries.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef _plot_fcst(fcst, train, model): \n    fig, ax = plt.subplots(1, 1, figsize = (20,7))\n    plt.plot(np.arange(0, len(train['y'])), train['y'])\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[model], label=model)\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[f'{model}-lo-90'], color = 'r', label='lo-90')\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[f'{model}-hi-90'], color = 'r', label='hi-90')\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[f'{model}-lo-80'], color = 'g', label='lo-80')\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[f'{model}-hi-80'], color = 'g', label='hi-80')\n    plt.legend()\n\n\nid = \"H105\"\ntemp_train = train.loc[train['unique_id'] == id]\ntemp_forecast = forecasts.loc[forecasts['unique_id'] == id]\n\nThe prediction interval with the SeasonalExponentialSmoothing seen below. Even if the model generates a point forecast, we are able to get a prediction interval. The 80% prediction interval does not cross the 90% prediction interval, which is a sign that the intervals are calibrated.\n\n_plot_fcst(temp_forecast, temp_train, \"SeasonalES\")\n\n\n\n\nFor weaker fitting models, the conformal prediction interval can be larger. A better model corresponds to a narrower interval.\n\n_plot_fcst(temp_forecast, temp_train,\"ADIDA\")\n\n\n\n\nARIMA is an example of a model that provides a forecast distribution, but we can still use conformal prediction to generate the prediction interval. As mentioned earlier, this method has the benefit of not assuming normality.\n\n_plot_fcst(temp_forecast, temp_train,\"ARIMA\")"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#statsforecast-object",
    "href": "docs/tutorials/conformalprediction.html#statsforecast-object",
    "title": "Conformal Prediction",
    "section": "StatsForecast Object",
    "text": "StatsForecast Object\nAlternatively, the prediction interval can be defined on the StatsForecast object. This will apply to all models that don‚Äôt have the prediction_intervals defined.\n\nfrom statsforecast.models import SimpleExponentialSmoothing, ADIDA\nfrom statsforecast.utils import ConformalIntervals\nfrom statsforecast import StatsForecast\n\nmodels = [\n    SimpleExponentialSmoothing(alpha=0.1),\n    ADIDA()\n]\n\nres = StatsForecast(\n    df=train, \n    models=models, \n    freq='H').forecast(h=24, prediction_intervals=ConformalIntervals(h=24, n_windows=2), level=[80]) \n\nres.head().reset_index()\n\n\n\n\n\n\n\n\nunique_id\nds\nSES\nSES-lo-80\nSES-hi-80\nADIDA\nADIDA-lo-80\nADIDA-hi-80\n\n\n\n\n0\nH1\n701\n742.669067\n672.099976\n813.238159\n747.292542\n672.099976\n822.485107\n\n\n1\nH1\n702\n742.669067\n570.400024\n914.938110\n747.292542\n570.400024\n924.185059\n\n\n2\nH1\n703\n742.669067\n549.700012\n935.638123\n747.292542\n549.700012\n944.885071\n\n\n3\nH1\n704\n742.669067\n512.200012\n973.138123\n747.292542\n512.200012\n982.385071\n\n\n4\nH1\n705\n742.669067\n489.299988\n996.038147\n747.292542\n489.299988\n1005.285095"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#future-work",
    "href": "docs/tutorials/conformalprediction.html#future-work",
    "title": "Conformal Prediction",
    "section": "Future work",
    "text": "Future work\nConformal prediction has become a powerful framework for uncertainty quantification, providing well-calibrated prediction intervals without making any distributional assumptions. Its use has surged in both academia and industry over the past few years. We‚Äôll continue working on it, and future tutorials may include:\n\nExploring larger datasets\nIncorporating industry-specific examples\nInvestigating specialized methods like the jackknife+ that are closely related to conformal prediction (for details on the jackknife+ see here).\n\nIf you‚Äôre interested in any of these, or in any other related topic, please let us know by opening an issue on GitHub"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#acknowledgements",
    "href": "docs/tutorials/conformalprediction.html#acknowledgements",
    "title": "Conformal Prediction",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Kevin Kho for writing this tutorial, and Valeriy Manokhin for his expertise on conformal prediction, as well as for promoting this work."
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#references",
    "href": "docs/tutorials/conformalprediction.html#references",
    "title": "Conformal Prediction",
    "section": "References",
    "text": "References\nManokhin, Valery. (2022). Machine Learning for Probabilistic Prediction. 10.5281/zenodo.6727505."
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html",
    "href": "docs/tutorials/statisticalneuralmethods.html",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "",
    "text": "Statistical, Machine Learning, and Neural Forecasting Methods In this tutorial, we will explore the process of forecasting on the M5 dataset by utilizing the most suitable model for each time series. We‚Äôll accomplish this through an essential technique known as cross-validation. This approach helps us in estimating the predictive performance of our models, and in selecting the model that yields the best performance for each time series.\nThe M5 dataset comprises of hierarchical sales data, spanning five years, from Walmart. The aim is to forecast daily sales for the next 28 days. The dataset is broken down into the 50 states of America, with 10 stores in each state.\nIn the realm of time series forecasting and analysis, one of the more complex tasks is identifying the model that is optimally suited for a specific group of series. Quite often, this selection process leans heavily on intuition, which may not necessarily align with the empirical reality of our dataset.\nIn this tutorial, we aim to provide a more structured, data-driven approach to model selection for different groups of series within the M5 benchmark dataset. This dataset, well-known in the field of forecasting, allows us to showcase the versatility and power of our methodology.\nWe will train an assortment of models from various forecasting paradigms:\nStatsForecast\nMLForecast\nMachine Learning: Leveraging ML models like LightGBM, XGBoost, and LinearRegression can be advantageous due to their capacity to uncover intricate patterns in data. We‚Äôll use the MLForecast library for this purpose.\nNeuralForecast\nDeep Learning: DL models, such as Transformers (AutoTFT) and Neural Networks (AutoNHITS), allow us to handle complex non-linear dependencies in time series data. We‚Äôll utilize the NeuralForecast library for these models.\nUsing the Nixtla suite of libraries, we‚Äôll be able to drive our model selection process with data, ensuring we utilize the most suitable models for specific groups of series in our dataset.\nOutline:\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#installing-libraries",
    "href": "docs/tutorials/statisticalneuralmethods.html#installing-libraries",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Installing Libraries",
    "text": "Installing Libraries\n\n!pip install statsforecast mlforecast neuralforecast datasetforecast s3fs pyarrow"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#download-and-prepare-data",
    "href": "docs/tutorials/statisticalneuralmethods.html#download-and-prepare-data",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Download and prepare data",
    "text": "Download and prepare data\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\n\nimport pandas as pd\n\n\n# Load the training target dataset from the provided URL\nY_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet')\n\n# Rename columns to match the Nixtlaverse's expectations\n# The 'item_id' becomes 'unique_id' representing the unique identifier of the time series\n# The 'timestamp' becomes 'ds' representing the time stamp of the data points\n# The 'demand' becomes 'y' representing the target variable we want to forecast\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\n\n# Convert the 'ds' column to datetime format to ensure proper handling of date-related operations in subsequent steps\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\nFor simplicity sake we will keep just one category\n\nY_df = Y_df.query('unique_id.str.startswith(\"FOODS_3\")').reset_index(drop=True)\n\nY_df['unique_id'] = Y_df['unique_id'].astype(str)"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#statsforecast",
    "href": "docs/tutorials/statisticalneuralmethods.html#statsforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "StatsForecast",
    "text": "StatsForecast\nStatsForecast is a comprehensive library providing a suite of popular univariate time series forecasting models, all designed with a focus on high performance and scalability.\nHere‚Äôs what makes StatsForecast a powerful tool for time series forecasting:\n\nCollection of Local Models: StatsForecast provides a diverse collection of local models that can be applied to each time series individually, allowing us to capture unique patterns within each series.\nSimplicity: With StatsForecast, training, forecasting, and backtesting multiple models become a straightforward process, requiring only a few lines of code. This simplicity makes it a convenient tool for both beginners and experienced practitioners.\nOptimized for Speed: The implementation of the models in StatsForecast is optimized for speed, ensuring that large-scale computations are performed efficiently, thereby reducing the overall time for model training and prediction.\nHorizontal Scalability: One of the distinguishing features of StatsForecast is its ability to scale horizontally. It is compatible with distributed computing frameworks such as Spark, Dask, and Ray. This feature allows it to handle large datasets by distributing the computations across multiple nodes in a cluster, making it a go-to solution for large-scale time series forecasting tasks.\n\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality.\n\n# Import necessary models from the statsforecast library\nfrom statsforecast.models import (\n    # SeasonalNaive: A model that uses the previous season's data as the forecast\n    SeasonalNaive,\n    # Naive: A simple model that uses the last observed value as the forecast\n    Naive,\n    # HistoricAverage: This model uses the average of all historical data as the forecast\n    HistoricAverage,\n    # CrostonOptimized: A model specifically designed for intermittent demand forecasting\n    CrostonOptimized,\n    # ADIDA: Adaptive combination of Intermittent Demand Approaches, a model designed for intermittent demand\n    ADIDA,\n    # IMAPA: Intermittent Multiplicative AutoRegressive Average, a model for intermittent series that incorporates autocorrelation\n    IMAPA,\n    # AutoETS: Automated Exponential Smoothing model that automatically selects the best Exponential Smoothing model based on AIC\n    AutoETS\n)\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails. Any settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\nhorizon = 28\nmodels = [\n    SeasonalNaive(season_length=7),\n    Naive(),\n    HistoricAverage(),\n    CrostonOptimized(),\n    ADIDA(),\n    IMAPA(),\n    AutoETS(season_length=7)\n]\n\n\n# Instantiate the StatsForecast class\nsf = StatsForecast(\n    models=models,  # A list of models to be used for forecasting\n    freq='D',  # The frequency of the time series data (in this case, 'D' stands for daily frequency)\n    n_jobs=-1,  # The number of CPU cores to use for parallel execution (-1 means use all available cores)\n)\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis block of code times how long it takes to run the forecasting function of the StatsForecast class, which predicts the next 28 days (h=28). The level is set to [90], meaning it will compute the 90% prediction interval. The time is calculated in minutes and printed out at the end.\n\nfrom time import time\n\n# Get the current time before forecasting starts, this will be used to measure the execution time\ninit = time()\n\n# Call the forecast method of the StatsForecast instance to predict the next 28 days (h=28) \n# Level is set to [90], which means that it will compute the 90% prediction interval\nfcst_df = sf.forecast(df=Y_df, h=28, level=[90])\n\n# Get the current time after the forecasting ends\nend = time()\n\n# Calculate and print the total time taken for the forecasting in minutes\nprint(f'Forecast Minutes: {(end - init) / 60}')\n\nForecast Minutes: 2.270755163828532\n\n\n\nfcst_df.head()\n\n\n\n\n\n\n\n\nds\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoETS-lo-90\nAutoETS-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOODS_3_001_CA_1\n2016-05-23\n1.0\n-2.847174\n4.847174\n2.0\n0.098363\n3.901637\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.381414\n-1.028122\n1.790950\n\n\nFOODS_3_001_CA_1\n2016-05-24\n0.0\n-3.847174\n3.847174\n2.0\n-0.689321\n4.689321\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.286933\n-1.124136\n1.698003\n\n\nFOODS_3_001_CA_1\n2016-05-25\n0.0\n-3.847174\n3.847174\n2.0\n-1.293732\n5.293732\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.334987\n-1.077614\n1.747588\n\n\nFOODS_3_001_CA_1\n2016-05-26\n1.0\n-2.847174\n4.847174\n2.0\n-1.803274\n5.803274\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.186851\n-1.227280\n1.600982\n\n\nFOODS_3_001_CA_1\n2016-05-27\n0.0\n-3.847174\n3.847174\n2.0\n-2.252190\n6.252190\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.308112\n-1.107548\n1.723771"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#mlforecast",
    "href": "docs/tutorials/statisticalneuralmethods.html#mlforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "MLForecast",
    "text": "MLForecast\nMLForecast is a powerful library that provides automated feature creation for time series forecasting, facilitating the use of global machine learning models. It is designed for high performance and scalability.\nKey features of MLForecast include:\n\nSupport for sklearn models: MLForecast is compatible with models that follow the scikit-learn API. This makes it highly flexible and allows it to seamlessly integrate with a wide variety of machine learning algorithms.\nSimplicity: With MLForecast, the tasks of training, forecasting, and backtesting models can be accomplished in just a few lines of code. This streamlined simplicity makes it user-friendly for practitioners at all levels of expertise.\nOptimized for speed: MLForecast is engineered to execute tasks rapidly, which is crucial when handling large datasets and complex models.\nHorizontal Scalability: MLForecast is capable of horizontal scaling using distributed computing frameworks such as Spark, Dask, and Ray. This feature enables it to efficiently process massive datasets by distributing the computations across multiple nodes in a cluster, making it ideal for large-scale time series forecasting tasks.\n\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom mlforecast.utils import PredictionIntervals\nfrom window_ops.expanding import expanding_mean\n\n\n!pip install lightgbm xgboost\n\n\n# Import the necessary models from various libraries\n\n# LGBMRegressor: A gradient boosting framework that uses tree-based learning algorithms from the LightGBM library\nfrom lightgbm import LGBMRegressor\n\n# XGBRegressor: A gradient boosting regressor model from the XGBoost library\nfrom xgboost import XGBRegressor\n\n# LinearRegression: A simple linear regression model from the scikit-learn library\nfrom sklearn.linear_model import LinearRegression\n\nTo use MLForecast for time series forecasting, we instantiate a new MLForecast object and provide it with various parameters to tailor the modeling process to our specific needs:\n\nmodels: This parameter accepts a list of machine learning models you wish to use for forecasting. You can import your preferred models from scikit-learn, lightgbm and xgboost.\nfreq: This is a string indicating the frequency of your data (hourly, daily, weekly, etc.). The specific format of this string should align with pandas‚Äô recognized frequency strings.\ntarget_transforms: These are transformations applied to the target variable before model training and after model prediction. This can be useful when working with data that may benefit from transformations, such as log-transforms for highly skewed data.\nlags: This parameter accepts specific lag values to be used as regressors. Lags represent how many steps back in time you want to look when creating features for your model. For example, if you want to use the previous day‚Äôs data as a feature for predicting today‚Äôs value, you would specify a lag of 1.\nlags_transforms: These are specific transformations for each lag. This allows you to apply transformations to your lagged features.\ndate_features: This parameter specifies date-related features to be used as regressors. For instance, you might want to include the day of the week or the month as a feature in your model.\nnum_threads: This parameter controls the number of threads to use for parallelizing feature creation, helping to speed up this process when working with large datasets.\n\nAll these settings are passed to the MLForecast constructor. Once the MLForecast object is initialized with these settings, we call its fit method and pass the historical data frame as the argument. The fit method trains the models on the provided historical data, readying them for future forecasting tasks.\n\n# Instantiate the MLForecast object\nmlf = MLForecast(\n    models=[LGBMRegressor(), XGBRegressor(), LinearRegression()],  # List of models for forecasting: LightGBM, XGBoost and Linear Regression\n    freq='D',  # Frequency of the data - 'D' for daily frequency\n    lags=list(range(1, 7)),  # Specific lags to use as regressors: 1 to 6 days\n    lag_transforms = {\n        1:  [expanding_mean],  # Apply expanding mean transformation to the lag of 1 day\n    },\n    date_features=['year', 'month', 'day', 'dayofweek', 'quarter', 'week'],  # Date features to use as regressors\n)\n\nJust call the fit models to train the select models. In this case we are generating conformal prediction intervals.\n\n# Start the timer to calculate the time taken for fitting the models\ninit = time()\n\n# Fit the MLForecast models to the data, with prediction intervals set using a window size of 28 days\nmlf.fit(Y_df, prediction_intervals=PredictionIntervals(window_size=28))\n\n# Calculate the end time after fitting the models\nend = time()\n\n# Print the time taken to fit the MLForecast models, in minutes\nprint(f'MLForecast Minutes: {(end - init) / 60}')\n\nMLForecast Minutes: 2.2809854547182717\n\n\nAfter that, just call predict to generate forecasts.\n\nfcst_mlf_df = mlf.predict(28, level=[90])\n\n\nfcst_mlf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\nXGBRegressor\nLinearRegression\nLGBMRegressor-lo-90\nLGBMRegressor-hi-90\nXGBRegressor-lo-90\nXGBRegressor-hi-90\nLinearRegression-lo-90\nLinearRegression-hi-90\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-05-23\n0.549520\n0.598431\n0.359638\n-0.213915\n1.312955\n-0.020050\n1.216912\n0.030000\n0.689277\n\n\n1\nFOODS_3_001_CA_1\n2016-05-24\n0.553196\n0.337268\n0.100361\n-0.251383\n1.357775\n-0.201449\n0.875985\n-0.216195\n0.416917\n\n\n2\nFOODS_3_001_CA_1\n2016-05-25\n0.599668\n0.349604\n0.175840\n-0.203974\n1.403309\n-0.284416\n0.983624\n-0.150593\n0.502273\n\n\n3\nFOODS_3_001_CA_1\n2016-05-26\n0.638097\n0.322144\n0.156460\n0.118688\n1.157506\n-0.085872\n0.730160\n-0.273851\n0.586771\n\n\n4\nFOODS_3_001_CA_1\n2016-05-27\n0.763305\n0.300362\n0.328194\n-0.313091\n1.839701\n-0.296636\n0.897360\n-0.657089\n1.313476"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#neuralforecast",
    "href": "docs/tutorials/statisticalneuralmethods.html#neuralforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "NeuralForecast",
    "text": "NeuralForecast\nNeuralForecast is a robust collection of neural forecasting models that focuses on usability and performance. It includes a variety of model architectures, from classic networks such as Multilayer Perceptrons (MLP) and Recurrent Neural Networks (RNN) to novel contributions like N-BEATS, N-HITS, Temporal Fusion Transformers (TFT), and more.\nKey features of NeuralForecast include:\n\nA broad collection of global models. Out of the box implementation of MLP, LSTM, RNN, TCN, DilatedRNN, NBEATS, NHITS, ESRNN, TFT, Informer, PatchTST and HINT.\nA simple and intuitive interface that allows training, forecasting, and backtesting of various models in a few lines of code.\nSupport for GPU acceleration to improve computational speed.\n\nThis machine doesn‚Äôt have GPU, but Google Colabs offers some for free.\nUsing Colab‚Äôs GPU to train NeuralForecast.\n\n# Read the results from Colab\nfcst_nf_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/forecast-nf.parquet')\n\n\nfcst_nf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-hi-90\nAutoTFT\nAutoTFT-lo-90\nAutoTFT-hi-90\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-05-23\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n1\nFOODS_3_001_CA_1\n2016-05-24\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n2\nFOODS_3_001_CA_1\n2016-05-25\n0.0\n0.0\n2.0\n0.0\n0.0\n1.0\n\n\n3\nFOODS_3_001_CA_1\n2016-05-26\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n4\nFOODS_3_001_CA_1\n2016-05-27\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n\n\n\n\n\n\n# Merge the forecasts from StatsForecast and NeuralForecast\nfcst_df = fcst_df.merge(fcst_nf_df, how='left', on=['unique_id', 'ds'])\n\n# Merge the forecasts from MLForecast into the combined forecast dataframe\nfcst_df = fcst_df.merge(fcst_mlf_df, how='left', on=['unique_id', 'ds'])\n\n\nfcst_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\n...\nAutoTFT-hi-90\nLGBMRegressor\nXGBRegressor\nLinearRegression\nLGBMRegressor-lo-90\nLGBMRegressor-hi-90\nXGBRegressor-lo-90\nXGBRegressor-hi-90\nLinearRegression-lo-90\nLinearRegression-hi-90\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-05-23\n1.0\n-2.847174\n4.847174\n2.0\n0.098363\n3.901637\n0.448738\n-1.009579\n...\n2.0\n0.549520\n0.598431\n0.359638\n-0.213915\n1.312955\n-0.020050\n1.216912\n0.030000\n0.689277\n\n\n1\nFOODS_3_001_CA_1\n2016-05-24\n0.0\n-3.847174\n3.847174\n2.0\n-0.689321\n4.689321\n0.448738\n-1.009579\n...\n2.0\n0.553196\n0.337268\n0.100361\n-0.251383\n1.357775\n-0.201449\n0.875985\n-0.216195\n0.416917\n\n\n2\nFOODS_3_001_CA_1\n2016-05-25\n0.0\n-3.847174\n3.847174\n2.0\n-1.293732\n5.293732\n0.448738\n-1.009579\n...\n1.0\n0.599668\n0.349604\n0.175840\n-0.203974\n1.403309\n-0.284416\n0.983624\n-0.150593\n0.502273\n\n\n3\nFOODS_3_001_CA_1\n2016-05-26\n1.0\n-2.847174\n4.847174\n2.0\n-1.803274\n5.803274\n0.448738\n-1.009579\n...\n2.0\n0.638097\n0.322144\n0.156460\n0.118688\n1.157506\n-0.085872\n0.730160\n-0.273851\n0.586771\n\n\n4\nFOODS_3_001_CA_1\n2016-05-27\n0.0\n-3.847174\n3.847174\n2.0\n-2.252190\n6.252190\n0.448738\n-1.009579\n...\n2.0\n0.763305\n0.300362\n0.328194\n-0.313091\n1.839701\n-0.296636\n0.897360\n-0.657089\n1.313476\n\n\n\n\n5 rows √ó 32 columns"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#forecast-plots",
    "href": "docs/tutorials/statisticalneuralmethods.html#forecast-plots",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Forecast plots",
    "text": "Forecast plots\n\nsf.plot(Y_df, fcst_df, max_insample_length=28 * 3)\n\n\n                                                \n\n\nUse the plot function to explore models and ID‚Äôs\n\nsf.plot(Y_df, fcst_df, max_insample_length=28 * 3, \n        models=['CrostonOptimized', 'AutoNHITS', 'SeasonalNaive', 'LGBMRegressor'])"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#cross-validation-in-statsforecast",
    "href": "docs/tutorials/statisticalneuralmethods.html#cross-validation-in-statsforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Cross Validation in StatsForecast",
    "text": "Cross Validation in StatsForecast\nThe cross_validation method from the StatsForecast class accepts the following arguments:\n\ndf: A DataFrame representing the training data.\nh (int): The forecast horizon, represented as the number of steps into the future that we wish to predict. For example, if we‚Äôre forecasting hourly data, h=24 would represent a 24-hour forecast.\nstep_size (int): The step size between each cross-validation window. This parameter determines how often we want to run the forecasting process.\nn_windows (int): The number of windows used for cross validation. This parameter defines how many past forecasting processes we want to evaluate.\n\nThese parameters allow us to control the extent and granularity of our cross-validation process. By tuning these settings, we can balance between computational cost and the thoroughness of the cross-validation.\n\ninit = time()\ncv_df = sf.cross_validation(df=Y_df, h=horizon, n_windows=3, step_size=horizon, level=[90])\nend = time()\nprint(f'CV Minutes: {(end - init) / 60}')\n\n/home/ubuntu/statsforecast/statsforecast/ets.py:1041: RuntimeWarning:\n\ndivide by zero encountered in double_scalars\n\n\n\nCV Minutes: 5.206169327100118\n\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncv_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoETS-lo-90\nAutoETS-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n2.0\n-1.878885\n5.878885\n0.0\n-1.917011\n1.917011\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.655286\n-0.765731\n2.076302\n\n\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n1.0\n0.0\n-3.878885\n3.878885\n0.0\n-2.711064\n2.711064\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.568595\n-0.853966\n1.991155\n\n\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n1.0\n0.0\n-3.878885\n3.878885\n0.0\n-3.320361\n3.320361\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.618805\n-0.805298\n2.042908\n\n\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n1.0\n-2.878885\n4.878885\n0.0\n-3.834023\n3.834023\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.455891\n-0.969753\n1.881534\n\n\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n1.0\n-2.878885\n4.878885\n0.0\n-4.286568\n4.286568\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.591197\n-0.835987\n2.018380"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#mlforecast-1",
    "href": "docs/tutorials/statisticalneuralmethods.html#mlforecast-1",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "MLForecast",
    "text": "MLForecast\nThe cross_validation method from the MLForecast class takes the following arguments.\n\ndata: training data frame\nwindow_size (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows (int): number of windows used for cross-validation. In other words: what number of forecasting processes in the past do you want to evaluate.\nprediction_intervals: class to compute conformal intervals.\n\n\ninit = time()\ncv_mlf_df = mlf.cross_validation(\n    data=Y_df, \n    window_size=horizon, \n    n_windows=3, \n    step_size=horizon, \n    level=[90],\n)\nend = time()\nprint(f'CV Minutes: {(end - init) / 60}')\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/mlforecast/forecast.py:576: UserWarning:\n\nExcuting `cross_validation` after `fit` can produce unexpected errors\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/mlforecast/forecast.py:468: UserWarning:\n\nPlease rerun the `fit` method passing a proper value to prediction intervals to compute them.\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/mlforecast/forecast.py:468: UserWarning:\n\nPlease rerun the `fit` method passing a proper value to prediction intervals to compute them.\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/mlforecast/forecast.py:468: UserWarning:\n\nPlease rerun the `fit` method passing a proper value to prediction intervals to compute them.\n\n\n\nCV Minutes: 2.961174162228902\n\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncv_mlf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n0.435674\n0.556261\n-0.312492\n\n\n1\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n1.0\n0.639676\n0.625806\n-0.041924\n\n\n2\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n1.0\n0.792989\n0.659650\n0.263699\n\n\n3\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n0.806868\n0.535121\n0.482491\n\n\n4\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n0.829106\n0.313353\n0.677326"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#neuralforecast-1",
    "href": "docs/tutorials/statisticalneuralmethods.html#neuralforecast-1",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "NeuralForecast",
    "text": "NeuralForecast\nThis machine doesn‚Äôt have GPU, but Google Colabs offers some for free.\nUsing Colab‚Äôs GPU to train NeuralForecast.\n\ncv_nf_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/cross-validation-nf.parquet')\n\n\ncv_nf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-hi-90\nAutoTFT\nAutoTFT-lo-90\nAutoTFT-hi-90\ny\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0\n\n\n1\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n1.0\n\n\n2\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n1.0\n\n\n3\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0\n\n\n4\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#merge-cross-validation-forecasts",
    "href": "docs/tutorials/statisticalneuralmethods.html#merge-cross-validation-forecasts",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Merge cross validation forecasts",
    "text": "Merge cross validation forecasts\n\ncv_df = cv_df.merge(cv_nf_df.drop(columns=['y']), how='left', on=['unique_id', 'ds', 'cutoff'])\ncv_df = cv_df.merge(cv_mlf_df.drop(columns=['y']), how='left', on=['unique_id', 'ds', 'cutoff'])"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#plots-cv",
    "href": "docs/tutorials/statisticalneuralmethods.html#plots-cv",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Plots CV",
    "text": "Plots CV\n\ncutoffs = cv_df['cutoff'].unique()\n\n\nfor cutoff in cutoffs:\n    img = sf.plot(\n        Y_df, \n        cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n        max_insample_length=28 * 5, \n        unique_ids=['FOODS_3_001_CA_1'],\n    )\n    img.show()\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\nAggregate Demand\n\nagg_cv_df = cv_df.loc[:,~cv_df.columns.str.contains('hi|lo')].groupby(['ds', 'cutoff']).sum(numeric_only=True).reset_index()\nagg_cv_df.insert(0, 'unique_id', 'agg_demand')\n\n\nagg_Y_df = Y_df.groupby(['ds']).sum(numeric_only=True).reset_index()\nagg_Y_df.insert(0, 'unique_id', 'agg_demand')\n\n\nfor cutoff in cutoffs:\n    img = sf.plot(\n        agg_Y_df, \n        agg_cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']),\n        max_insample_length=28 * 5,\n    )\n    img.show()"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#evaluation-per-series-and-cv-window",
    "href": "docs/tutorials/statisticalneuralmethods.html#evaluation-per-series-and-cv-window",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Evaluation per series and CV window",
    "text": "Evaluation per series and CV window\nIn this section, we will evaluate the performance of each model for each time series and each cross validation window. Since we have many combinations, we will use dask to parallelize the evaluation. The parallelization will be done using fugue.\n\nfrom typing import List, Callable\n\nfrom distributed import Client\nfrom fugue import transform\nfrom fugue_dask import DaskExecutionEngine\nfrom datasetsforecast.losses import mse, mae, smape\n\nThe evaluate function receives a unique combination of a time series and a window, and calculates different metrics for each model in df.\n\ndef evaluate(df: pd.DataFrame, metrics: List[Callable]) -&gt; pd.DataFrame:\n    eval_ = {}\n    models = df.loc[:, ~df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\n    for model in models:\n        eval_[model] = {}\n        for metric in metrics:\n            eval_[model][metric.__name__] = metric(df['y'], df[model])\n    eval_df = pd.DataFrame(eval_).rename_axis('metric').reset_index()\n    eval_df.insert(0, 'cutoff', df['cutoff'].iloc[0])\n    eval_df.insert(0, 'unique_id', df['unique_id'].iloc[0])\n    return eval_df\n\n\nstr_models = cv_df.loc[:, ~cv_df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\nstr_models = ','.join([f\"{model}:float\" for model in str_models])\ncv_df['cutoff'] = cv_df['cutoff'].astype(str)\ncv_df['unique_id'] = cv_df['unique_id'].astype(str)\n\nLet‚Äôs cleate a dask client.\n\nclient = Client() # without this, dask is not in distributed mode\n# fugue.dask.dataframe.default.partitions determines the default partitions for a new DaskDataFrame\nengine = DaskExecutionEngine({\"fugue.dask.dataframe.default.partitions\": 96})\n\nThe transform function takes the evaluate functions and applies it to each combination of time series (unique_id) and cross validation window (cutoff) using the dask client we created before.\n\nevaluation_df = transform(\n    cv_df.loc[:, ~cv_df.columns.str.contains('lo|hi')], \n    evaluate, \n    engine=\"dask\",\n    params={'metrics': [mse, mae, smape]}, \n    schema=f\"unique_id:str,cutoff:str,metric:str, {str_models}\", \n    as_local=True,\n    partition={'by': ['unique_id', 'cutoff']}\n)\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/distributed/client.py:3109: UserWarning:\n\nSending large graph of size 49.63 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n\n\n\n\nevaluation_df.head()\n\n\n\n\n\n\n\n\nunique_id\ncutoff\nmetric\nSeasonalNaive\nNaive\nHistoricAverage\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoNHITS\nAutoTFT\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\nFOODS_3_003_WI_3\n2016-02-28\nmse\n1.142857\n1.142857\n0.816646\n0.816471\n1.142857\n1.142857\n1.142857\n1.142857\n1.142857\n0.832010\n1.020361\n0.887121\n\n\n1\nFOODS_3_003_WI_3\n2016-02-28\nmae\n0.571429\n0.571429\n0.729592\n0.731261\n0.571429\n0.571429\n0.571429\n0.571429\n0.571429\n0.772788\n0.619949\n0.685413\n\n\n2\nFOODS_3_003_WI_3\n2016-02-28\nsmape\n71.428574\n71.428574\n158.813507\n158.516235\n200.000000\n200.000000\n200.000000\n71.428574\n71.428574\n145.901947\n188.159164\n178.883743\n\n\n3\nFOODS_3_013_CA_3\n2016-04-24\nmse\n4.000000\n6.214286\n2.406764\n3.561202\n2.267853\n2.267600\n2.268677\n2.750000\n2.125000\n2.160508\n2.370228\n2.289606\n\n\n4\nFOODS_3_013_CA_3\n2016-04-24\nmae\n1.500000\n2.142857\n1.214286\n1.340446\n1.214286\n1.214286\n1.214286\n1.107143\n1.142857\n1.140084\n1.157548\n1.148813\n\n\n\n\n\n\n\n\n# Calculate the mean metric for each cross validation window\nevaluation_df.groupby(['cutoff', 'metric']).mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nSeasonalNaive\nNaive\nHistoricAverage\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoNHITS\nAutoTFT\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\ncutoff\nmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-02-28\nmae\n1.744289\n2.040496\n1.730704\n1.633017\n1.527965\n1.528772\n1.497553\n1.434938\n1.485419\n1.688403\n1.514102\n1.576320\n\n\nmse\n14.510710\n19.080585\n12.858994\n11.785032\n11.114497\n11.100909\n10.347847\n10.010982\n10.964664\n10.436206\n10.968788\n10.792831\n\n\nsmape\n85.202042\n87.719086\n125.418488\n124.749908\n127.591858\n127.704102\n127.790672\n79.132614\n80.983368\n118.489983\n140.420578\n127.043137\n\n\n2016-03-27\nmae\n1.795973\n2.106449\n1.754029\n1.662087\n1.570701\n1.572741\n1.535301\n1.432412\n1.502393\n1.712493\n1.600193\n1.601612\n\n\nmse\n14.810259\n26.044472\n12.804104\n12.020620\n12.083861\n12.120033\n11.315013\n9.445867\n10.762877\n10.723589\n12.924312\n10.943772\n\n\nsmape\n87.407471\n89.453247\n123.587196\n123.460030\n123.428459\n123.538521\n123.612991\n79.926781\n82.013168\n116.089699\n138.885941\n127.304871\n\n\n2016-04-24\nmae\n1.785983\n1.990774\n1.762506\n1.609268\n1.527627\n1.529721\n1.501820\n1.447401\n1.505127\n1.692946\n1.541845\n1.590985\n\n\nmse\n13.476350\n16.234917\n13.151311\n10.647048\n10.072225\n10.062395\n9.393439\n9.363891\n10.436214\n10.347073\n10.774202\n10.608137\n\n\nsmape\n89.238815\n90.685867\n121.124947\n119.721245\n120.325401\n120.345284\n120.649582\n81.402748\n83.614029\n113.334198\n136.755234\n124.618622\n\n\n\n\n\n\n\nResults showed in previous experiments.\n\n\n\nmodel\nMSE\n\n\n\n\nMQCNN\n10.09\n\n\nDeepAR-student_t\n10.11\n\n\nDeepAR-lognormal\n30.20\n\n\nDeepAR\n9.13\n\n\nNPTS\n11.53\n\n\n\nTop 3 models: DeepAR, AutoNHITS, AutoETS.\n\nDistribution of errors\n\n!pip install seaborn\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nevaluation_df_melted = pd.melt(evaluation_df, id_vars=['unique_id', 'cutoff', 'metric'], var_name='model', value_name='error')\n\n\nSMAPE\n\nsns.violinplot(evaluation_df_melted.query('metric==\"smape\"'), x='error', y='model')\n\n&lt;Axes: xlabel='error', ylabel='model'&gt;\n\n\n\n\n\n\n\n\nChoose models for groups of series\nFeature:\n\nA unified dataframe with forecasts for all different models\nEasy Ensamble\nE.g. Average predictions\nOr MinMax (Choosing is ensembling)\n\n\n# Choose the best model for each time series, metric, and cross validation window\nevaluation_df['best_model'] = evaluation_df.idxmin(axis=1, numeric_only=True)\n# count how many times a model wins per metric and cross validation window\ncount_best_model = evaluation_df.groupby(['cutoff', 'metric', 'best_model']).size().rename('n').to_frame().reset_index()\n# plot results\nsns.barplot(count_best_model, x='n', y='best_model', hue='metric')\n\n&lt;Axes: xlabel='n', ylabel='best_model'&gt;\n\n\n\n\n\n\n\nEt pluribus unum: an inclusive forecasting Pie.\n\n# For the mse, calculate how many times a model wins\neval_series_df = evaluation_df.query('metric == \"mse\"').groupby(['unique_id']).mean(numeric_only=True)\neval_series_df['best_model'] = eval_series_df.idxmin(axis=1)\ncounts_series = eval_series_df.value_counts('best_model')\nplt.pie(counts_series, labels=counts_series.index, autopct='%.0f%%')\nplt.show()\n\n\n\n\n\nsf.plot(Y_df, cv_df.drop(columns=['cutoff', 'y']), \n        max_insample_length=28 * 6, \n        models=['AutoNHITS'],\n        unique_ids=eval_series_df.query('best_model == \"AutoNHITS\"').index[:8])"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html",
    "href": "docs/tutorials/intermittentdata.html",
    "title": "Intermittent or Sparse Data",
    "section": "",
    "text": "Intermittent or sparse data has very few non-zero observations. This type of data is hard to forecast because the zero values increase the uncertainty about the underlying patterns in the data. Furthermore, once a non-zero observation occurs, there can be considerable variation in its size. Intermittent time series are common in many industries, including finance, retail, transportation, and energy. Given the ubiquity of this type of series, special methods have been developed to forecast them. The first was from Croston (1972), followed by several variants and by different aggregation frameworks.\nStatsForecast has implemented several models to forecast intermittent time series. By the end of this tutorial, you‚Äôll have a good understanding of these models and how to use them.\nOutline:\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#install-libraries",
    "href": "docs/tutorials/intermittentdata.html#install-libraries",
    "title": "Intermittent or Sparse Data",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#load-and-explore-the-data",
    "href": "docs/tutorials/intermittentdata.html#load-and-explore-the-data",
    "title": "Intermittent or Sparse Data",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we‚Äôll use a subset of the M5 Competition dataset. Each time series represents the unit sales of a particular product in a given Walmart store. At this level (product-store), most of the data is intermittent. We first need to import the data, and to do that, we‚Äôll need datasetsforecast.\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.m5 import M5\n\nThe function to load the data is M5.load(). It requieres the following argument: - directory: (str) The directory where the data will be downloaded.\nThis function returns multiple outputs, but only the first one with the unit sales is needed.\n\ndf_total, *_ = M5.load('./data')\n\n\ndf_total.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_1_001_CA_1\n2011-01-29\n3.0\n\n\n1\nFOODS_1_001_CA_1\n2011-01-30\n0.0\n\n\n2\nFOODS_1_001_CA_1\n2011-01-31\n0.0\n\n\n3\nFOODS_1_001_CA_1\n2011-02-01\n1.0\n\n\n4\nFOODS_1_001_CA_1\n2011-02-02\n4.0\n\n\n\n\n\n\n\nFrom this dataset, we‚Äôll select the first 8 time series. You can select any number you want by changing the value of n_series.\n\nn_series = 8 \nuids = df_total['unique_id'].unique()[:n_series]\ndf = df_total.query('unique_id in @uids')\n\nWe can plot these series using the plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: (bool = True) Plots the time series randomly.\nmax_insample_length: (int) The maximum number of train/insample observations to be plotted.\nengine: (str = plotly). The library used to generate the plots. It can also be matplotlib for static plots.\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(df, plot_random = False, max_insample_length = 100)\n\n\n                                                \n\n\nHere we only plotted the last 100 observations, but we can visualize the complete history by removing max_insample_length. From these plots, we can confirm that the data is indeed intermittent since it has multiple periods with zero sales. In fact, in all cases but one, the median value is zero.\n\ndf.groupby('unique_id')[['y']].median().query('unique_id in @uids')\n\n\n\n\n\n\n\n\ny\n\n\nunique_id\n\n\n\n\n\nFOODS_1_001_CA_1\n0.0\n\n\nFOODS_1_001_CA_2\n1.0\n\n\nFOODS_1_001_CA_3\n0.0\n\n\nFOODS_1_001_CA_4\n0.0\n\n\nFOODS_1_001_TX_1\n0.0\n\n\nFOODS_1_001_TX_2\n0.0\n\n\nFOODS_1_001_TX_3\n0.0\n\n\nFOODS_1_001_WI_1\n0.0"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#train-models-for-intermittent-data",
    "href": "docs/tutorials/intermittentdata.html#train-models-for-intermittent-data",
    "title": "Intermittent or Sparse Data",
    "section": "Train models for intermittent data",
    "text": "Train models for intermittent data\nBefore training any model, we need to separate the data in a train and a test set. The M5 Competition used the last 28 days as test set, so we‚Äôll do the same.\n\ndates = df['ds'].unique()[-28:] # last 28 days\n\ntrain = df.query('ds not in @dates')\ntest = df.query('ds in @dates')\n\nStatsForecast has efficient implementations of multiple models for intermittent data. The complete list of models available is here. In this notebook, we‚Äôll use:\n\nAgregate-Dissagregate Intermittent Demand Approach (ADIDA)\nCroston Classic\nIntermittent Multiple Aggregation Prediction Algorithm (IMAPA)\nTeunter-Syntetos-Babai (TSB)\n\nTo use these models, we first need to import them from statsforecast.models and then we need to instantiate them.\n\nfrom statsforecast.models import (\n    ADIDA,\n    CrostonClassic, \n    IMAPA, \n    TSB\n)\n\n# Create a list of models and instantiation parameters \nmodels = [\n    ADIDA(), \n    CrostonClassic(), \n    IMAPA(), \n    TSB(alpha_d = 0.2, alpha_p = 0.2)\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\n\nfreq: A string indicating the frequency of the data. See pandas‚Äô available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df = train, \n    models = models, \n    freq = 'D', \n    n_jobs = -1\n)\n\nNow we‚Äôre ready to generate the forecast. To do this, we‚Äôll use the forecast method, which requires the forecasting horizon (in this case, 28 days) as argument.\nThe models for intermittent series that are currently available in StatsForecast can only generate point-forecasts. If prediction intervals are needed, then a probabilisitic model should be used.\n\nhorizon = 28 \nforecasts = sf.forecast(h=horizon)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nADIDA\nCrostonClassic\nIMAPA\nTSB\n\n\n\n\n0\nFOODS_1_001_CA_1\n2016-05-23\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n1\nFOODS_1_001_CA_1\n2016-05-24\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n2\nFOODS_1_001_CA_1\n2016-05-25\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n3\nFOODS_1_001_CA_1\n2016-05-26\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n4\nFOODS_1_001_CA_1\n2016-05-27\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n\n\n\n\n\nFinally, we‚Äôll merge the forecast with the actual values.\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#plot-forecasts-and-compute-accuracy",
    "href": "docs/tutorials/intermittentdata.html#plot-forecasts-and-compute-accuracy",
    "title": "Intermittent or Sparse Data",
    "section": "Plot forecasts and compute accuracy",
    "text": "Plot forecasts and compute accuracy\nWe can generate plots using the plot described above.\n\nStatsForecast.plot(train, test, plot_random = False, max_insample_length = 100)\n\n\n                                                \n\n\nTo compute the accuracy of the forecasts, we‚Äôll use the Mean Average Error (MAE), which is the sum of the absolute errors divided by the number of forecasts. We‚Äôll create a function to compute the MAE, and for that, we‚Äôll need to import numpy.\n\nimport numpy as np \n\ndef mae(y_hat, y_true):\n    return np.mean(np.abs(y_hat-y_true))\n\n\ny_true = test['y'].values\nadida_preds = test['ADIDA'].values\ncroston_preds = test['CrostonClassic'].values\nimapa_preds = test['IMAPA'].values\ntsb_preds = test['TSB'].values\n\nprint('ADIDA MAE: \\t %0.3f' % mae(adida_preds, y_true))\nprint('Croston Classic MAE: \\t %0.3f' % mae(croston_preds, y_true))\nprint('IMAPA MAE: \\t %0.3f' % mae(imapa_preds, y_true))\nprint('TSB   MAE: \\t %0.3f' % mae(tsb_preds, y_true))\n\nADIDA MAE:   0.949\nCroston Classic MAE:     0.944\nIMAPA MAE:   0.957\nTSB   MAE:   1.023\n\n\nHence, on average, the forecasts are one unit off."
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#references",
    "href": "docs/tutorials/intermittentdata.html#references",
    "title": "Intermittent or Sparse Data",
    "section": "References",
    "text": "References\nCroston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303."
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html",
    "href": "docs/tutorials/crossvalidation.html",
    "title": "Cross validation",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#introduction",
    "href": "docs/tutorials/crossvalidation.html#introduction",
    "title": "Cross validation",
    "section": "Introduction",
    "text": "Introduction\nTime series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it.\n\nStatsforecast has an implementation of time series cross-validation that is fast and easy to use. This implementation makes cross-validation a distributed operation, which makes it less time-consuming. In this notebook, we‚Äôll use it on a subset of the M4 Competition hourly dataset.\nOutline:\n\nInstall libraries\nLoad and explore data\nTrain model\nPerform time series cross-validation\nEvaluate results\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#install-libraries",
    "href": "docs/tutorials/crossvalidation.html#install-libraries",
    "title": "Cross validation",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages with pip install statsforecast\n\npip install statsforecast\n\n\nfrom statsforecast import StatsForecast # required to instantiate StastForecast object and use cross-validation method"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#load-and-explore-the-data",
    "href": "docs/tutorials/crossvalidation.html#load-and-explore-the-data",
    "title": "Cross validation",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nAs stated in the introduction, we‚Äôll use the M4 Competition hourly dataset. We‚Äôll first import the data from an URL using pandas.\n\nimport pandas as pd \n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet') # load the data \nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nThe input to StatsForecast is a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int, or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS.\nThe y (numeric) represents the measurement we wish to forecast.\n\nThe data in this example already has this format, so no changes are needed.\nTo keep the time required to execute this notebook to a minimum, we‚Äôll only use one time series from the data, namely the one with unique_id == 'H1'. However, you can use as many as you want, with no additional changes to the code needed.\n\ndf = Y_df[Y_df['unique_id'] == 'H1'] # select time series\n\nWe can plot the time series we‚Äôll work with using StatsForecast.plot method.\n\nStatsForecast.plot(df)"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#train-model",
    "href": "docs/tutorials/crossvalidation.html#train-model",
    "title": "Cross validation",
    "section": "Train model",
    "text": "Train model\nFor this example, we‚Äôll use StastForecast AutoETS. We first need to import it from statsforecast.models and then we need to instantiate a new StatsForecast object.\nThe StatsForecast object has the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. See panda‚Äôs available frequencies.\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame df.\n\nfrom statsforecast.models import AutoETS\n\nmodels = [AutoETS(season_length = 24)]\n\nsf = StatsForecast(\n    df = df, \n    models = models, \n    freq = 'H', \n    n_jobs = -1\n)"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#perform-time-series-cross-validation",
    "href": "docs/tutorials/crossvalidation.html#perform-time-series-cross-validation",
    "title": "Cross validation",
    "section": "Perform time series cross-validation",
    "text": "Perform time series cross-validation\nOnce the StatsForecastobject has been instantiated, we can use the cross_validation method, which takes the following arguments:\n\ndf: training data frame with StatsForecast format\nh (int): represents the h steps into the future that will be forecasted\nstep_size (int): step size between each window, meaning how often do you want to run the forecasting process.\nn_windows (int): number of windows used for cross-validation, meaning the number of forecasting processes in the past you want to evaluate.\n\nFor this particular example, we‚Äôll use 3 windows of 24 hours.\n\ncrossvalidation_df = sf.cross_validation(\n    df = df,\n    h = 24,\n    step_size = 24,\n    n_windows = 3\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoETS\n\n\nunique_id\n\n\n\n\n\n\n\n\nH1\n677\n676\n691.0\n677.761047\n\n\nH1\n678\n676\n618.0\n607.817871\n\n\nH1\n679\n676\n563.0\n569.437744\n\n\nH1\n680\n676\n529.0\n537.340027\n\n\nH1\n681\n676\n504.0\n515.571106\n\n\n\n\n\n\n\nWe‚Äôll now plot the forecast for each cutoff period. To make the plots clearer, we‚Äôll rename the actual values in each period.\n\ncrossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True) # rename actual values \n\ncutoff = crossvalidation_df['cutoff'].unique()\n\nfor k in range(len(cutoff)): \n    cv = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff[k]]\n    StatsForecast.plot(df, cv.loc[:, cv.columns != 'cutoff'])\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nNotice that in each cutoff period, we generated a forecast for the next 24 hours using only the data y before said period."
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#evaluate-results",
    "href": "docs/tutorials/crossvalidation.html#evaluate-results",
    "title": "Cross validation",
    "section": "Evaluate results",
    "text": "Evaluate results\nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\npip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\n\nThe forecasts, in this case, AutoETS.\n\n\nrmse = rmse(crossvalidation_df['actual'], crossvalidation_df['AutoETS'])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  33.90342\n\n\nThis measure should better reflect the predictive abilities of our model, since it used different time periods to test its accuracy.\n\n\n\n\n\n\nTip\n\n\n\nCross validation is especially useful when comparing multiple models. Here‚Äôs an example with multiple models and time series."
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#references",
    "href": "docs/tutorials/crossvalidation.html#references",
    "title": "Cross validation",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù."
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html",
    "href": "docs/tutorials/multipleseasonalities.html",
    "title": "Multiple seasonalities",
    "section": "",
    "text": "Tip\n\n\n\nFor this task, StatsForecast‚Äôs MSTL is 68% more accurate and 600% faster than Prophet and NeuralProphet. (Reproduce experiments here)\nMultiple seasonal data refers to time series that have more than one clear seasonality. Multiple seasonality is traditionally present in data that is sampled at a low frequency. For example, hourly electricity data exhibits daily and weekly seasonality. That means that there are clear patterns of electricity consumption for specific hours of the day like 6:00pm vs 3:00am or for specific days like Sunday vs Friday.\nTraditional statistical models are not able to model more than one seasonal length. In this example, we will show how to model the two seasonalities efficiently using Multiple Seasonal-Trend decompositions with LOESS (MSTL).\nFor this example, we will use hourly electricity load data from Pennsylvania, New Jersey, and Maryland (PJM). The original data can be found here. (Click here for info on PJM)\nFirst, we will load the data, then we will use the StatsForecast.fit and StatsForecast.predict methods to predict the next 24 hours. We will then decompose the different elements of the time series into trends and its multiple seasonalities. At the end, you will use the StatsForecast.forecast for production-ready forecasting.\nOutline\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#install-libraries",
    "href": "docs/tutorials/multipleseasonalities.html#install-libraries",
    "title": "Multiple seasonalities",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast ``"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#load-data",
    "href": "docs/tutorials/multipleseasonalities.html#load-data",
    "title": "Multiple seasonalities",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nYou will read the data with pandas and change the necessary names. This step should take around 2s.\n\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/jnagura/Energy-consumption-prediction-analysis/master/PJM_Load_hourly.csv')\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf.tail()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n32891\nPJM_Load_hourly\n2001-01-01 20:00:00\n35209.0\n\n\n32892\nPJM_Load_hourly\n2001-01-01 21:00:00\n34791.0\n\n\n32893\nPJM_Load_hourly\n2001-01-01 22:00:00\n33669.0\n\n\n32894\nPJM_Load_hourly\n2001-01-01 23:00:00\n31809.0\n\n\n32895\nPJM_Load_hourly\n2001-01-02 00:00:00\n29506.0\n\n\n\n\n\n\n\nStatsForecast can handle unsorted data, however, for plotting purposes, it is convenient to sort the data frame.\n\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA. In this case, it will print just one series given that we have just one unique_id.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n/Users/max.mergenthaler/Nixtla/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n                                                \n\n\nThe time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods."
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#fit-an-mstl-model",
    "href": "docs/tutorials/multipleseasonalities.html#fit-an-mstl-model",
    "title": "Multiple seasonalities",
    "section": "Fit an MSTL model",
    "text": "Fit an MSTL model\nThe MSTL (Multiple Seasonal-Trend decompositions using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a non-seasonal model and each seasonality using a SeasonalNaive model. You can choose the non-seasonal model you want to use to forecast the trend component of the MSTL model. In this example, we will use an AutoARIMA.\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] for season length. The trend component will be forecasted with an AutoARIMA model. (You can also try with: AutoTheta, AutoCES, and AutoETS)\n\n# Create a list of models and instantiation parameters\n\nmodels = [MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA() # model used to forecast trend\n)]\n\nWe fit the models by instantiating a new StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(\n    models=models, # model used to fit each time series \n    freq='H', # frequency of the data\n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nUse the fit method to fit each model to each time series. In this case, we are just fitting one model to one series. Check this guide to learn how to fit many models to many series.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 10 seconds. The second time -once Numba compiled your settings- it should take less than 5s.\n\n\n\nsf = sf.fit(df=df)"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#decompose-the-series",
    "href": "docs/tutorials/multipleseasonalities.html#decompose-the-series",
    "title": "Multiple seasonalities",
    "section": "Decompose the series",
    "text": "Decompose the series\nOnce the model is fitted, access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case, we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n\n\n\n\n\ndata\ntrend\nseasonal24\nseasonal168\nremainder\n\n\n\n\n0\n22259.0\n26183.898892\n-5215.124554\n609.000432\n681.225229\n\n\n1\n21244.0\n26181.599305\n-6255.673234\n603.823918\n714.250011\n\n\n2\n20651.0\n26179.294886\n-6905.329895\n636.820423\n740.214587\n\n\n3\n20421.0\n26176.985472\n-7073.420118\n615.825999\n701.608647\n\n\n4\n20713.0\n26174.670877\n-7062.395760\n991.521912\n609.202971\n\n\n...\n...\n...\n...\n...\n...\n\n\n32891\n36392.0\n33123.552727\n4387.149171\n-488.177882\n-630.524015\n\n\n32892\n35082.0\n33148.242575\n3479.852929\n-682.928737\n-863.166767\n\n\n32893\n33890.0\n33172.926165\n2307.808829\n-650.566775\n-940.168219\n\n\n32894\n32590.0\n33197.603322\n748.587723\n-555.177849\n-801.013195\n\n\n32895\n31569.0\n33222.273902\n-967.124123\n-265.895357\n-420.254422\n\n\n\n\n32896 rows √ó 5 columns\n\n\n\nWe will use matplotlib, to visualize the different components of the series.\n\nimport matplotlib.pyplot as plt\n\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe observe a clear upward trend (orange line) and seasonality repeating every day (24H) and every week (168H)."
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#predict-the-next-24-hours",
    "href": "docs/tutorials/multipleseasonalities.html#predict-the-next-24-hours",
    "title": "Multiple seasonalities",
    "section": "Predict the next 24 hours",
    "text": "Predict the next 24 hours\n\nProbabilistic forecasting with levels\n\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2002-01-01 01:00:00\n29956.744141\n29585.187500\n30328.298828\n\n\nPJM_Load_hourly\n2002-01-01 02:00:00\n29057.691406\n28407.498047\n29707.884766\n\n\nPJM_Load_hourly\n2002-01-01 03:00:00\n28654.699219\n27767.101562\n29542.298828\n\n\nPJM_Load_hourly\n2002-01-01 04:00:00\n28499.009766\n27407.640625\n29590.378906\n\n\nPJM_Load_hourly\n2002-01-01 05:00:00\n28821.716797\n27552.236328\n30091.197266\n\n\n\n\n\n\n\nYou can plot the forecast by calling the StatsForecast.plot method and passing in your forecast dataframe.\n\nsf.plot(df, forecasts, max_insample_length=24 * 7)"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#forecast-in-production",
    "href": "docs/tutorials/multipleseasonalities.html#forecast-in-production",
    "title": "Multiple seasonalities",
    "section": "Forecast in production",
    "text": "Forecast in production\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 10 seconds. The second time -once Numba compiled your settings- it should take less than 5s.\n\n\n\nforecasts_df = sf.forecast(h=24, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2002-01-01 01:00:00\n29956.744141\n29585.187500\n30328.298828\n\n\nPJM_Load_hourly\n2002-01-01 02:00:00\n29057.691406\n28407.498047\n29707.884766\n\n\nPJM_Load_hourly\n2002-01-01 03:00:00\n28654.699219\n27767.101562\n29542.298828\n\n\nPJM_Load_hourly\n2002-01-01 04:00:00\n28499.009766\n27407.640625\n29590.378906\n\n\nPJM_Load_hourly\n2002-01-01 05:00:00\n28821.716797\n27552.236328\n30091.197266"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#references",
    "href": "docs/tutorials/multipleseasonalities.html#references",
    "title": "Multiple seasonalities",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). ‚ÄúMSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns‚Äù."
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#next-steps",
    "href": "docs/tutorials/multipleseasonalities.html#next-steps",
    "title": "Multiple seasonalities",
    "section": "Next Steps",
    "text": "Next Steps\n\nLearn how to use cross-validation to assess the robustness of your model."
  },
  {
    "objectID": "docs/getting-started/getting_started_short.html",
    "href": "docs/getting-started/getting_started_short.html",
    "title": "Quick Start",
    "section": "",
    "text": "StatsForecast follows the sklearn model API. For this minimal example, you will create an instance of the StatsForecast class and then call its fit and predict methods. We recommend this option if speed is not paramount and you want to explore the fitted values and parameters.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to forecast many series, we recommend using the forecast method. Check this Getting Started with multiple time series guide.\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\nAs an example, let‚Äôs look at the US Air Passengers dataset. This time series consists of monthly totals of a US airline passengers from 1949 to 1960. The CSV is available here.\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nFirst, we‚Äôll import the data:\n\nimport pandas as pd\n\n\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/air-passengers.csv')\ndf.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAirPassengers\n1949-01-01\n112\n\n\n1\nAirPassengers\n1949-02-01\n118\n\n\n2\nAirPassengers\n1949-03-01\n132\n\n\n3\nAirPassengers\n1949-04-01\n129\n\n\n4\nAirPassengers\n1949-05-01\n121\n\n\n\n\n\n\n\nWe fit the model by instantiating a new StatsForecast object with its two required parameters: https://nixtla.github.io/statsforecast/src/core/models.html * models: a list of models. Select the models you want from models and import them. For this example, we will use a AutoARIMA model. We set season_length to 12 because we expect seasonal effects every 12 months. (See: Seasonal periods)\n\nfreq: a string indicating the frequency of the data. (See pandas available frequencies.)\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nsf = StatsForecast(\n    models = [AutoARIMA(season_length = 12)],\n    freq = 'M'\n)\n\nsf.fit(df)\n\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\nforecast_df = sf.predict(h=12, level=[90]) \n\nforecast_df.tail()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nAirPassengers\n1961-07-31\n633.230774\n589.562378\n676.899170\n\n\nAirPassengers\n1961-08-31\n535.230774\n489.082153\n581.379456\n\n\nAirPassengers\n1961-09-30\n488.230804\n439.728699\n536.732910\n\n\nAirPassengers\n1961-10-31\n417.230804\n366.484253\n467.977356\n\n\nAirPassengers\n1961-11-30\n459.230804\n406.334930\n512.126648\n\n\n\n\n\n\n\nYou can plot the forecast by calling the StatsForecast.plot method and passing in your forecast dataframe.\n\ndf[\"ds\"]=pd.to_datetime(df[\"ds\"])\nsf.plot(df, forecast_df, level=[90], engine='plotly')\n\n\n                                                \n\n\n\n\n\n\n\n\nNext Steps\n\n\n\n\nBuild and end-to-end forecasting pipeline following best practices in End to End Walkthrough\nForecast millions of series in a scalable cluster in the cloud using Spark and Nixtla\nDetect anomalies in your past observations\n\n\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/autoarima_vs_prophet.html",
    "href": "docs/how-to-guides/autoarima_vs_prophet.html",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/autoarima_vs_prophet.html#motivation",
    "href": "docs/how-to-guides/autoarima_vs_prophet.html#motivation",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Motivation",
    "text": "Motivation\nThe AutoARIMA model is widely used to forecast time series in production and as a benchmark. However, the python implementation (pmdarima) is so slow that prevent data scientist practioners from quickly iterating and deploying AutoARIMA in production for a large number of time series. In this notebook we present Nixtla‚Äôs AutoARIMA based on the R implementation (developed by Rob Hyndman) and optimized using numba."
  },
  {
    "objectID": "docs/how-to-guides/autoarima_vs_prophet.html#example",
    "href": "docs/how-to-guides/autoarima_vs_prophet.html#example",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Example",
    "text": "Example\n\nLibraries\n\n# !pip install statsforecast prophet statsmodels sklearn matplotlib pmdarima\n\n\nimport logging\nimport os\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\nfrom multiprocessing import cpu_count, Pool # for prophet\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima as auto_arima_p\nfrom prophet import Prophet\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, _TS\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom sklearn.model_selection import ParameterGrid\nfrom utilsforecast.plotting import plot_series\n\n\nUseful functions\n\ndef plot_autocorrelation_grid(df_train):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) &gt;= 8, \"Must provide at least 8 ts\"\n\n    unique_ids = random.sample(list(unique_ids), k=8)\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        plot_acf(train_uid['y'].values, ax=axes[idx, idy], \n                 title=f'ACF M4 Hourly {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Autocorrelation')\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\n\n\nData\nFor testing purposes, we will use the Hourly dataset from the M4 competition.\n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\nIn this example we will use a subset of the data to avoid waiting too long. You can modify the number of series if you want.\n\nn_series = 16\nuids = train['unique_id'].unique()[:n_series]\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\n\nplot_series(train, test, max_ids=n_series)\n\n\n\n\nWould an autorregresive model be the right choice for our data? There is no doubt that we observe seasonal periods. The autocorrelation function (acf) can help us to answer the question. Intuitively, we have to observe a decreasing correlation to opt for an AR model.\n\nplot_autocorrelation_grid(train)\n\n\n\n\nThus, we observe a high autocorrelation for previous lags and also for the seasonal lags. Therefore, we will let auto_arima to handle our data.\n\n\nTraining and forecasting\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Hourly data, it would be benefitial to use 24 as seasonality.\n\n?AutoARIMA\n\n\nInit signature:\nAutoARIMA(\n    d: Optional[int] = None,\n    D: Optional[int] = None,\n    max_p: int = 5,\n    max_q: int = 5,\n    max_P: int = 2,\n    max_Q: int = 2,\n    max_order: int = 5,\n    max_d: int = 2,\n    max_D: int = 1,\n    start_p: int = 2,\n    start_q: int = 2,\n    start_P: int = 1,\n    start_Q: int = 1,\n    stationary: bool = False,\n    seasonal: bool = True,\n    ic: str = 'aicc',\n    stepwise: bool = True,\n    nmodels: int = 94,\n    trace: bool = False,\n    approximation: Optional[bool] = False,\n    method: Optional[str] = None,\n    truncate: Optional[bool] = None,\n    test: str = 'kpss',\n    test_kwargs: Optional[str] = None,\n    seasonal_test: str = 'seas',\n    seasonal_test_kwargs: Optional[Dict] = None,\n    allowdrift: bool = False,\n    allowmean: bool = False,\n    blambda: Optional[float] = None,\n    biasadj: bool = False,\n    season_length: int = 1,\n    alias: str = 'AutoARIMA',\n    prediction_intervals: Optional[statsforecast.utils.ConformalIntervals] = None,\n)\nDocstring:     \nAutoARIMA model.\nAutomatically selects the best ARIMA (AutoRegressive Integrated Moving Average)\nmodel using an information criterion. Default is Akaike Information Criterion (AICc).\n**Note:**&lt;br&gt;\nThis implementation is a mirror of Hyndman's [forecast::auto.arima](https://github.com/robjhyndman/forecast).\n**References:**&lt;br&gt;\n[Rob J. Hyndman, Yeasmin Khandakar (2008). \"Automatic Time Series Forecasting: The forecast package for R\"](https://www.jstatsoft.org/article/view/v027i03).\nParameters\n----------\nd : Optional[int]\n    Order of first-differencing.\nD : Optional[int]\n    Order of seasonal-differencing.\nmax_p : int\n    Max autorregresives p.\nmax_q : int\n    Max moving averages q.\nmax_P : int\n    Max seasonal autorregresives P.\nmax_Q : int\n    Max seasonal moving averages Q.\nmax_order : int\n    Max p+q+P+Q value if not stepwise selection.\nmax_d : int\n    Max non-seasonal differences.\nmax_D : int\n    Max seasonal differences.\nstart_p : int\n    Starting value of p in stepwise procedure.\nstart_q : int\n    Starting value of q in stepwise procedure.\nstart_P : int\n    Starting value of P in stepwise procedure.\nstart_Q : int\n    Starting value of Q in stepwise procedure.\nstationary : bool\n    If True, restricts search to stationary models.\nseasonal : bool\n    If False, restricts search to non-seasonal models.\nic : str\n    Information criterion to be used in model selection.\nstepwise : bool\n    If True, will do stepwise selection (faster).\nnmodels : int\n    Number of models considered in stepwise search.\ntrace : bool\n    If True, the searched ARIMA models is reported.\napproximation : Optional[bool]\n    If True, conditional sums-of-squares estimation, final MLE.\nmethod : Optional[str]\n    Fitting method between maximum likelihood or sums-of-squares.\ntruncate : Optional[int]\n    Observations truncated series used in model selection.\ntest : str\n    Unit root test to use. See `ndiffs` for details.\ntest_kwargs : Optional[str]\n    Unit root test additional arguments.\nseasonal_test : str\n    Selection method for seasonal differences.\nseasonal_test_kwargs : Optional[dict]\n    Seasonal unit root test arguments.\nallowdrift : bool (default True)\n    If True, drift models terms considered.\nallowmean : bool (default True)\n    If True, non-zero mean models considered.\nblambda : Optional[float]\n    Box-Cox transformation parameter.\nbiasadj : bool\n    Use adjusted back-transformed mean Box-Cox.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\nFile:           /hdd/github/statsforecast/statsforecast/models.py\nType:           type\nSubclasses:     \n\n\n\nAs we see, we can pass season_length to AutoARIMA, so the definition of our models would be,\n\nmodels = [AutoARIMA(season_length=24, approximation=True)]\n\n\nfcst = StatsForecast(df=train, \n                     models=models, \n                     freq='H', \n                     n_jobs=-1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla = end - init\ntime_nixtla\n\n40.38660216331482\n\n\n\nforecasts.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\n\n\nunique_id\n\n\n\n\n\n\nH1\n701\n616.084167\n\n\nH1\n702\n544.432129\n\n\nH1\n703\n510.414490\n\n\nH1\n704\n481.046539\n\n\nH1\n705\n460.893066\n\n\n\n\n\n\n\n\nforecasts = forecasts.reset_index()\n\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])\n\n\nplot_series(train, test)"
  },
  {
    "objectID": "docs/how-to-guides/autoarima_vs_prophet.html#alternatives",
    "href": "docs/how-to-guides/autoarima_vs_prophet.html#alternatives",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Alternatives",
    "text": "Alternatives\n\npmdarima\nYou can use the StatsForecast class to parallelize your own models. In this section we will use it to run the auto_arima model from pmdarima.\n\nclass PMDAutoARIMA(_TS):\n    \n    def __init__(self, season_length: int):\n        self.season_length = season_length\n        \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        mod = auto_arima_p(\n            y, m=self.season_length,\n            with_intercept=False #ensure comparability with Nixtla's implementation\n        ) \n        return {'mean': mod.predict(h)}\n    \n    def __repr__(self):\n        return 'pmdarima'\n\n\nn_series_pmdarima = 2\n\n\nfcst = StatsForecast(\n    df = train.query('unique_id in [\"H1\", \"H10\"]'), \n    models=[PMDAutoARIMA(season_length=24)],\n    freq='H',\n    n_jobs=-1\n)\n\n\ninit = time.time()\nforecast_pmdarima = fcst.forecast(48)\nend = time.time()\n\ntime_pmdarima = end - init\ntime_pmdarima\n\n886.2768685817719\n\n\n\nforecast_pmdarima.head()\n\n\n\n\n\n\n\n\nds\npmdarima\n\n\nunique_id\n\n\n\n\n\n\nH1\n701\n628.310547\n\n\nH1\n702\n571.659851\n\n\nH1\n703\n543.504700\n\n\nH1\n704\n517.539062\n\n\nH1\n705\n502.829559\n\n\n\n\n\n\n\n\nforecast_pmdarima = forecast_pmdarima.reset_index()\n\n\ntest = test.merge(forecast_pmdarima, how='left', on=['unique_id', 'ds'])\n\n\nplot_series(train, test, plot_random=False)\n\n\n\n\n\n\nProphet\nProphet is designed to receive a pandas dataframe, so we cannot use StatForecast. Therefore, we need to parallize from scratch.\n\nparams_grid = {'seasonality_mode': ['multiplicative','additive'],\n               'growth': ['linear', 'flat'], \n               'changepoint_prior_scale': [0.1, 0.2, 0.3, 0.4, 0.5], \n               'n_changepoints': [5, 10, 15, 20]} \ngrid = ParameterGrid(params_grid)\n\n\ndef fit_and_predict(index, ts):\n    df = ts.drop(columns='unique_id', axis=1)\n    max_ds = df['ds'].max()\n    df['ds'] = pd.date_range(start='1970-01-01', periods=df.shape[0], freq='H')\n    df_val = df.tail(48) \n    df_train = df.drop(df_val.index) \n    y_val = df_val['y'].values\n    \n    if len(df_train) &gt;= 48:\n        val_results = {'losses': [], 'params': []}\n\n        for params in grid:\n            model = Prophet(seasonality_mode=params['seasonality_mode'],\n                            growth=params['growth'],\n                            weekly_seasonality=True,\n                            daily_seasonality=True,\n                            yearly_seasonality=True,\n                            n_changepoints=params['n_changepoints'],\n                            changepoint_prior_scale=params['changepoint_prior_scale'])\n            model = model.fit(df_train)\n            \n            forecast = model.make_future_dataframe(periods=48, \n                                                   include_history=False, \n                                                   freq='H')\n            forecast = model.predict(forecast)\n            forecast['unique_id'] = index\n            forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n            \n            loss = np.mean(abs(y_val - forecast['yhat'].values))\n            \n            val_results['losses'].append(loss)\n            val_results['params'].append(params)\n\n        idx_params = np.argmin(val_results['losses']) \n        params = val_results['params'][idx_params]\n    else:\n        params = {'seasonality_mode': 'multiplicative',\n                  'growth': 'flat',\n                  'n_changepoints': 150,\n                  'changepoint_prior_scale': 0.5}\n    model = Prophet(seasonality_mode=params['seasonality_mode'],\n                    growth=params['growth'],\n                    weekly_seasonality=True,\n                    daily_seasonality=True,\n                    yearly_seasonality=True,\n                    n_changepoints=params['n_changepoints'],\n                    changepoint_prior_scale=params['changepoint_prior_scale'])\n    model = model.fit(df)\n    \n    forecast = model.make_future_dataframe(periods=48, \n                                           include_history=False, \n                                           freq='H')\n    forecast = model.predict(forecast)\n    forecast.insert(0, 'unique_id', index)\n    forecast['ds'] = np.arange(max_ds + 1, max_ds + 48 + 1)\n    forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n    \n    return forecast\n\n\ninit = time.time()\nwith Pool(cpu_count()) as pool:\n    forecast_prophet = pool.starmap(fit_and_predict, train.groupby('unique_id'))\nend = time.time()\nforecast_prophet = pd.concat(forecast_prophet).rename(columns={'yhat': 'prophet'})\ntime_prophet = end - init\ntime_prophet\n\n120.7272641658783\n\n\n\nforecast_prophet\n\n\n\n\n\n\n\n\nunique_id\nds\nprophet\n\n\n\n\n0\nH1\n701\n635.914254\n\n\n1\nH1\n702\n565.976464\n\n\n2\nH1\n703\n505.095507\n\n\n3\nH1\n704\n462.559539\n\n\n4\nH1\n705\n438.766801\n\n\n...\n...\n...\n...\n\n\n43\nH112\n744\n6184.686240\n\n\n44\nH112\n745\n6188.851888\n\n\n45\nH112\n746\n6129.306256\n\n\n46\nH112\n747\n6058.040672\n\n\n47\nH112\n748\n5991.982370\n\n\n\n\n768 rows √ó 3 columns\n\n\n\n\ntest = test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_series(train, test)\n\n\n\n\n\n\nEvaluation\n\n\nTime\nSince AutoARIMA works with numba is useful to calculate the time for just one time series.\n\nfcst = StatsForecast(df=train.query('unique_id == \"H1\"'), \n                     models=models, freq='H', \n                     n_jobs=1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla_1 = end - init\ntime_nixtla_1\n\n18.752424716949463\n\n\n\ntimes = pd.DataFrame({'n_series': np.arange(1, 414 + 1)})\ntimes['pmdarima'] = time_pmdarima * times['n_series'] / n_series_pmdarima\ntimes['prophet'] = time_prophet * times['n_series'] / n_series\ntimes['AutoARIMA_nixtla'] = time_nixtla_1 + times['n_series'] * (time_nixtla - time_nixtla_1) / n_series\ntimes = times.set_index('n_series')\n\n\ntimes.tail(5)\n\n\n\n\n\n\n\n\npmdarima\nprophet\nAutoARIMA_nixtla\n\n\nn_series\n\n\n\n\n\n\n\n410\n181686.758059\n3093.636144\n573.128222\n\n\n411\n182129.896494\n3101.181598\n574.480358\n\n\n412\n182573.034928\n3108.727052\n575.832494\n\n\n413\n183016.173362\n3116.272506\n577.184630\n\n\n414\n183459.311796\n3123.817960\n578.536766\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize = (24, 7))\n(times/3600).plot(ax=axes[0], linewidth=4)\nnp.log10(times).plot(ax=axes[1], linewidth=4)\naxes[0].set_title('Time across models [Hours]', fontsize=22)\naxes[1].set_title('Time across models [Log10 Scale]', fontsize=22)\naxes[0].set_ylabel('Time [Hours]', fontsize=20)\naxes[1].set_ylabel('Time Seconds [Log10 Scale]', fontsize=20)\nfig.suptitle('Time comparison using M4-Hourly data', fontsize=27)\nfor ax in axes:\n    ax.set_xlabel('Number of Time Series [N]', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid()\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n        label.set_fontsize(20)\n\n\n\n\n\nfig.savefig('computational-efficiency.png', dpi=300)\n\n\n\nPerformance\n\npmdarima (only two time series)\n\nname_models = test.drop(columns=['unique_id', 'ds', 'y_test']).columns.tolist()\n\n\ntest_pmdarima = test.query('unique_id in [\"H1\", \"H10\"]')\neval_pmdarima = []\nfor model in name_models:\n    mae = np.mean(abs(test_pmdarima[model] - test_pmdarima['y_test']))\n    eval_pmdarima.append({'model': model, 'mae': mae})\npd.DataFrame(eval_pmdarima).sort_values('mae')\n\n\n\n\n\n\n\n\nmodel\nmae\n\n\n\n\n0\nAutoARIMA\n20.289669\n\n\n1\npmdarima\n24.676279\n\n\n2\nprophet\n39.201933\n\n\n\n\n\n\n\n\n\nProphet\n\neval_prophet = []\nfor model in name_models:\n    if 'pmdarima' in model:\n        continue\n    mae = np.mean(abs(test[model] - test['y_test']))\n    eval_prophet.append({'model': model, 'mae': mae})\npd.DataFrame(eval_prophet).sort_values('mae')\n\n\n\n\n\n\n\n\nmodel\nmae\n\n\n\n\n0\nAutoARIMA\n680.202965\n\n\n1\nprophet\n1058.578963\n\n\n\n\n\n\n\nFor a complete comparison check the complete experiment."
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html",
    "href": "docs/how-to-guides/amazonstatsforecast.html",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "",
    "text": "We will make use of the M5 competition dataset provided by Walmart. This dataset is interesting for its scale but also the fact that it features many timeseries with infrequent occurances. Such timeseries are common in retail scenarios and are difficult for traditional timeseries forecasting techniques to address.\nThe data are ready for download at the following URLs:\n\nTrain set: https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet\nTemporal exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/temporal.parquet\nStatic exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/static.parquet\n\nA more detailed description of the data can be found here.\n\n\n\n\n\n\nWarning\n\n\n\nThe M5 competition is hierarchical. That is, forecasts are required for different levels of aggregation: national, state, store, etc. In this experiment, we only generate forecasts using the bottom-level data. The evaluation is performed using the bottom-up reconciliation method to obtain the forecasts for the higher hierarchies.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html#data",
    "href": "docs/how-to-guides/amazonstatsforecast.html#data",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "",
    "text": "We will make use of the M5 competition dataset provided by Walmart. This dataset is interesting for its scale but also the fact that it features many timeseries with infrequent occurances. Such timeseries are common in retail scenarios and are difficult for traditional timeseries forecasting techniques to address.\nThe data are ready for download at the following URLs:\n\nTrain set: https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet\nTemporal exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/temporal.parquet\nStatic exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/static.parquet\n\nA more detailed description of the data can be found here.\n\n\n\n\n\n\nWarning\n\n\n\nThe M5 competition is hierarchical. That is, forecasts are required for different levels of aggregation: national, state, store, etc. In this experiment, we only generate forecasts using the bottom-level data. The evaluation is performed using the bottom-up reconciliation method to obtain the forecasts for the higher hierarchies."
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html#amazon-forecast",
    "href": "docs/how-to-guides/amazonstatsforecast.html#amazon-forecast",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Amazon Forecast",
    "text": "Amazon Forecast\nAmazon Forecast is a fully automated solution for time series forecasting. The solution can take the time series to forecast and exogenous variables (temporal and static). For this experiment, we used the AutoPredict functionality of Amazon Forecast following the steps of this tutorial. A detailed description of the particular steps for this dataset can be found here.\nAmazon Forecast creates predictors with AutoPredictor, which involves applying the optimal combination of algorithms to each time series in your datasets. The predictor is an Amazon Forecast model that is trained using your target time series, related time series, item metadata, and any additional datasets you include.\nIncluded algorithms range from commonly used statistical algorithms like Autoregressive Integrated Moving Average (ARIMA), to complex neural network algorithms like CNN-QR and DeepAR+.: CNN-QR, DeepAR+, Prophet, NPTS, ARIMA, and ETS.\nTo leverage the probabilistic features of Amazon Forecast and enable confidence intervals for further analysis we forecasted the following quantiles: 0.1 | 0.5 | 0.9.\nThe full pipeline of Amazon Forecast took 4.1 hours and the results can be found here: s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet"
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html#nixtlas-statsforecast",
    "href": "docs/how-to-guides/amazonstatsforecast.html#nixtlas-statsforecast",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Nixtla‚Äôs StatsForecast",
    "text": "Nixtla‚Äôs StatsForecast\n\nInstall necessary libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS. (If you don‚Äôt want to use a cloud storage provider, you can read your files locally using pandas)\n\n!pip install statsforecast s3fs\n\n\n\nInput format\nWe will use pandas to read the data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nSo we will rename the original columns to make it compatible with StatsForecast.\nDepending on your internet connection, this step should take around 20 seconds.\n\n\n\n\n\n\nWarning\n\n\n\nWe are reading a file from S3, so you need to install the s3fs library. To install it, run ! pip install s3fs\n\n\n\n\nRead data\n\nimport pandas as pd\n\nY_df_m5 = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet') \n\nY_df_m5 = Y_df_m5.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n}) \n\nY_df_m5.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_1_001_CA_1\n2011-01-29\n3.0\n\n\n1\nFOODS_1_001_CA_1\n2011-01-30\n0.0\n\n\n2\nFOODS_1_001_CA_1\n2011-01-31\n0.0\n\n\n3\nFOODS_1_001_CA_1\n2011-02-01\n1.0\n\n\n4\nFOODS_1_001_CA_1\n2011-02-02\n4.0\n\n\n\n\n\n\n\n\n\nTrain statistical models\nWe fit the model by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them. For this example, we will use AutoETS and DynamicOptimizedTheta. We set season_length to 7 because we expect seasonal effects every week. (See: Seasonal periods)\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\n\nAutoETS: Exponential Smoothing model. Automatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Ref: AutoETS.\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive.\nDynamicOptimizedTheta: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Ref: DynamicOptimizedTheta.\n\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import (\n    AutoETS,\n    DynamicOptimizedTheta,\n    SeasonalNaive\n)\n\n# Create list of models\nmodels = [\n    AutoETS(season_length=7),\n    DynamicOptimizedTheta(season_length=7),\n]\n\n# Instantiate StatsForecast class\nsf = StatsForecast( \n    models=models,\n    freq='D', \n    n_jobs=-1,\n    fallback_model=SeasonalNaive(season_length=7)\n)\n\n/home/ubuntu/fede/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\nThe forecast method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\n\n\n\n\n\nNote\n\n\n\nThe forecast is inteded to be compatible with distributed clusters, so it does not store any model parameters. If you want to store parameter for everymodel you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nfrom time import time\n\n\ninit = time()\nforecasts_df = sf.forecast(df=Y_df_m5, h=28)\nend = time()\nprint(f'Statsforecast time M5 {(end - init) / 60}')\n\nStatsforecast time M5 14.274124479293823\n\n\nStore the results for further evaluation.\n\nforecasts_df['ThETS'] = forecasts_df[['DynamicOptimizedTheta', 'AutoETS']].clip(0).median(axis=1, numeric_only=True)\nforecasts_df.to_parquet('s3://m5-benchmarks/forecasts/statsforecast-m5.parquet')"
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html#evaluation",
    "href": "docs/how-to-guides/amazonstatsforecast.html#evaluation",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Evaluation",
    "text": "Evaluation\nThis section evaluates the performance of StatsForecast and AmazonForecast. To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a large battery of benchmark datasets and evaluation utilities. The library will allow us to calculate the performance of the models using the original evaluation used in the competition.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.m5 import M5, M5Evaluation\n\nThe following function will allow us to evaluate a specific model included in the input dataframe. The function is useful for evaluating different models.\n\nfrom datasetsforecast.m5 import M5, M5Evaluation\nfrom statsforecast import StatsForecast\n\n### Evaluator\ndef evaluate_forecasts(df, model, model_name):\n    Y_hat = df.set_index('ds', append=True)[model].unstack()\n    *_, S_df = M5.load('data')\n    Y_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])\n    eval_ = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n    eval_ = eval_.rename(columns={'wrmsse': f'{model_name}_{model}_wrmsse'})\n    return eval_\n\nNow let‚Äôs read the forecasts generated for each solution.\n\n### Read Forecasts\nstatsforecasts_df = pd.read_parquet('s3://m5-benchmarks/forecasts/statsforecast-m5.parquet')\namazonforecasts_df = pd.read_parquet('s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet')\n\n### Amazon Forecast wrangling\namazonforecasts_df = amazonforecasts_df.rename(columns={'item_id': 'unique_id', 'date': 'ds'})\n# amazon forecast returns the unique_id column in lower case\n# we need to transform it to upper case to ensure proper merging\namazonforecasts_df['unique_id'] = amazonforecasts_df['unique_id'].str.upper()\namazonforecasts_df = amazonforecasts_df.set_index('unique_id')\n# parse datestamp\namazonforecasts_df['ds'] = pd.to_datetime(amazonforecasts_df['ds']).dt.tz_localize(None)\n\nFinally, let‚Äôs use our predefined function to compute the performance of each model.\n\n### Evaluate performances\nm5_eval_df = pd.concat([\n    evaluate_forecasts(statsforecasts_df, 'ThETS', 'StatsForecast'),\n    evaluate_forecasts(statsforecasts_df, 'AutoETS', 'StatsForecast'),\n    evaluate_forecasts(statsforecasts_df, 'DynamicOptimizedTheta', 'StatsForecast'),\n    evaluate_forecasts(amazonforecasts_df, 'p50', 'AmazonForecast'),\n], axis=1)\nm5_eval_df.T\n\n\n\n\n\n\n\n\nTotal\nLevel1\nLevel2\nLevel3\nLevel4\nLevel5\nLevel6\nLevel7\nLevel8\nLevel9\nLevel10\nLevel11\nLevel12\n\n\n\n\nStatsForecast_ThETS_wrmsse\n0.669606\n0.424331\n0.515777\n0.580670\n0.474098\n0.552459\n0.578092\n0.651079\n0.642446\n0.725324\n1.009390\n0.967537\n0.914068\n\n\nStatsForecast_AutoETS_wrmsse\n0.672404\n0.430474\n0.516340\n0.580736\n0.482090\n0.559721\n0.579939\n0.655362\n0.643638\n0.727967\n1.010596\n0.968168\n0.913820\n\n\nStatsForecast_DynamicOptimizedTheta_wrmsse\n0.675333\n0.429670\n0.521640\n0.589278\n0.478730\n0.557520\n0.584278\n0.656283\n0.650613\n0.731735\n1.013910\n0.971758\n0.918576\n\n\nAmazonForecast_p50_wrmsse\n1.617815\n1.912144\n1.786991\n1.736382\n1.972658\n2.010498\n1.805926\n1.819329\n1.667225\n1.619216\n1.156432\n1.012942\n0.914040\n\n\n\n\n\n\n\nThe results (including processing time and costs) can be summarized in the following table."
  },
  {
    "objectID": "docs/how-to-guides/numba_cache.html",
    "href": "docs/how-to-guides/numba_cache.html",
    "title": "Numba caching",
    "section": "",
    "text": "statsforecast makes heavy use of numba to speed up several critical functions that estimate model parameters. This comes at a cost though, which is that the functions have to be JIT compiled the first time they‚Äôre run, which can be expensive. Once a function has ben JIT compiled, subsequent calls are significantly faster. One problem is that this compilation is saved (by default) on a per-session basis.\nIn order to mitigate the compilation overhead numba offers the option to cache the function compiled code to a file, which can be then reused across sessions, and even copied over to different machines that share the same CPU characteristics (more info).\nTo leverage caching, you can set the NIXTLA_NUMBA_CACHE environment variable (e.g.¬†NIXTLA_NUMBA_CACHE=1), which will enable caching for all functions. By default the cache is saved to the __pycache__ directory, but you can override this with the NUMBA_CACHE_DIR environment variable to save it to a different path (e.g.¬†NUMBA_CACHE_DIR=numba_cache), you can find more information in the docs.\nIf you want to have this enabled for all your sessions, we suggest adding export NIXTLA_NUMBA_CACHE=1 to your profile files, such as .bashrc, .zshrc, etc.\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/ray.html",
    "href": "docs/how-to-guides/ray.html",
    "title": "Ray",
    "section": "",
    "text": "As long as Ray is installed and configured, StatsForecast will be able to use it. If executing on a distributed Ray cluster, make use the statsforecast library is installed across all the workers.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/ray.html#installation",
    "href": "docs/how-to-guides/ray.html#installation",
    "title": "Ray",
    "section": "",
    "text": "As long as Ray is installed and configured, StatsForecast will be able to use it. If executing on a distributed Ray cluster, make use the statsforecast library is installed across all the workers."
  },
  {
    "objectID": "docs/how-to-guides/ray.html#statsforecast-on-pandas",
    "href": "docs/how-to-guides/ray.html#statsforecast-on-pandas",
    "title": "Ray",
    "section": "StatsForecast on Pandas",
    "text": "StatsForecast on Pandas\nBefore running on Ray, it‚Äôs recommended to test on a smaller Pandas dataset to make sure everything is working. This example also helps show the small differences when using Ray.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    AutoARIMA,\n    AutoETS,\n)\nfrom statsforecast.utils import generate_series\n\nn_series = 4\nhorizon = 7\n\nseries = generate_series(n_series)\n\nsf = StatsForecast(\n    models=[AutoETS(season_length=7)],\n    freq='D',\n)\nsf.forecast(df=series, h=horizon).head()\n\n\n\n\n\n\n\n\nds\nAutoETS\n\n\nunique_id\n\n\n\n\n\n\n0\n2000-08-10\n5.261609\n\n\n0\n2000-08-11\n6.196357\n\n\n0\n2000-08-12\n0.282309\n\n\n0\n2000-08-13\n1.264195\n\n\n0\n2000-08-14\n2.262453"
  },
  {
    "objectID": "docs/how-to-guides/ray.html#executing-on-ray",
    "href": "docs/how-to-guides/ray.html#executing-on-ray",
    "title": "Ray",
    "section": "Executing on Ray",
    "text": "Executing on Ray\nTo run the forecasts distributed on Ray, just pass in a Ray Dataset instead. Instead of having the unique_id as an index, it needs to be a column because Ray has no index.\n\nimport ray\nimport logging\nray.init(logging_level=logging.ERROR)\n\nseries = series.reset_index()\nseries['unique_id'] = series['unique_id'].astype(str)\nctx = ray.data.context.DatasetContext.get_current()\nctx.use_streaming_executor = False\nray_series = ray.data.from_pandas(series).repartition(4)\n\n\nsf.forecast(df=ray_series, h=horizon).take(5)\n\n2023-06-17 01:39:08,329 INFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -&gt; AllToAllOperator[Repartition]\n2023-06-17 01:39:09,554 INFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -&gt; AllToAllOperator[Repartition]\n2023-06-17 01:39:09,727 INFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -&gt; TaskPoolMapOperator[MapBatches(add_simple_key)]\n2023-06-17 01:39:11,134 INFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -&gt; AllToAllOperator[Sort] -&gt; TaskPoolMapOperator[MapBatches(group_fn)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 10, 0, 0),\n  'AutoETS': 5.261609077453613},\n {'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 11, 0, 0),\n  'AutoETS': 6.196357250213623},\n {'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 12, 0, 0),\n  'AutoETS': 0.28230854868888855},\n {'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 13, 0, 0),\n  'AutoETS': 1.2641948461532593},\n {'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 14, 0, 0),\n  'AutoETS': 2.2624528408050537}]"
  },
  {
    "objectID": "docs/how-to-guides/mlflow.html",
    "href": "docs/how-to-guides/mlflow.html",
    "title": "MLFlow",
    "section": "",
    "text": "from statsforecast.utils import generate_series\nseries = generate_series(5, min_length=50, max_length=50, equal_ends=True, n_static_features=1)\nseries.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\n\n\n\n\n0\n0\n2000-01-01\n12.073897\n43\n\n\n1\n0\n2000-01-02\n59.734166\n43\n\n\n2\n0\n2000-01-03\n101.260794\n43\n\n\n3\n0\n2000-01-04\n143.987430\n43\n\n\n4\n0\n2000-01-05\n185.320406\n43\nFor the next part, mlflow and mlflavors are needed. Install them with:\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/mlflow.html#model-logging",
    "href": "docs/how-to-guides/mlflow.html#model-logging",
    "title": "MLFlow",
    "section": "Model Logging",
    "text": "Model Logging\n\nimport pandas as pd\nimport mlflow\nfrom sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nimport mlflavors\nimport requests\n\n\nARTIFACT_PATH = \"model\"\nDATA_PATH = \"./data\"\nHORIZON = 7\nLEVEL = [90]\n\nwith mlflow.start_run() as run:\n    series = generate_series(5, min_length=50, max_length=50, equal_ends=True, n_static_features=1)\n    \n    train_df = series.groupby('unique_id').head(43)\n    test_df = series.groupby('unique_id').tail(7)\n    X_test = test_df.drop(columns=[\"y\"])\n    y_test = test_df[[\"y\"]]\n\n    models = [AutoARIMA(season_length=7)]\n\n    sf = StatsForecast(df=train_df, models=models, freq=\"D\", n_jobs=-1)\n\n    sf.fit()\n\n    # Evaluate model\n    y_pred = sf.predict(h=HORIZON, X_df=X_test, level=LEVEL)[\"AutoARIMA\"]\n\n    metrics = {\n        \"mae\": mean_absolute_error(y_test, y_pred),\n        \"mape\": mean_absolute_percentage_error(y_test, y_pred),\n    }\n\n    print(f\"Metrics: \\n{metrics}\")\n\n    # Log metrics\n    mlflow.log_metrics(metrics)\n\n    # Log model using pickle serialization (default).\n    mlflavors.statsforecast.log_model(\n        statsforecast_model=sf,\n        artifact_path=ARTIFACT_PATH,\n        serialization_format=\"pickle\",\n    )\n    model_uri = mlflow.get_artifact_uri(ARTIFACT_PATH)\n\nprint(f\"\\nMLflow run id:\\n{run.info.run_id}\")\n\nMetrics: \n{'mae': 6.712853959225143, 'mape': 0.11719246764336884}\n\nMLflow run id:\n0319bbd664424fcd88d6c532e3ecac77\n\n\n2023/10/20 23:45:36 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/w2/91_v34nx0xs2npnl3zsl9tmm0000gn/T/tmpt4686vpu/model/model.pkl, flavor: statsforecast), fall back to return ['statsforecast==1.6.0']. Set logging level to DEBUG to see the full traceback."
  },
  {
    "objectID": "docs/how-to-guides/mlflow.html#viewing-experiment",
    "href": "docs/how-to-guides/mlflow.html#viewing-experiment",
    "title": "MLFlow",
    "section": "Viewing Experiment",
    "text": "Viewing Experiment\nTo view the newly created experiment and logged artifacts open the MLflow UI:\nmlflow ui"
  },
  {
    "objectID": "docs/how-to-guides/mlflow.html#loading-statsforecast-model",
    "href": "docs/how-to-guides/mlflow.html#loading-statsforecast-model",
    "title": "MLFlow",
    "section": "Loading Statsforecast Model",
    "text": "Loading Statsforecast Model\nThe statsforecast model can be loaded from the MLFlow registry using the mlflow.statsforecast.load_model function and used to generate predictions.\n\nloaded_model = mlflavors.statsforecast.load_model(model_uri=model_uri)\nresults = loaded_model.predict(h=HORIZON, X_df=X_test, level=LEVEL)\nresults.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n0\n2000-02-13\n55.894432\n44.343880\n67.444984\n\n\n0\n2000-02-14\n97.818054\n86.267502\n109.368607\n\n\n0\n2000-02-15\n146.745422\n135.194870\n158.295975\n\n\n0\n2000-02-16\n188.888336\n177.337784\n200.438904\n\n\n0\n2000-02-17\n231.493637\n219.943085\n243.044189"
  },
  {
    "objectID": "docs/how-to-guides/mlflow.html#loading-model-with-pyfunc",
    "href": "docs/how-to-guides/mlflow.html#loading-model-with-pyfunc",
    "title": "MLFlow",
    "section": "Loading Model with pyfunc",
    "text": "Loading Model with pyfunc\nPyfunc is another interface for MLFlow models that has utilities for loading and saving models. This code is equivalent in making predictions as above.\n\nloaded_pyfunc = mlflavors.statsforecast.pyfunc.load_model(model_uri=model_uri)\n\n# Convert test data to 2D numpy array so it can be passed to pyfunc predict using\n# a single-row Pandas DataFrame configuration argument\nX_test_array = X_test.to_numpy()\n\n# Create configuration DataFrame\npredict_conf = pd.DataFrame(\n    [\n        {\n            \"X\": X_test_array,\n            \"X_cols\": X_test.columns,\n            \"X_dtypes\": list(X_test.dtypes),\n            \"h\": HORIZON,\n            \"level\": LEVEL,\n        }\n    ]\n)\n\n\npyfunc_result = loaded_pyfunc.predict(predict_conf)\npyfunc_result.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n0\n2000-02-13\n55.894432\n44.343880\n67.444984\n\n\n0\n2000-02-14\n97.818054\n86.267502\n109.368607\n\n\n0\n2000-02-15\n146.745422\n135.194870\n158.295975\n\n\n0\n2000-02-16\n188.888336\n177.337784\n200.438904\n\n\n0\n2000-02-17\n231.493637\n219.943085\n243.044189"
  },
  {
    "objectID": "docs/how-to-guides/mlflow.html#model-serving",
    "href": "docs/how-to-guides/mlflow.html#model-serving",
    "title": "MLFlow",
    "section": "Model Serving",
    "text": "Model Serving\nThis section illustrates an example of serving the pyfunc flavor to a local REST API endpoint and subsequently requesting a prediction from the served model. To serve the model run the command below where you substitute the run id printed during execution training code.\nmlflow models serve -m runs:/&lt;run_id&gt;/model --env-manager local --host 127.0.0.1\nAfter running this, the code below can be ran to send a request.\n\nHORIZON = 7\nLEVEL = [90, 95]\n\n# Define local host and endpoint url\nhost = \"127.0.0.1\"\nurl = f\"http://{host}:5000/invocations\"\n\n# Convert DateTime to string for JSON serialization\nX_test_pyfunc = X_test.copy()\nX_test_pyfunc[\"ds\"] = X_test_pyfunc[\"ds\"].dt.strftime(date_format=\"%Y-%m-%d\")\n\n# Convert to list for JSON serialization\nX_test_list = X_test_pyfunc.to_numpy().tolist()\n\n# Convert index to list of strings for JSON serialization\nX_cols = list(X_test.columns)\n\n# Convert dtypes to string for JSON serialization\nX_dtypes = [str(dtype) for dtype in list(X_test.dtypes)]\n\npredict_conf = pd.DataFrame(\n    [\n        {\n            \"X\": X_test_list,\n            \"X_cols\": X_cols,\n            \"X_dtypes\": X_dtypes,\n            \"h\": HORIZON,\n            \"level\": LEVEL,\n        }\n    ]\n)\n\n# Create dictionary with pandas DataFrame in the split orientation\njson_data = {\"dataframe_split\": predict_conf.to_dict(orient=\"split\")}\n\n# Score model\nresponse = requests.post(url, json=json_data)\n\n\npd.DataFrame(response.json()['predictions']).head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-95\nAutoARIMA-lo-90\nAutoARIMA-hi-90\nAutoARIMA-hi-95\n\n\n\n\n0\n2000-02-13T00:00:00\n55.894432\n42.131100\n44.343880\n67.444984\n69.657768\n\n\n1\n2000-02-14T00:00:00\n97.818054\n84.054718\n86.267502\n109.368607\n111.581390\n\n\n2\n2000-02-15T00:00:00\n146.745422\n132.982086\n135.194870\n158.295975\n160.508759\n\n\n3\n2000-02-16T00:00:00\n188.888336\n175.125015\n177.337784\n200.438904\n202.651672\n\n\n4\n2000-02-17T00:00:00\n231.493637\n217.730301\n219.943085\n243.044189\n245.256973"
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html",
    "href": "docs/how-to-guides/prophet_spark_m5.html",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "",
    "text": "The purpose of this notebook is to create a scalability benchmark (time and performance). To that end, Nixtla‚Äôs StatsForecast (using the ETS model) is trained on the M5 dataset using spark to distribute the training. As a comparison, Facebook‚Äôs Prophet model is used.\nAn AWS cluster (mounted on databricks) of 11 instances of type m5.2xlarge (8 cores, 32 GB RAM) with runtime 10.4 LTS was used. This notebook was used as base case.\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html#main-results",
    "href": "docs/how-to-guides/prophet_spark_m5.html#main-results",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Main results",
    "text": "Main results\n\n\n\nMethod\nTime (mins)\nPerformance (wRMSSE)\n\n\n\n\nStatsForecast\n7.5\n0.68\n\n\nProphet\n18.23\n0.77"
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html#installing-libraries",
    "href": "docs/how-to-guides/prophet_spark_m5.html#installing-libraries",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Installing libraries",
    "text": "Installing libraries\n\npip install prophet \"neuralforecast&lt;1.0.0\" \"statsforecast[fugue]\""
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html#statsforecast-pipeline",
    "href": "docs/how-to-guides/prophet_spark_m5.html#statsforecast-pipeline",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "StatsForecast pipeline",
    "text": "StatsForecast pipeline\n\nfrom time import time\n\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom statsforecast.distributed.utils import forecast\nfrom statsforecast.distributed.fugue import FugueBackend\nfrom statsforecast.models import ETS, SeasonalNaive\nfrom statsforecast.core import StatsForecast\n\nfrom pyspark.sql import SparkSession\n\n\n\n\n\n\nspark = SparkSession.builder.getOrCreate()\nbackend = FugueBackend(spark, {\"fugue.spark.use_pandas_udf\":True})\n\n\n\n\n\n\nForecast\nWith statsforecast you don‚Äôt have to download your data. The distributed backend can handle a file with your data.\n\ninit = time()\nets_forecasts = backend.forecast(\n    \"s3://m5-benchmarks/data/train/m5-target.parquet\", \n    [ETS(season_length=7, model='ZAA')], \n    freq=\"D\", \n    h=28, \n).toPandas()\nend = time()\nprint(f'Minutes taken by StatsForecast on a Spark cluster: {(end - init) / 60}')\n\n\nMinutes taken by StatsForecast on a Spark cluster: 7.471468730767568\n\n\n\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = ets_forecasts.set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n\n\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])#.drop(columns=['unique_id'])\n\n\nwrmsse_ets = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\nwrmsse_ets\n\n\nOut[14]: \n\n\n\n\n\n\n\n\n\nwrmsse\n\n\n\n\nTotal\n0.682358\n\n\nLevel1\n0.449115\n\n\nLevel2\n0.533754\n\n\nLevel3\n0.592317\n\n\nLevel4\n0.497086\n\n\nLevel5\n0.572189\n\n\nLevel6\n0.593880\n\n\nLevel7\n0.665358\n\n\nLevel8\n0.652183\n\n\nLevel9\n0.734492\n\n\nLevel10\n1.012633\n\n\nLevel11\n0.969902\n\n\nLevel12\n0.915380"
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html#prophet-pipeline",
    "href": "docs/how-to-guides/prophet_spark_m5.html#prophet-pipeline",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Prophet pipeline",
    "text": "Prophet pipeline\n\nimport logging\nfrom time import time\n\nimport pandas as pd\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom prophet import Prophet\nfrom pyspark.sql.types import *\n\n# disable informational messages from prophet\nlogging.getLogger('py4j').setLevel(logging.ERROR)\n\n\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\n\n\n\n\nDownload data\n\n# structure of the training data set\ntrain_schema = StructType([\n  StructField('unique_id', StringType()),  \n  StructField('ds', DateType()),\n  StructField('y', DoubleType())\n  ])\n \n# read the training file into a dataframe\ntrain = spark.read.parquet(\n  's3://m5-benchmarks/data/train/m5-target.parquet', \n  header=True, \n  schema=train_schema\n )\n \n# make the dataframe queriable as a temporary view\ntrain.createOrReplaceTempView('train')\n\n\n\n\n\n\nsql_statement = '''\n  SELECT\n    unique_id AS unique_id,\n    CAST(ds as date) as ds,\n    y as y\n  FROM train\n  '''\n \nm5_history = (\n  spark\n    .sql( sql_statement )\n    .repartition(sc.defaultParallelism, ['unique_id'])\n  ).cache()\n\n\n\n\n\n\n\nForecast function using Prophet\n\ndef forecast( history_pd: pd.DataFrame ) -&gt; pd.DataFrame:\n  \n  # TRAIN MODEL AS BEFORE\n  # --------------------------------------\n  # remove missing values (more likely at day-store-item level)\n    history_pd = history_pd.dropna()\n\n    # configure the model\n    model = Prophet(\n        growth='linear',\n        daily_seasonality=False,\n        weekly_seasonality=True,\n        yearly_seasonality=True,\n        seasonality_mode='multiplicative'\n    )\n\n    # train the model\n    model.fit( history_pd )\n    # --------------------------------------\n\n    # BUILD FORECAST AS BEFORE\n    # --------------------------------------\n    # make predictions\n    future_pd = model.make_future_dataframe(\n        periods=28, \n        freq='d', \n        include_history=False\n    )\n    forecast_pd = model.predict( future_pd )  \n    # --------------------------------------\n\n    # ASSEMBLE EXPECTED RESULT SET\n    # --------------------------------------\n    # get relevant fields from forecast\n    forecast_pd['unique_id'] = history_pd['unique_id'].unique()[0]\n    f_pd = forecast_pd[['unique_id', 'ds','yhat']]\n    # --------------------------------------\n\n    # return expected dataset\n    return f_pd\n\n\n\n\n\n\nresult_schema = StructType([\n  StructField('unique_id', StringType()), \n  StructField('ds',DateType()),\n  StructField('yhat',FloatType()),\n])\n\n\n\n\n\n\nTraining Prophet on the M5 dataset\n\ninit = time()\nresults = (\n  m5_history\n    .groupBy('unique_id')\n      .applyInPandas(forecast, schema=result_schema)\n    ).toPandas()\nend = time()\nprint(f'Minutes taken by Prophet on a Spark cluster: {(end - init) / 60}')\n\n\nMinutes taken by Prophet on a Spark cluster: 18.23116923570633\n\n\n\n\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = results.set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n\n\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])#.drop(columns=['unique_id'])\n\n\n\n\n\n\nwrmsse = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\n\n\n\n\nwrmsse\n\n\nOut[10]: \n\n\n\n\n\n\n\n\n\nwrmsse\n\n\n\n\nTotal\n0.771800\n\n\nLevel1\n0.507905\n\n\nLevel2\n0.586328\n\n\nLevel3\n0.666686\n\n\nLevel4\n0.549358\n\n\nLevel5\n0.655003\n\n\nLevel6\n0.647176\n\n\nLevel7\n0.747047\n\n\nLevel8\n0.743422\n\n\nLevel9\n0.824667\n\n\nLevel10\n1.207069\n\n\nLevel11\n1.108780\n\n\nLevel12\n1.018163"
  },
  {
    "objectID": "docs/how-to-guides/spark.html",
    "href": "docs/how-to-guides/spark.html",
    "title": "Spark",
    "section": "",
    "text": "StatsForecast works on top of Spark, Dask, and Ray through Fugue. StatsForecast will read the input DataFrame and use the corresponding engine. For example, if the input is a Spark DataFrame, StatsForecast will use the existing Spark session to run the forecast.\nA benchmark (with older syntax) can be found here where we forecasted one million timeseries in under 15 minutes.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/how-to-guides/spark.html#installation",
    "href": "docs/how-to-guides/spark.html#installation",
    "title": "Spark",
    "section": "Installation",
    "text": "Installation\nAs long as Spark is installed and configured, StatsForecast will be able to use it. If executing on a distributed Spark cluster, make use the statsforecast library is installed across all the workers."
  },
  {
    "objectID": "docs/how-to-guides/spark.html#statsforecast-on-pandas",
    "href": "docs/how-to-guides/spark.html#statsforecast-on-pandas",
    "title": "Spark",
    "section": "StatsForecast on Pandas",
    "text": "StatsForecast on Pandas\nBefore running on Spark, it‚Äôs recommended to test on a smaller Pandas dataset to make sure everything is working. This example also helps show the small differences when using Spark.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    AutoARIMA,\n    AutoETS,\n)\nfrom statsforecast.utils import generate_series\n\nn_series = 4\nhorizon = 7\n\nseries = generate_series(n_series)\n\nsf = StatsForecast(\n    models=[AutoETS(season_length=7)],\n    freq='D',\n)\nsf.forecast(df=series, h=horizon).head()\n\n\n\n\n\n\n\n\nds\nAutoETS\n\n\nunique_id\n\n\n\n\n\n\n0\n2000-08-10\n5.261609\n\n\n0\n2000-08-11\n6.196357\n\n\n0\n2000-08-12\n0.282309\n\n\n0\n2000-08-13\n1.264195\n\n\n0\n2000-08-14\n2.262453"
  },
  {
    "objectID": "docs/how-to-guides/spark.html#executing-on-spark",
    "href": "docs/how-to-guides/spark.html#executing-on-spark",
    "title": "Spark",
    "section": "Executing on Spark",
    "text": "Executing on Spark\nTo run the forecasts distributed on Spark, just pass in a Spark DataFrame instead. Instead of having the unique_id as an index, it needs to be a column because Spark has no index.\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\n\n# Make unique_id a column\nseries = series.reset_index()\nseries['unique_id'] = series['unique_id'].astype(str)\n\n# Convert to Spark\nsdf = spark.createDataFrame(series)\n\n# Returns a Spark DataFrame\nsf.forecast(df=sdf, h=horizon, level=[90]).show(5)\n\n+---------+-------------------+----------+\n|unique_id|                 ds|   AutoETS|\n+---------+-------------------+----------+\n|        1|2000-04-07 00:00:00|  4.312628|\n|        1|2000-04-08 00:00:00|  5.228625|\n|        1|2000-04-09 00:00:00|   6.24151|\n|        1|2000-04-10 00:00:00|0.23369633|\n|        1|2000-04-11 00:00:00|  1.173954|\n+---------+-------------------+----------+\nonly showing top 5 rows"
  },
  {
    "objectID": "docs/how-to-guides/spark.html#helpful-configuration",
    "href": "docs/how-to-guides/spark.html#helpful-configuration",
    "title": "Spark",
    "section": "Helpful Configuration",
    "text": "Helpful Configuration\nThere are some Spark-specific configurations that may help optimize the workload.\n\"spark.speculation\": \"true\",\n\"spark.sql.shuffle.partitions\": \"8000\",\n\"spark.sql.adaptive.enabled\": \"false\",\n\"spark.task.cpus\": \"1\""
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html",
    "href": "docs/models/simpleexponentialsmoothing.html",
    "title": "Simple Exponential Smoothing Model",
    "section": "",
    "text": "Introduction\nSimple Exponential Smoothing\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of SimpleExponentialSmoothing with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#table-of-contents",
    "href": "docs/models/simpleexponentialsmoothing.html#table-of-contents",
    "title": "Simple Exponential Smoothing Model",
    "section": "",
    "text": "Introduction\nSimple Exponential Smoothing\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of SimpleExponentialSmoothing with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#introduction",
    "href": "docs/models/simpleexponentialsmoothing.html#introduction",
    "title": "Simple Exponential Smoothing Model",
    "section": "Introduction ",
    "text": "Introduction \nExponential smoothing was proposed in the late 1950s (Brown, 1959; Holt, 1957; Winters, 1960), and has motivated some of the most successful forecasting methods. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in industry.\nThe simple exponential smoothing model is a method used in time series analysis to predict future values based on historical observations. This model is based on the idea that future values of a time series will be influenced by past values, and that the influence of past values will decrease exponentially as you go back in time.\nThe simple exponential smoothing model uses a smoothing factor, which is a number between 0 and 1 that indicates the relative importance given to past observations in predicting future values. A value of 1 indicates that all past observations are given equal importance, while a value of 0 indicates that only the latest observation is considered.\nThe simple exponential smoothing model can be expressed mathematically as:\n\\[\\hat{y}_{T+1|T} = \\alpha y_T + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^2 y_{T-2}+ \\cdots,  \\]\nwhere \\(y_T\\) is the observed value in period \\(t\\), \\(\\hat{y}_{T+1|T}\\) is the predicted value for the next period, y \\((t-1)\\) is the observed value in the previous period, and \\(\\alpha\\) is the smoothing factor.\nThe simple exponential smoothing model is a widely used forecasting model due to its simplicity and ease of use. However, it also has its limitations, as it cannot capture complex patterns in the data and is not suitable for time series with trends or seasonal patterns."
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#building-of-simple-exponential-smoothing-model",
    "href": "docs/models/simpleexponentialsmoothing.html#building-of-simple-exponential-smoothing-model",
    "title": "Simple Exponential Smoothing Model",
    "section": "Building of Simple exponential smoothing model ",
    "text": "Building of Simple exponential smoothing model \nThe simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern.\nUsing the na√Øve method, all forecasts for the future are equal to the last observed value of the series, \\[\\hat{y}_{T+h|T} = y_{T},\\]\nfor $h=1,2,$. Hence, the na√Øve method assumes that the most recent observation is the only important one, and all previous observations provide no information for the future. This can be thought of as a weighted average where all of the weight is given to the last observation.\nUsing the average method, all future forecasts are equal to a simple average of the observed data, \\[\\hat{y}_{T+h|T} = \\frac1T \\sum_{t=1}^T y_t, \\]\nfor $h=1,2,$ Hence, the average method assumes that all observations are of equal importance, and gives them equal weights when generating forecasts.\nWe often want something between these two extremes. For example, it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past ‚Äî the smallest weights are associated with the oldest observations:\n\\[\\begin{equation}\n  \\hat{y}_{T+1|T} = \\alpha y_T + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^2 y_{T-2}+ \\cdots,   \\tag{1}\n\\end{equation}\\]\nwhere \\(0 \\le \\alpha \\le 1\\) is the smoothing parameter. The one-step-ahead forecast for time \\(T+1\\) is a weighted average of all of the observations in the series \\(y_1,\\dots,y_T\\). The rate at which the weights decrease is controlled by the parameter \\(\\alpha\\).\nFor any \\(\\alpha\\) between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name ‚Äúexponential smoothing‚Äù. If \\(\\alpha\\) is small (i.e., close to 0), more weight is given to observations from the more distant past. If \\(\\alpha\\) is large (i.e., close to 1), more weight is given to the more recent observations. For the extreme case where \\(\\alpha=1\\), \\(\\hat{y}_{T+1|T}=y_T\\) and the forecasts are equal to the na√Øve forecasts.\nWe present two equivalent forms of simple exponential smoothing, each of which leads to the forecast Equation (1).\n\nWeighted average form\nThe forecast at time \\(T+1\\) is equal to a weighted average between the most recent observation \\(y_T\\) and the previous forecast \\(\\hat{y}_{T|T-1}\\):\n\\[\\hat{y}_{T+1|T} = \\alpha y_T + (1-\\alpha) \\hat{y}_{T|T-1},\\]\nwhere \\(0 \\le \\alpha \\le 1\\) is the smoothing parameter. Similarly, we can write the fitted values as \\[\\hat{y}_{t+1|t} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t|t-1},\\]\nfor \\(t=1,\\dots,T\\). (Recall that fitted values are simply one-step forecasts of the training data.)\nThe process has to start somewhere, so we let the first fitted value at time 1 be denoted by \\(\\ell_{0}\\) (which we will have to estimate). Then\n\\[\\begin{align*}\n  \\hat{y}_{2|1} &= \\alpha y_1 + (1-\\alpha) \\ell_0\\\\\n  \\hat{y}_{3|2} &= \\alpha y_2 + (1-\\alpha) \\hat{y}_{2|1}\\\\\n  \\hat{y}_{4|3} &= \\alpha y_3 + (1-\\alpha) \\hat{y}_{3|2}\\\\\n  \\vdots\\\\\n  \\hat{y}_{T|T-1} &= \\alpha y_{T-1} + (1-\\alpha) \\hat{y}_{T-1|T-2}\\\\\n  \\hat{y}_{T+1|T} &= \\alpha y_T + (1-\\alpha) \\hat{y}_{T|T-1}.\n\\end{align*}\\]\nSubstituting each equation into the following equation, we obtain\n\\[\\begin{align*}\n  \\hat{y}_{3|2}   & = \\alpha y_2 + (1-\\alpha) \\left[\\alpha y_1 + (1-\\alpha) \\ell_0\\right]              \\\\\n                 & = \\alpha y_2 + \\alpha(1-\\alpha) y_1 + (1-\\alpha)^2 \\ell_0                          \\\\\n  \\hat{y}_{4|3}   & = \\alpha y_3 + (1-\\alpha) [\\alpha y_2 + \\alpha(1-\\alpha) y_1 + (1-\\alpha)^2 \\ell_0]\\\\\n                 & = \\alpha y_3 + \\alpha(1-\\alpha) y_2 + \\alpha(1-\\alpha)^2 y_1 + (1-\\alpha)^3 \\ell_0 \\\\\n                 & ~~\\vdots                                                                           \\\\\n  \\hat{y}_{T+1|T} & =  \\sum_{j=0}^{T-1} \\alpha(1-\\alpha)^j y_{T-j} + (1-\\alpha)^T \\ell_{0}.\n\\end{align*}\\]\nThe last term becomes tiny for large \\(T\\). So, the weighted average form leads to the same forecast Equation (1).\n\n\nComponent form\nAn alternative representation is the component form. For simple exponential smoothing, the only component included is the level, \\(\\ell_{t}\\). Component form representations of exponential smoothing methods comprise a forecast equation and a smoothing equation for each of the components included in the method. The component form of simple exponential smoothing is given by:\n\\[\\begin{align*}\n  \\text{Forecast equation}  && \\hat{y}_{t+h|t} & = \\ell_{t}\\\\\n  \\text{Smoothing equation} && \\ell_{t}        & = \\alpha y_{t} + (1 - \\alpha)\\ell_{t-1},\n\\end{align*}\\]\nwhere \\(\\ell_{t}\\) is the level (or the smoothed value) of the series at time \\(t\\). Setting \\(h=1\\) gives the fitted values, while setting \\(t=T\\) gives the true forecasts beyond the training data.\nThe forecast equation shows that the forecast value at time \\(t+1\\) is the estimated level at time \\(t\\). The smoothing equation for the level (usually referred to as the level equation) gives the estimated level of the series at each period \\(t\\).\nIf we replace \\(\\ell_{t}\\) with \\(\\hat{y}_{t+1|t}\\) and \\(\\ell_{t-1}\\) with \\(\\hat{y}_{t|t-1}\\) in the smoothing equation, we will recover the weighted average form of simple exponential smoothing.\nThe component form of simple exponential smoothing is not particularly useful on its own, but it will be the easiest form to use when we start adding other components.\n\n\nFlat forecasts\nSimple exponential smoothing has a ‚Äúflat‚Äù forecast function:\n\\[\\hat{y}_{T+h|T} = \\hat{y}_{T+1|T}=\\ell_T, \\qquad h=2,3,\\dots.\\]\nThat is, all forecasts take the same value, equal to the last level component. Remember that these forecasts will only be suitable if the time series has no trend or seasonal component."
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#loading-libraries-and-data",
    "href": "docs/models/simpleexponentialsmoothing.html#loading-libraries-and-data",
    "title": "Simple Exponential Smoothing Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nimport pandas as pd\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/ads.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nTime\nAds\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n\n\n1\n2017-09-13T01:00:00\n79885\n\n\n2\n2017-09-13T02:00:00\n89325\n\n\n3\n2017-09-13T03:00:00\n101930\n\n\n4\n2017-09-13T04:00:00\n121630\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n1\n\n\n1\n2017-09-13T01:00:00\n79885\n1\n\n\n2\n2017-09-13T02:00:00\n89325\n1\n\n\n...\n...\n...\n...\n\n\n213\n2017-09-21T21:00:00\n103080\n1\n\n\n214\n2017-09-21T22:00:00\n95155\n1\n\n\n215\n2017-09-21T23:00:00\n80285\n1\n\n\n\n\n216 rows √ó 3 columns\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#explore-data-with-the-plot-method",
    "href": "docs/models/simpleexponentialsmoothing.html#explore-data-with-the-plot-method",
    "title": "Simple Exponential Smoothing Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\n# Grafico\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\n#plt.savefig(\"Gr√°fico de Densidad y qq\")\nplt.show();"
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#split-the-data-into-training-and-testing",
    "href": "docs/models/simpleexponentialsmoothing.html#split-the-data-into-training-and-testing",
    "title": "Simple Exponential Smoothing Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our Simple Exponential Smoothing (SES).\nData to test our model\n\nFor the test data we will use the last 30 hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2017-09-20 17:00:00'] \ntest = df[df.ds&gt;'2017-09-20 17:00:00']\n\n\ntrain.shape, test.shape\n\n((186, 3), (30, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"Ads watched (hourly data)\");\nplt.show()"
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#implementation-of-simpleexponentialsmoothing-with-statsforecast",
    "href": "docs/models/simpleexponentialsmoothing.html#implementation-of-simpleexponentialsmoothing-with-statsforecast",
    "title": "Simple Exponential Smoothing Model",
    "section": "Implementation of SimpleExponentialSmoothing with StatsForecast ",
    "text": "Implementation of SimpleExponentialSmoothing with StatsForecast \nTo also know more about the parameters of the functions of the SimpleExponentialSmoothing Model, they are listed below. For more information, visit the documentation.\nalpha : float\n    Smoothing parameter.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import SimpleExponentialSmoothing\n\n\n\nInstantiating Model\nWe are going to build different models, for different values of alpha.\n\nhorizon = len(test)\n# We call the model that we are going to use\nmodels = [SimpleExponentialSmoothing(alpha=0.1, alias=\"SES01\"),\n          SimpleExponentialSmoothing(alpha=0.5,alias=\"SES05\"),\n          SimpleExponentialSmoothing(alpha=0.8,alias=\"SES08\")\n          ]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', # hourly frequency\n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[SES01,SES05,SES08])\n\n\nLet‚Äôs see the results of our Simple Simple Exponential Smoothing model (SES). We can observe it with the following instruction:\n\nresult01=sf.fitted_[0,0].model_\nresult05=sf.fitted_[0,1].model_\nresult08=sf.fitted_[0,2].model_\nresult01\n\n{'mean': array([120864.91], dtype=float32),\n 'fitted': array([       nan,  80115.   ,  80092.   ,  81015.3  ,  83106.77 ,\n         86959.09 ,  89910.69 ,  91569.12 ,  92691.7  ,  94228.03 ,\n         96417.73 ,  99878.96 , 104793.06 , 110072.76 , 114136.98 ,\n        117652.78 , 120897.5  , 123285.75 , 126026.18 , 129807.56 ,\n        133450.3  , 134057.28 , 131241.05 , 127794.945, 123267.445,\n        118953.2  , 114591.38 , 111642.74 , 110686.47 , 112131.32 ,\n        112721.19 , 112371.57 , 111381.914, 110467.73 , 111004.95 ,\n        112958.45 , 116095.11 , 119382.6  , 122359.336, 124927.41 ,\n        127315.664, 129567.1  , 131667.39 , 133444.66 , 135152.19 ,\n        134549.97 , 131476.47 , 127546.32 , 123068.19 , 118392.875,\n        114066.586, 110923.92 , 108711.03 , 109682.93 , 110233.64 ,\n        110304.27 , 109159.84 , 108662.36 , 108662.625, 110460.36 ,\n        113457.83 , 117359.05 , 120250.64 , 123027.58 , 125498.32 ,\n        127523.484, 129699.64 , 132702.17 , 135540.45 , 135538.4  ,\n        133279.06 , 129971.164, 125735.55 , 121945.49 , 118635.445,\n        116006.9  , 114852.71 , 114961.44 , 116360.3  , 118862.766,\n        121420.484, 123603.44 , 124562.09 , 125229.88 , 126954.9  ,\n        129996.91 , 132247.22 , 134396.   , 136075.89 , 137532.81 ,\n        138523.03 , 139923.22 , 140618.4  , 139081.06 , 136965.45 ,\n        132938.9  , 129006.016, 125011.414, 121444.77 , 118357.8  ,\n        116351.016, 115972.914, 117322.625, 119730.86 , 123013.77 ,\n        125970.4  , 127490.36 , 129496.32 , 132657.69 , 136025.42 ,\n        139100.88 , 141504.8  , 143084.81 , 144681.83 , 146215.64 ,\n        148428.58 , 150575.72 , 149789.16 , 146105.73 , 141229.66 ,\n        135274.2  , 129697.77 , 124563.   , 120911.2  , 118799.08 ,\n        119297.17 , 118499.95 , 116593.96 , 114700.06 , 112995.555,\n        111952.5  , 112750.25 , 115050.73 , 117557.66 , 119974.89 ,\n        122199.4  , 124515.46 , 126597.414, 128978.67 , 132232.81 ,\n        134351.03 , 134387.92 , 131655.62 , 127994.57 , 123146.61 ,\n        118665.445, 114265.91 , 111038.31 , 109729.484, 110691.03 ,\n        110933.43 , 109728.086, 108155.28 , 106705.75 , 106453.68 ,\n        107783.305, 110603.98 , 114189.08 , 116686.67 , 119740.51 ,\n        122259.95 , 125170.96 , 128261.86 , 131574.17 , 134917.77 ,\n        134834.98 , 131909.98 , 128004.484, 123131.04 , 118815.94 ,\n        114745.34 , 111849.305, 110665.375, 111986.836, 112421.66 ,\n        111608.49 , 110591.64 , 109295.98 , 109192.875, 110398.59 ,\n        113443.734, 115954.86 , 118458.375, 120765.04 , 122847.53 ,\n        124623.78 , 126112.9  , 128123.11 , 129553.3  , 128992.47 ,\n        126229.23 , 122423.3  , 117785.97 , 113040.875, 108951.79 ,\n        106076.11 , 104963.   , 106657.695, 107386.93 , 107297.734,\n        106296.96 , 105553.266, 105561.44 , 106443.3  , 109032.47 ,\n        112792.22 , 115712.5  , 118422.75 , 121182.47 , 124276.23 ,\n        127027.6  , 129891.34 , 132491.2  , 131581.6  , 128731.43 ,\n        125373.79 ], dtype=float32)}\n\n\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nfitted=pd.DataFrame(result01.get(\"fitted\"), columns=[\"fitted01\"])\nfitted[\"fitted05\"]=result05.get(\"fitted\")\nfitted[\"fitted08\"]=result08.get(\"fitted\")\nfitted[\"ds\"]=df[\"ds\"]\nfitted\n\n\n\n\n\n\n\n\nfitted01\nfitted05\nfitted08\nds\n\n\n\n\n0\nNaN\nNaN\nNaN\n2017-09-13 00:00:00\n\n\n1\n80115.000000\n80115.000000\n80115.000000\n2017-09-13 01:00:00\n\n\n2\n80092.000000\n80000.000000\n79931.000000\n2017-09-13 02:00:00\n\n\n...\n...\n...\n...\n...\n\n\n213\n131581.593750\n138842.562500\n129852.359375\n2017-09-21 21:00:00\n\n\n214\n128731.429688\n120961.281250\n108434.468750\n2017-09-21 22:00:00\n\n\n215\n125373.789062\n108058.140625\n97810.890625\n2017-09-21 23:00:00\n\n\n\n\n216 rows √ó 4 columns\n\n\n\n\nsns.lineplot(df, x=\"ds\", y=\"y\", label=\"Actual\", linewidth=2)\nsns.lineplot(fitted,x=\"ds\", y=\"fitted01\", label=\"Fitted01\", linestyle=\"--\", )\nsns.lineplot(fitted, x=\"ds\", y=\"fitted05\", label=\"Fitted05\", color=\"lime\")\nsns.lineplot(fitted, x=\"ds\", y=\"fitted08\", label=\"Fitted08\")\nplt.title(\"Ads watched (hourly data)\");\nplt.show()\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\n# Prediction\nY_hat = sf.forecast(h=horizon, fitted=True)\nY_hat.head()\n\n\n\n\n\n\n\n\nds\nSES01\nSES05\nSES08\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n2017-09-22 01:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n2017-09-22 02:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n2017-09-22 03:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n2017-09-22 04:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n\n\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nSES01\nSES05\nSES08\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n1\n2017-09-13 00:00:00\n80115.0\nNaN\nNaN\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\n80115.000000\n80115.00\n80115.000000\n\n\n1\n2017-09-13 02:00:00\n89325.0\n80092.000000\n80000.00\n79931.000000\n\n\n1\n2017-09-13 03:00:00\n101930.0\n81015.296875\n84662.50\n87446.203125\n\n\n1\n2017-09-13 04:00:00\n121630.0\n83106.773438\n93296.25\n99033.242188\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nSES01\nSES05\nSES08\n\n\n\n\n0\n1\n2017-09-22 00:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n1\n2017-09-22 01:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n2\n1\n2017-09-22 02:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n3\n1\n2017-09-22 03:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n4\n1\n2017-09-22 04:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n\n\n\n\n\n\n\nPredict method\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon). * h (int): represents the forecast \\(h\\) steps into the future. In this case, 30 hours ahead.\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \nforecast_df\n\n\n\n\n\n\n\n\nds\nSES01\nSES05\nSES08\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n2017-09-22 01:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n2017-09-22 02:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n...\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n2017-09-23 04:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n1\n2017-09-23 05:00:00\n120864.90625\n94171.570312\n83790.179688\n\n\n\n\n30 rows √ó 4 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\ndf_plot=pd.concat([df, forecast_df]).set_index('ds')\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nSES01\nSES05\nSES08\n\n\nds\n\n\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n120864.90625\n94171.570312\n83790.179688\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n120864.90625\n94171.570312\n83790.179688\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n120864.90625\n94171.570312\n83790.179688\n\n\n\n\n246 rows √ó 5 columns\n\n\n\n\n# Plot the data and the exponentially smoothed data\nplt.plot(df_plot['y'],label=\"Actual\", linewidth=2.5)\nplt.plot(df_plot['SES01'], label=\"Predict-SES01\")\nplt.plot(df_plot['SES05'], label=\"Predict-SES05\")\nplt.plot(df_plot['SES08'], label=\"Predicti-SES08\")\nsns.lineplot(fitted,x=\"ds\", y=\"fitted01\", label=\"Fitted01\", linestyle=\"--\", )  # '-', '--', '-.', ':',\nsns.lineplot(fitted, x=\"ds\", y=\"fitted05\", label=\"Fitted05\", color=\"yellow\", linestyle=\"-.\",)\nsns.lineplot(fitted, x=\"ds\", y=\"fitted08\", label=\"Fitted08\", color=\"fuchsia\" ,linestyle=\":\")\nplt.title(\"Ads watched (hourly data)\");\nplt.ylabel(\"\")\n\nText(0, 0.5, '')\n\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#cross-validation",
    "href": "docs/models/simpleexponentialsmoothing.html#cross-validation",
    "title": "Simple Exponential Smoothing Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 30 hourly (n_windows=), forecasting every second months (step_size=30). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 30 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=30,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nSES01\nSES05\nSES08\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n1\n2017-09-18 06:00:00\n2017-09-18 05:00:00\n99440.0\n118499.953125\n109816.250\n112747.695312\n\n\n1\n2017-09-18 07:00:00\n2017-09-18 05:00:00\n97655.0\n118499.953125\n109816.250\n112747.695312\n\n\n1\n2017-09-18 08:00:00\n2017-09-18 05:00:00\n97655.0\n118499.953125\n109816.250\n112747.695312\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2017-09-21 21:00:00\n2017-09-20 17:00:00\n103080.0\n126112.898438\n140008.125\n139770.906250\n\n\n1\n2017-09-21 22:00:00\n2017-09-20 17:00:00\n95155.0\n126112.898438\n140008.125\n139770.906250\n\n\n1\n2017-09-21 23:00:00\n2017-09-20 17:00:00\n80285.0\n126112.898438\n140008.125\n139770.906250\n\n\n\n\n90 rows √ó 6 columns"
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#evaluate-model",
    "href": "docs/models/simpleexponentialsmoothing.html#evaluate-model",
    "title": "Simple Exponential Smoothing Model",
    "section": "Evaluate Model ",
    "text": "Evaluate Model \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Simple Exponential Smoothing Model (SES).\n\n\nfrom datasetsforecast.losses import mse, mae, rmse\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['ds', 'cutoff', 'y']).columns.tolist()\n    evals = []\n    for model in models:\n        eval_ = df.groupby(['unique_id', 'cutoff']).apply(lambda x: metric(x['y'].values, x[model].values)).to_frame() # Calculate loss for every unique_id, model and cutoff.\n        eval_.columns = [model]\n        evals.append(eval_)\n    evals = pd.concat(evals, axis=1)\n    evals = evals.groupby(['unique_id']).mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n    evals['best_model'] = evals.idxmin(axis=1)\n    return evals\n\n\nevaluation_df = evaluate_cross_validation(crossvalidation_df, rmse)\nevaluation_df\n\n\n\n\n\n\n\n\nSES01\nSES05\nSES08\nbest_model\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n27676.533203\n29132.158203\n29308.0\nSES01"
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#acknowledgements",
    "href": "docs/models/simpleexponentialsmoothing.html#acknowledgements",
    "title": "Simple Exponential Smoothing Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/simpleexponentialsmoothing.html#references",
    "href": "docs/models/simpleexponentialsmoothing.html#references",
    "title": "Simple Exponential Smoothing Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/holtwinters.html",
    "href": "docs/models/holtwinters.html",
    "title": "Holt Winters Model",
    "section": "",
    "text": "Introduction\nHolt-Winters Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of Holt-Winters with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/holtwinters.html#table-of-contents",
    "href": "docs/models/holtwinters.html#table-of-contents",
    "title": "Holt Winters Model",
    "section": "",
    "text": "Introduction\nHolt-Winters Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of Holt-Winters with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/holtwinters.html#introduction",
    "href": "docs/models/holtwinters.html#introduction",
    "title": "Holt Winters Model",
    "section": "Introduction",
    "text": "Introduction\nThe Holt-Winter model, also known as the triple exponential smoothing method, is a forecasting technique widely used in time series analysis. It was developed by Charles Holt and Peter Winters in 1960 as an improvement on Holt‚Äôs double exponential smoothing method.\nThe Holt-Winter model is used to predict future values of a time series that exhibits a trend and seasonality. The model uses three smoothing parameters, one for estimating the trend, another for estimating the level or base level of the time series, and another for estimating seasonality. These parameters are called Œ±, Œ≤ and Œ≥, respectively.\nThe Holt-Winter model is an extension of Holt‚Äôs double exponential smoothing method, which uses only two smoothing parameters to estimate the trend and base level of the time series. The Holt-Winter model improves the accuracy of the forecasts by adding a third smoothing parameter for seasonality.\nOne of the main advantages of the Holt-Winter model is that it is easy to implement and does not require a large amount of historical data to generate accurate predictions. Furthermore, the model is highly adaptable and can be customized to fit a wide variety of time series with seasonality.\nHowever, the Holt-Winter model has some limitations. For example, the model assumes that the time series is stationary and that seasonality is constant. If the time series is not stationary or has non-constant seasonality, the Holt-Winter model may not be the most appropriate.\nIn general, the Holt-Winter model is a useful and widely used technique in time series analysis, especially when the series is expected to exhibit a constant trend and seasonality."
  },
  {
    "objectID": "docs/models/holtwinters.html#holt-winters-method",
    "href": "docs/models/holtwinters.html#holt-winters-method",
    "title": "Holt Winters Model",
    "section": "Holt-Winters Method ",
    "text": "Holt-Winters Method \nThe Holt-Winters seasonal method comprises the forecast equation and three smoothing equations ‚Äî one for the level \\(\\ell_{t}\\), one for the trend \\(b_t\\), and one for the seasonal component \\(s_t\\) , with corresponding smoothing parameters \\(\\alpha\\) , \\(\\beta^*\\) and $ $. We use \\(m\\) to denote the period of the seasonality, i.e., the number of seasons in a year. For example, for quarterly data \\(m=4\\), and for monthly data \\(m=12\\).\nThere are two variations to this method that differ in the nature of the seasonal component. The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series. With the additive method, the seasonal component is expressed in absolute terms in the scale of the observed series, and in the level equation the series is seasonally adjusted by subtracting the seasonal component. Within each year, the seasonal component will add up to approximately zero. With the multiplicative method, the seasonal component is expressed in relative terms (percentages), and the series is seasonally adjusted by dividing through by the seasonal component. Within each year, the seasonal component will sum up to approximately \\(m\\).\n\nHolt-Winters‚Äô additive method\nHolt-Winters‚Äô additive method is a time series forecasting technique that extends the Holt-Winters‚Äô method by incorporating an additive seasonality component. It is suitable for time series data that exhibit a seasonal pattern that changes over time.\nThe Holt-Winters‚Äô additive method uses three smoothing parameters - alpha (Œ±), beta (Œ≤), and gamma (Œ≥) - to estimate the level, trend, and seasonal components of the time series. The alpha parameter controls the smoothing of the level component, the beta parameter controls the smoothing of the trend component, and the gamma parameter controls the smoothing of the additive seasonal component.\nThe forecasting process involves three steps: first, the level, trend, and seasonal components are estimated using the smoothing parameters and the historical data; second, these components are used to forecast future values of the time series; and third, the forecasted values are adjusted for the seasonal component using an additive factor.\nOne of the advantages of Holt-Winters‚Äô additive method is that it can handle time series data with an additive seasonality component, which is common in many real-world applications. The method is also easy to implement and can be extended to handle time series data with changing seasonal patterns.\nHowever, the method has some limitations. It assumes that the seasonality pattern is additive, which may not be the case for all time series. Additionally, the method requires a sufficient amount of historical data to accurately estimate the smoothing parameters and the seasonal component.\nOverall, Holt-Winters‚Äô additive method is a powerful and widely used forecasting technique that can be used to generate accurate predictions for time series data with an additive seasonality component. The method is easy to implement and can be extended to handle time series data with changing seasonal patterns.\nThe component form for the additive method is:\n\\[\\begin{align*}\n  \\hat{y}_{t+h|t} &= \\ell_{t} + hb_{t} + s_{t+h-m(k+1)} \\\\\n  \\ell_{t} &= \\alpha(y_{t} - s_{t-m}) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\\\\n  b_{t} &= \\beta^*(\\ell_{t} - \\ell_{t-1}) + (1 - \\beta^*)b_{t-1}\\\\\n  s_{t} &= \\gamma (y_{t}-\\ell_{t-1}-b_{t-1}) + (1-\\gamma)s_{t-m},\n\\end{align*}\\]\nwhere \\(k\\) is the integer part of \\((h-1)/m\\), which ensures that the estimates of the seasonal indices used for forecasting come from the final year of the sample. The level equation shows a weighted average between the seasonally adjusted observation \\((y_{t} - s_{t-m})\\) and the non-seasonal forecast \\((\\ell_{t-1}+b_{t-1})\\) for time \\(t\\). The trend equation is identical to Holt‚Äôs linear method. The seasonal equation shows a weighted average between the current seasonal index, \\((y_{t}-\\ell_{t-1}-b_{t-1})\\), and the seasonal index of the same season last year (i.e., \\(m\\) time periods ago).\nThe equation for the seasonal component is often expressed as\n\\[s_{t} = \\gamma^* (y_{t}-\\ell_{t})+ (1-\\gamma^*)s_{t-m}.\\]\nIf we substitute \\(\\ell_{t}\\) from the smoothing equation for the level of the component form above, we get\n\\[s_{t} = \\gamma^*(1-\\alpha) (y_{t}-\\ell_{t-1}-b_{t-1})+ [1-\\gamma^*(1-\\alpha)]s_{t-m},\\]\nwhich is identical to the smoothing equation for the seasonal component we specify here, with \\(\\gamma=\\gamma^*(1-\\alpha)\\). The usual parameter restriction is \\(0\\le\\gamma^*\\le1\\), which translates to \\(0\\le\\gamma\\le 1-\\alpha\\).\n\n\nHolt-Winters‚Äô multiplicative method\nThe Holt-Winters‚Äô multiplicative method uses three smoothing parameters - alpha (Œ±), beta (Œ≤), and gamma (Œ≥) - to estimate the level, trend, and seasonal components of the time series. The alpha parameter controls the smoothing of the level component, the beta parameter controls the smoothing of the trend component, and the gamma parameter controls the smoothing of the multiplicative seasonal component.\nThe forecasting process involves three steps: first, the level, trend, and seasonal components are estimated using the smoothing parameters and the historical data; second, these components are used to forecast future values of the time series; and third, the forecasted values are adjusted for the seasonal component using a multiplicative factor.\nOne of the advantages of Holt-Winters‚Äô multiplicative method is that it can handle time series data with a multiplicative seasonality component, which is common in many real-world applications. The method is also easy to implement and can be extended to handle time series data with changing seasonal patterns.\nHowever, the method has some limitations. It assumes that the seasonality pattern is multiplicative, which may not be the case for all time series. Additionally, the method requires a sufficient amount of historical data to accurately estimate the smoothing parameters and the seasonal component.\nOverall, Holt-Winters‚Äô multiplicative method is a powerful and widely used forecasting technique that can be used to generate accurate predictions for time series data with a multiplicative seasonality component. The method is easy to implement and can be extended to handle time series data with changing seasonal patterns.\nIn the multiplicative version, the seasonality averages to one. Use the multiplicative method if the seasonal variation increases with the level.\n\\[\\begin{align*}\n  \\hat{y}_{t+h|t} &= (\\ell_{t} + hb_{t})s_{t+h-m(k+1)} \\\\\n  \\ell_{t} &= \\alpha \\frac{y_{t}}{s_{t-m}} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\\\\n  b_{t} &= \\beta^*(\\ell_{t}-\\ell_{t-1}) + (1 - \\beta^*)b_{t-1}                \\\\\n  s_{t} &= \\gamma \\frac{y_{t}}{(\\ell_{t-1} + b_{t-1})} + (1 - \\gamma)s_{t-m}.\n\\end{align*}\\]\n\n\nMathematical models in the ETS taxonomy\nI hope that it becomes more apparent to the reader how the ETS framework is built upon the idea of time series decomposition. By introducing different components, defining their types, and adding the equations for their update, we can construct models that would work better in capturing the key features of the time series. But we should also consider the potential change in components over time. The ‚Äútransition‚Äù or ‚Äústate‚Äù equations are supposed to reflect this change: they explain how the level, trend or seasonal components evolve.\nAs discussed in Section 2.2, given different types of components and their interactions, we end up with 30 models in the taxonomy. Tables 1 and 2 summarise mathematically all 30 ETS models shown graphically on Figures 1 and 2, presenting formulae for measurement and transition equations.\nTable 1: Additive error ETS models | | Nonseasonal |Additive |Multiplicative| |‚Äî-|‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äì| |No trend|$\n\\[\\begin{aligned} &y_{t} = l_{t-1} + \\epsilon_t \\\\  &l_t = l_{t-1} + \\alpha \\epsilon_t  \\end{aligned}\\]\n$ |\\(\\begin{aligned} &y_{t} = l_{t-1} + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) |\\(\\begin{aligned} &y_{t} = l_{t-1} s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}} \\end{aligned}\\)| |Additive| \\(\\begin{aligned} &y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\ &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + b_{t-1}} \\end{aligned}\\)| |Additive damped| \\(\\begin{aligned} &y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &b_t = \\phi b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + \\phi b_{t-1}} \\end{aligned}\\)| |Multiplicative| $\n\\[\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} + \\epsilon_t \\\\  &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\  &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}}  \\end{aligned}\\]\n$ | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\end{aligned}\\)| |Multiplicative damped| \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\ &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} + \\epsilon_t \\\\ &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\ &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\end{aligned}\\)|\nTable 2: Multiplicative error ETS models | |Nonseasonal |Additive |Multiplicative| |‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äì| |No trend| \\(\\begin{aligned} &y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\ &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} s_{t-m}(1 + \\epsilon_t) \\\\ &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\ &s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\end{aligned}\\)| |Additive| \\(\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1})(1 + \\epsilon_t) \\\\ &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} + b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1} + \\beta (l_{t-1} + b_{t-1}) \\epsilon_t \\\\ &s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}\\)| |Additive damped| \\(\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1})(1 + \\epsilon_t) \\\\ &l_t = (l_{t-1} + \\phi b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} + \\phi b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &b_t = \\phi b_{t-1} + \\beta (l_{t-1} + \\phi b_{t-1}) \\epsilon_t \\\\ &s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\end{aligned}\\)| |Multiplicative| \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} (1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = (l_{t-1} b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &b_t = b_{t-1} + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\)| \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\ &s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}\\)| |Multiplicative damped| \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi (1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1}^\\phi (1 + \\alpha \\epsilon_t) \\\\ &b_t = b_{t-1}^\\phi (1 + \\beta \\epsilon_t) \\end{aligned}\\)| \\(\\begin{aligned} &y_{t} = (l_{t-1} b_{t-1}^\\phi + s_{t-m})(1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &b_t = b_{t-1}^\\phi + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\ &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) | \\(\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} (1 + \\epsilon_t) \\\\ &l_t = l_{t-1} b_{t-1}^\\phi \\left(1 + \\alpha \\epsilon_t\\right) \\\\ &b_t = b_{t-1}^\\phi \\left(1 + \\beta \\epsilon_t\\right) \\\\ &s_t = s_{t-m} \\left(1 + \\gamma \\epsilon_t\\right) \\end{aligned}\\)|\nFrom a statistical point of view, formulae in Tables 1 and 2 correspond to the ‚Äútrue models‚Äù, they explain the models underlying potential data, but when it comes to their construction and estimation, the \\(\\epsilon_t\\) is substituted by the estimated \\(e_t\\) (which is calculated differently depending on the error type), and time series components and smoothing parameters are also replaced by their estimates (e.g.¬†\\(\\hat \\alpha\\) instead of \\(\\alpha\\)). However, if the values of these models‚Äô parameters were known, it would be possible to produce point forecasts and conditional h steps ahead expectations from these models.\n\n\nModel selection\nA great advantage of the Holt Winters statistical framework is that information criteria can be used for model selection. The AIC, AIC_c and BIC, can be used here to determine which of the Holt Winters models is most appropriate for a given time series.\nFor Holt Winters models, Akaike‚Äôs Information Criterion (AIC) is defined as\n\\[\\text{AIC} = -2\\log(L) + 2k,\\]\nwhere \\(L\\) is the likelihood of the model and \\(k\\) is the total number of parameters and initial states that have been estimated (including the residual variance).\nThe AIC corrected for small sample bias (AIC_c) is defined as\n\\[AIC_c = AIC + \\frac{2k(k+1)}{T-k-1}\\]\nand the Bayesian Information Criterion (BIC) is\n\\[\\text{BIC} = \\text{AIC} + k[\\log(T)-2]\\]\nThree of the combinations of (Error, Trend, Seasonal) can lead to numerical difficulties. Specifically, the models that can cause such instabilities are ETS(A,N,M), ETS(A,A,M), and ETS(A,Ad,M), due to division by values potentially close to zero in the state equations. We normally do not consider these particular combinations when selecting a model.\nModels with multiplicative errors are useful when the data are strictly positive, but are not numerically stable when the data contain zeros or negative values. Therefore, multiplicative error models will not be considered if the time series is not strictly positive. In that case, only the six fully additive models will be applied."
  },
  {
    "objectID": "docs/models/holtwinters.html#loading-libraries-and-data",
    "href": "docs/models/holtwinters.html#loading-libraries-and-data",
    "title": "Holt Winters Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/ads.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nTime\nAds\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n\n\n1\n2017-09-13T01:00:00\n79885\n\n\n2\n2017-09-13T02:00:00\n89325\n\n\n3\n2017-09-13T03:00:00\n101930\n\n\n4\n2017-09-13T04:00:00\n121630\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n1\n\n\n1\n2017-09-13T01:00:00\n79885\n1\n\n\n2\n2017-09-13T02:00:00\n89325\n1\n\n\n3\n2017-09-13T03:00:00\n101930\n1\n\n\n4\n2017-09-13T04:00:00\n121630\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/holtwinters.html#explore-data-with-the-plot-method",
    "href": "docs/models/holtwinters.html#explore-data-with-the-plot-method",
    "title": "Holt Winters Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nThe Augmented Dickey-Fuller Test\nAn Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\n\nNull Hypothesis: Time Series is non-stationary. It gives a time-dependent trend.\nAlternate Hypothesis: Time Series is stationary. In another term, the series doesn‚Äôt depend on time.\nADF or t Statistic &lt; critical values: Reject the null hypothesis, time series is stationary.\nADF or t Statistic &gt; critical values: Failed to reject the null hypothesis, time series is non-stationary.\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],'Ads')\n\nDickey-Fuller test results for columns: Ads\nTest Statistic         -7.089634e+00\np-value                 4.444804e-10\nNo Lags Used            9.000000e+00\n                            ...     \nCritical Value (1%)    -3.462499e+00\nCritical Value (5%)    -2.875675e+00\nCritical Value (10%)   -2.574304e+00\nLength: 7, dtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\n\n\nAutocorrelation plots\nAutocorrelation Function\nDefinition 1. Let \\(\\{x_t;1 ‚â§ t ‚â§ n\\}\\) be a time series sample of size n from \\(\\{X_t\\}\\). 1. \\(\\bar x = \\sum_{t=1}^n \\frac{x_t}{n}\\) is called the sample mean of \\(\\{X_t\\}\\). 2. \\(c_k =\\sum_{t=1}^{n‚àík} (x_{t+k}- \\bar x)(x_t‚àí\\bar x)/n\\) is known as the sample autocovariance function of \\(\\{X_t\\}\\). 3. \\(r_k = c_k /c_0\\) is said to be the sample autocorrelation function of \\(\\{X_t\\}\\).\nNote the following remarks about this definition:\n\nLike most literature, this guide uses ACF to denote the sample autocorrelation function as well as the autocorrelation function. What is denoted by ACF can easily be identified in context.\nClearly c0 is the sample variance of \\(\\{X_t\\}\\). Besides, \\(r_0 = c_0/c_0 = 1\\) and for any integer \\(k, |r_k| ‚â§ 1\\).\nWhen we compute the ACF of any sample series with a fixed length \\(n\\), we cannot put too much confidence in the values of \\(r_k\\) for large k‚Äôs, since fewer pairs of \\((x_{t +k }, x_t )\\) are available for calculating \\(r_k\\) as \\(k\\) is large. One rule of thumb is not to estimate \\(r_k\\) for \\(k &gt; n/3\\), and another is \\(n ‚â• 50, k ‚â§ n/4\\). In any case, it is always a good idea to be careful.\nWe also compute the ACF of a nonstationary time series sample by Definition 1. In this case, however, the ACF or \\(r_k\\) very slowly or hardly tapers off as \\(k\\) increases.\nPlotting the ACF \\((r_k)\\) against lag \\(k\\) is easy but very helpful in analyzing time series sample. Such an ACF plot is known as a correlogram.\nIf \\(\\{X_t\\}\\) is stationary with \\(E(X_t)=0\\) and \\(\\rho_k =0\\) for all \\(k \\neq 0\\),thatis,itisa white noise series, then the sampling distribution of \\(r_k\\) is asymptotically normal with the mean 0 and the variance of \\(1/n\\). Hence, there is about 95% chance that \\(r_k\\) falls in the interval \\([‚àí1.96/‚àön, 1.96/‚àön]\\).\n\nNow we can give a summary that (1) if the time series plot of a time series clearly shows a trend or/and seasonality, it is surely nonstationary; (2) if the ACF \\(r_k\\) very slowly or hardly tapers off as lag \\(k\\) increases, the time series should also be nonstationary.\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\n\nAdditive\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=24)\na.plot();\n\n\n\n\n\n\nMultiplicative\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=24)\na.plot();"
  },
  {
    "objectID": "docs/models/holtwinters.html#split-the-data-into-training-and-testing",
    "href": "docs/models/holtwinters.html#split-the-data-into-training-and-testing",
    "title": "Holt Winters Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our Holt Winters Model.\nData to test our model\n\nFor the test data we will use the last 30 hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2017-09-20 17:00:00'] \ntest = df[df.ds&gt;'2017-09-20 17:00:00']\n\n\ntrain.shape, test.shape\n\n((186, 3), (30, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\",linewidth=2)\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\", linewidth=2, color=\"yellow\")\nplt.title(\"Ads watched (hourly data)\");\nplt.show()"
  },
  {
    "objectID": "docs/models/holtwinters.html#implementation-of-holt-winters-method-with-statsforecast",
    "href": "docs/models/holtwinters.html#implementation-of-holt-winters-method-with-statsforecast",
    "title": "Holt Winters Model",
    "section": "Implementation of Holt-Winters Method with StatsForecast ",
    "text": "Implementation of Holt-Winters Method with StatsForecast \nTo also know more about the parameters of the functions of the Holt-Winters Model, they are listed below. For more information, visit the documentation.\nseason_length : int\n    Number of observations per unit of time. Ex: 12 Monthly data.\nerror_type : str\n    The type of error of the ETS model. Can be additive (A) or multiplicative (M).\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import HoltWinters\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\nIn this case we are going to test two alternatives of the model, one additive and one multiplicative.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\nmodels = [HoltWinters(season_length=season_length, error_type=\"A\", alias=\"Add\"),\n          HoltWinters(season_length=season_length, error_type=\"M\", alias=\"Multi\")]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[Add,Multi])\n\n\nLet‚Äôs see the results of our Holt Winters Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nprint(result.keys())\nprint(result['fit'])\n\ndict_keys(['loglik', 'aic', 'bic', 'aicc', 'mse', 'amse', 'fit', 'residuals', 'components', 'm', 'nstate', 'fitted', 'states', 'par', 'sigma2', 'n_params', 'method', 'actual_residuals'])\nresults(x=array([ 2.44373452e-02,  1.38129692e-03,  3.10806480e-02,  8.90103969e-01,\n        1.24049460e+05, -8.68954810e+01, -3.89677433e+04, -2.62776804e+04,\n       -1.53306670e+04,  1.04556014e+04,  3.40799540e+04,  3.61115645e+04,\n        2.95135877e+04,  2.70191012e+04,  2.61439737e+04,  2.73344984e+04,\n        2.70239387e+04,  2.94934401e+04,  2.12740663e+04,  4.02181550e+03,\n       -7.08633837e+03, -1.29309441e+04, -1.23216723e+04, -9.32780505e+03,\n       -3.92528658e+03, -1.18781713e+03, -2.43122964e+04, -3.56913114e+04,\n       -4.32670579e+04]), fn=5078.452315148393, nit=1000, simplex=array([[ 2.40487996e-02,  1.40159178e-03,  2.98715521e-02,\n         8.89367428e-01,  1.23345215e+05, -8.56751447e+01,\n        -3.94103267e+04, -2.58132744e+04, -1.46476758e+04,\n         1.04203903e+04,  3.50465498e+04,  3.59654863e+04,\n         2.93863399e+04,  2.77527548e+04,  2.63918127e+04,\n         2.74059591e+04,  2.59885977e+04,  2.94583381e+04,\n         2.19996508e+04,  4.22020655e+03, -7.12092088e+03,\n        -1.29452569e+04, -1.21734962e+04, -9.28051739e+03,\n        -4.09832793e+03, -1.22159944e+03, -2.40047026e+04,\n        -3.59264081e+04, -4.37141281e+04],\n       [ 2.30584659e-02,  1.29870085e-03,  3.23295869e-02,\n         8.95699018e-01,  1.23226757e+05, -8.51134029e+01,\n        -3.94188347e+04, -2.66463805e+04, -1.46040706e+04,\n         1.00508287e+04,  3.48121192e+04,  3.66574880e+04,\n         2.95156329e+04,  2.75214239e+04,  2.61766232e+04,\n         2.70638403e+04,  2.59821671e+04,  2.92220715e+04,\n         2.15353216e+04,  4.11207780e+03, -7.26888692e+03,\n        -1.31025981e+04, -1.24323871e+04, -9.46450899e+03,\n        -3.87523173e+03, -1.29619824e+03, -2.48262400e+04,\n        -3.65297643e+04, -4.26697834e+04],\n       [ 2.38566990e-02,  1.38393698e-03,  2.86485845e-02,\n         8.91732054e-01,  1.22667885e+05, -8.63064081e+01,\n        -3.95610795e+04, -2.64459058e+04, -1.50401318e+04,\n         1.03748608e+04,  3.46772905e+04,  3.61258281e+04,\n         3.03028728e+04,  2.71476029e+04,  2.59766785e+04,\n         2.83781299e+04,  2.62171593e+04,  2.87088506e+04,\n         2.11399641e+04,  4.26251638e+03, -6.86374732e+03,\n        -1.30572982e+04, -1.25961133e+04, -9.28515605e+03,\n        -3.92065232e+03, -1.29918039e+03, -2.41322902e+04,\n        -3.62733255e+04, -4.35070867e+04],\n       [ 2.27798959e-02,  1.34604573e-03,  3.14497882e-02,\n         8.97672321e-01,  1.23457441e+05, -8.39505753e+01,\n        -3.84752143e+04, -2.70271535e+04, -1.51565030e+04,\n         1.01558921e+04,  3.56374371e+04,  3.57659522e+04,\n         2.92214649e+04,  2.64450530e+04,  2.59612798e+04,\n         2.79127971e+04,  2.65311803e+04,  2.95053263e+04,\n         2.20459941e+04,  4.27175853e+03, -7.11671275e+03,\n        -1.28483284e+04, -1.20024908e+04, -9.14008142e+03,\n        -3.94099609e+03, -1.27913818e+03, -2.43917933e+04,\n        -3.59036544e+04, -4.37145499e+04],\n       [ 2.29779847e-02,  1.35988450e-03,  3.20546039e-02,\n         8.97392076e-01,  1.23999353e+05, -8.57416139e+01,\n        -3.86796082e+04, -2.61112159e+04, -1.46410300e+04,\n         1.00447411e+04,  3.46746838e+04,  3.56015924e+04,\n         2.92653819e+04,  2.72512111e+04,  2.64957723e+04,\n         2.82119577e+04,  2.61112530e+04,  2.90487409e+04,\n         2.15387214e+04,  4.05550604e+03, -6.88742058e+03,\n        -1.28509450e+04, -1.21550165e+04, -9.26733160e+03,\n        -4.09280472e+03, -1.31561243e+03, -2.42237989e+04,\n        -3.58787629e+04, -4.36947890e+04],\n       [ 2.34931397e-02,  1.36066473e-03,  2.92309535e-02,\n         8.94631252e-01,  1.23220366e+05, -7.99821563e+01,\n        -3.97471338e+04, -2.62212988e+04, -1.54426040e+04,\n         1.01804753e+04,  3.45628830e+04,  3.61783359e+04,\n         2.83321227e+04,  2.75443313e+04,  2.65554748e+04,\n         2.85866656e+04,  2.65994443e+04,  2.99352483e+04,\n         2.12904274e+04,  4.19579939e+03, -7.04756866e+03,\n        -1.35378259e+04, -1.21517302e+04, -9.38313255e+03,\n        -3.93098135e+03, -1.23125948e+03, -2.43161758e+04,\n        -3.56808651e+04, -4.30934906e+04],\n       [ 2.34226303e-02,  1.36919062e-03,  3.05044515e-02,\n         8.93651006e-01,  1.22119669e+05, -8.45830782e+01,\n        -3.83859519e+04, -2.66545546e+04, -1.46663235e+04,\n         1.03044558e+04,  3.37863355e+04,  3.58787384e+04,\n         2.98614652e+04,  2.79521053e+04,  2.65483349e+04,\n         2.80130243e+04,  2.67030580e+04,  2.95716350e+04,\n         2.13867299e+04,  4.20532353e+03, -7.19231268e+03,\n        -1.29869745e+04, -1.22214265e+04, -9.35200831e+03,\n        -3.99601887e+03, -1.25191151e+03, -2.45210288e+04,\n        -3.55315031e+04, -4.33561528e+04],\n       [ 2.33686092e-02,  1.38777500e-03,  3.05842986e-02,\n         8.95765424e-01,  1.24435621e+05, -8.44258777e+01,\n        -3.95388846e+04, -2.70585507e+04, -1.42279741e+04,\n         1.04108267e+04,  3.41935238e+04,  3.61927880e+04,\n         2.96134192e+04,  2.72521230e+04,  2.63894944e+04,\n         2.73577833e+04,  2.63681631e+04,  2.89873337e+04,\n         2.13122122e+04,  4.27459659e+03, -7.10258507e+03,\n        -1.26644787e+04, -1.22537168e+04, -9.49233627e+03,\n        -4.01010478e+03, -1.23803020e+03, -2.39978907e+04,\n        -3.60888141e+04, -4.29553133e+04],\n       [ 2.34816233e-02,  1.37991019e-03,  3.06089532e-02,\n         8.94874645e-01,  1.23617905e+05, -8.40833189e+01,\n        -3.85116704e+04, -2.63905726e+04, -1.43692871e+04,\n         1.05551899e+04,  3.51276038e+04,  3.58524187e+04,\n         2.94603806e+04,  2.74703366e+04,  2.55213418e+04,\n         2.83361549e+04,  2.60687975e+04,  2.86600785e+04,\n         2.14422052e+04,  4.10932930e+03, -7.16315898e+03,\n        -1.34079834e+04, -1.23877734e+04, -9.20876927e+03,\n        -3.95489699e+03, -1.23349778e+03, -2.47612959e+04,\n        -3.48066942e+04, -4.33211841e+04],\n       [ 2.41575092e-02,  1.39907614e-03,  3.02357059e-02,\n         8.91102368e-01,  1.23794366e+05, -8.48796907e+01,\n        -3.90608848e+04, -2.64189926e+04, -1.45758290e+04,\n         9.86006031e+03,  3.44455393e+04,  3.62279168e+04,\n         2.93705791e+04,  2.75920864e+04,  2.50923293e+04,\n         2.84262381e+04,  2.67984741e+04,  2.89640135e+04,\n         2.16286886e+04,  4.29540433e+03, -7.25171917e+03,\n        -1.29078390e+04, -1.19948534e+04, -9.29963339e+03,\n        -3.99568567e+03, -1.24285574e+03, -2.43650156e+04,\n        -3.65231527e+04, -4.28366363e+04],\n       [ 2.36961761e-02,  1.38273987e-03,  3.10961396e-02,\n         8.93478329e-01,  1.22419456e+05, -8.54817124e+01,\n        -3.83003011e+04, -2.62799796e+04, -1.47684198e+04,\n         1.03088809e+04,  3.52182251e+04,  3.52679730e+04,\n         2.92136442e+04,  2.74002342e+04,  2.61909267e+04,\n         2.78569566e+04,  2.68610704e+04,  2.89820187e+04,\n         2.15006485e+04,  4.28929006e+03, -6.97610950e+03,\n        -1.28970410e+04, -1.24884156e+04, -9.67725756e+03,\n        -3.89959668e+03, -1.20865626e+03, -2.39696630e+04,\n        -3.60623458e+04, -4.25044368e+04],\n       [ 2.33721259e-02,  1.37576033e-03,  3.07287778e-02,\n         8.95381214e-01,  1.23361917e+05, -8.59292902e+01,\n        -3.97494538e+04, -2.59881823e+04, -1.42733538e+04,\n         1.01850859e+04,  3.48288372e+04,  3.50586273e+04,\n         2.97507897e+04,  2.71064540e+04,  2.67061201e+04,\n         2.78353166e+04,  2.65264771e+04,  2.98277569e+04,\n         2.14489843e+04,  4.23889638e+03, -7.08556474e+03,\n        -1.31498142e+04, -1.22079975e+04, -9.28165029e+03,\n        -3.89871487e+03, -1.25831398e+03, -2.48016766e+04,\n        -3.59037067e+04, -4.20714772e+04],\n       [ 2.26415137e-02,  1.34430820e-03,  3.23341885e-02,\n         8.99285053e-01,  1.22878310e+05, -8.53335700e+01,\n        -3.92562952e+04, -2.59721752e+04, -1.48816794e+04,\n         1.00898476e+04,  3.50587088e+04,  3.62440964e+04,\n         2.98438930e+04,  2.79605178e+04,  2.61637574e+04,\n         2.76187001e+04,  2.60357145e+04,  2.90427564e+04,\n         2.07810067e+04,  4.42036847e+03, -7.03406014e+03,\n        -1.26056030e+04, -1.19217451e+04, -8.98726114e+03,\n        -3.90645105e+03, -1.24678256e+03, -2.42136819e+04,\n        -3.58509789e+04, -4.43751915e+04],\n       [ 2.31801372e-02,  1.37749747e-03,  2.93541139e-02,\n         8.95770558e-01,  1.22858350e+05, -8.80052223e+01,\n        -3.86617013e+04, -2.59278016e+04, -1.47751488e+04,\n         1.03042927e+04,  3.41708552e+04,  3.56988813e+04,\n         2.91682837e+04,  2.67894217e+04,  2.58456196e+04,\n         2.70119460e+04,  2.68253248e+04,  2.93127849e+04,\n         2.12248716e+04,  4.22651962e+03, -7.12436510e+03,\n        -1.29474406e+04, -1.21520227e+04, -9.53356225e+03,\n        -3.96352288e+03, -1.27666223e+03, -2.44328890e+04,\n        -3.56137046e+04, -4.35671367e+04],\n       [ 2.30363008e-02,  1.34895700e-03,  3.31052656e-02,\n         8.96994359e-01,  1.23002355e+05, -8.50280976e+01,\n        -3.91939881e+04, -2.63829796e+04, -1.46326214e+04,\n         1.04634298e+04,  3.45295261e+04,  3.54928661e+04,\n         2.95432569e+04,  2.75345263e+04,  2.50916467e+04,\n         2.68369024e+04,  2.68635715e+04,  3.03912706e+04,\n         2.11714956e+04,  4.28329750e+03, -6.90261152e+03,\n        -1.30645780e+04, -1.22326849e+04, -9.08641469e+03,\n        -4.04851644e+03, -1.32494294e+03, -2.38092257e+04,\n        -3.57905204e+04, -4.35116477e+04],\n       [ 2.44373452e-02,  1.38129692e-03,  3.10806480e-02,\n         8.90103969e-01,  1.24049460e+05, -8.68954810e+01,\n        -3.89677433e+04, -2.62776804e+04, -1.53306670e+04,\n         1.04556014e+04,  3.40799540e+04,  3.61115645e+04,\n         2.95135877e+04,  2.70191012e+04,  2.61439737e+04,\n         2.73344984e+04,  2.70239387e+04,  2.94934401e+04,\n         2.12740663e+04,  4.02181550e+03, -7.08633837e+03,\n        -1.29309441e+04, -1.23216723e+04, -9.32780505e+03,\n        -3.92528658e+03, -1.18781713e+03, -2.43122964e+04,\n        -3.56913114e+04, -4.32670579e+04],\n       [ 2.34225937e-02,  1.37504026e-03,  3.20762187e-02,\n         8.94715930e-01,  1.22649715e+05, -8.51815678e+01,\n        -3.92349107e+04, -2.60372086e+04, -1.46713893e+04,\n         1.03940388e+04,  3.42200364e+04,  3.54151977e+04,\n         2.86168968e+04,  2.72219207e+04,  2.65021110e+04,\n         2.77555263e+04,  2.61144978e+04,  2.88679128e+04,\n         2.14504642e+04,  4.26955607e+03, -7.33178601e+03,\n        -1.28672736e+04, -1.23366734e+04, -9.27407389e+03,\n        -3.98684306e+03, -1.26689130e+03, -2.42014964e+04,\n        -3.60431164e+04, -4.35735281e+04],\n       [ 2.30245102e-02,  1.33633606e-03,  3.17433016e-02,\n         8.95865886e-01,  1.23029482e+05, -8.29602756e+01,\n        -3.96755895e+04, -2.63099175e+04, -1.48152395e+04,\n         1.05185870e+04,  3.52063359e+04,  3.52312467e+04,\n         2.99077028e+04,  2.71533181e+04,  2.66237164e+04,\n         2.78798999e+04,  2.74977313e+04,  2.80935898e+04,\n         2.09427481e+04,  4.09547938e+03, -7.09751151e+03,\n        -1.29766286e+04, -1.18250113e+04, -9.54750290e+03,\n        -3.97635927e+03, -1.32445291e+03, -2.43437156e+04,\n        -3.60393575e+04, -4.30058401e+04],\n       [ 2.35957957e-02,  1.31866210e-03,  3.04466963e-02,\n         8.94704385e-01,  1.23506905e+05, -8.81033940e+01,\n        -3.92910011e+04, -2.67861535e+04, -1.43723619e+04,\n         1.02596397e+04,  3.42680222e+04,  3.49198652e+04,\n         2.96607616e+04,  2.71335556e+04,  2.62749412e+04,\n         2.78456008e+04,  2.67384571e+04,  2.90090597e+04,\n         2.21391413e+04,  4.25656010e+03, -6.97489434e+03,\n        -1.31391614e+04, -1.21060497e+04, -9.02606877e+03,\n        -3.96765488e+03, -1.25601904e+03, -2.41311166e+04,\n        -3.64834934e+04, -4.39441979e+04],\n       [ 2.36751650e-02,  1.36543428e-03,  2.91691871e-02,\n         8.94846111e-01,  1.22793505e+05, -8.69820381e+01,\n        -3.90199341e+04, -2.60601993e+04, -1.47352545e+04,\n         1.01516410e+04,  3.51794657e+04,  3.57287886e+04,\n         2.98594202e+04,  2.80104436e+04,  2.60402555e+04,\n         2.73922257e+04,  2.62342050e+04,  2.91820141e+04,\n         2.17442081e+04,  4.06104487e+03, -7.27342325e+03,\n        -1.29440707e+04, -1.20623302e+04, -9.39257599e+03,\n        -4.11688432e+03, -1.22491978e+03, -2.42825476e+04,\n        -3.61681582e+04, -4.31967736e+04],\n       [ 2.36356407e-02,  1.40249143e-03,  3.11055579e-02,\n         8.94345588e-01,  1.22524471e+05, -8.18301894e+01,\n        -3.97067939e+04, -2.64168124e+04, -1.48209981e+04,\n         1.02443748e+04,  3.46517106e+04,  3.60741202e+04,\n         2.94602626e+04,  2.75042300e+04,  2.60176469e+04,\n         2.71780405e+04,  2.62591460e+04,  2.90272777e+04,\n         2.21503102e+04,  4.14004508e+03, -6.53769777e+03,\n        -1.28541437e+04, -1.20646212e+04, -9.61019431e+03,\n        -4.01591048e+03, -1.26480647e+03, -2.51893997e+04,\n        -3.57918334e+04, -4.39872769e+04],\n       [ 2.31857120e-02,  1.35188810e-03,  3.04128998e-02,\n         8.96437230e-01,  1.23431237e+05, -8.47006111e+01,\n        -3.99591747e+04, -2.66301164e+04, -1.49368574e+04,\n         1.04096179e+04,  3.42782255e+04,  3.52664081e+04,\n         2.93964291e+04,  2.71483335e+04,  2.55844710e+04,\n         2.85473780e+04,  2.62439803e+04,  2.95137214e+04,\n         2.16999982e+04,  4.21616087e+03, -7.13754628e+03,\n        -1.20070878e+04, -1.24988683e+04, -9.28901448e+03,\n        -3.95465776e+03, -1.31514929e+03, -2.43986957e+04,\n        -3.54573986e+04, -4.29463528e+04],\n       [ 2.37859054e-02,  1.39168374e-03,  3.09656129e-02,\n         8.93949631e-01,  1.22688446e+05, -8.27726967e+01,\n        -3.85019011e+04, -2.61638638e+04, -1.47580029e+04,\n         1.02622252e+04,  3.44676069e+04,  3.60705301e+04,\n         2.96185616e+04,  2.73176309e+04,  2.65596714e+04,\n         2.75277449e+04,  2.62402989e+04,  2.92453590e+04,\n         2.17648536e+04,  4.23176214e+03, -7.23546281e+03,\n        -1.29448992e+04, -1.21012800e+04, -9.30023489e+03,\n        -3.83220485e+03, -1.35293557e+03, -2.39553815e+04,\n        -3.50787839e+04, -4.32983892e+04],\n       [ 2.31755551e-02,  1.36972669e-03,  3.10073566e-02,\n         8.95714770e-01,  1.23159605e+05, -8.46149210e+01,\n        -3.94983330e+04, -2.62406849e+04, -1.47819273e+04,\n         1.01373745e+04,  3.46650362e+04,  3.63296076e+04,\n         2.87720185e+04,  2.77427498e+04,  2.67639279e+04,\n         2.72905992e+04,  2.74381779e+04,  2.87693629e+04,\n         2.18008833e+04,  4.23717627e+03, -7.10957534e+03,\n        -1.29620482e+04, -1.25000167e+04, -9.03037734e+03,\n        -3.96084115e+03, -1.27278527e+03, -2.43366281e+04,\n        -3.52451140e+04, -4.32780284e+04],\n       [ 2.36133672e-02,  1.35985554e-03,  3.11513958e-02,\n         8.94275972e-01,  1.23447227e+05, -7.88721079e+01,\n        -3.84923510e+04, -2.59996897e+04, -1.50010097e+04,\n         1.02508651e+04,  3.47205430e+04,  3.55858360e+04,\n         3.04894948e+04,  2.62117116e+04,  2.68062560e+04,\n         2.76499265e+04,  2.64629471e+04,  2.93271473e+04,\n         2.09505773e+04,  4.35292061e+03, -7.25190305e+03,\n        -1.28526873e+04, -1.24771698e+04, -8.93110886e+03,\n        -4.29429884e+03, -1.25765558e+03, -2.46098978e+04,\n        -3.55435309e+04, -4.37079615e+04],\n       [ 2.36439940e-02,  1.33889715e-03,  3.01748889e-02,\n         8.94229932e-01,  1.23290474e+05, -8.80150842e+01,\n        -3.88097928e+04, -2.63137034e+04, -1.44499856e+04,\n         1.03428185e+04,  3.49849551e+04,  3.63993785e+04,\n         2.87588452e+04,  2.74069044e+04,  2.61451007e+04,\n         2.81270537e+04,  2.64494936e+04,  2.92261510e+04,\n         2.06734305e+04,  4.38913406e+03, -6.90340334e+03,\n        -1.27939382e+04, -1.19989815e+04, -9.20233589e+03,\n        -4.06279150e+03, -1.35076768e+03, -2.50712735e+04,\n        -3.61164361e+04, -4.28106292e+04],\n       [ 2.38312121e-02,  1.35348442e-03,  3.11372755e-02,\n         8.92879959e-01,  1.23435424e+05, -8.75236266e+01,\n        -3.98877150e+04, -2.63840774e+04, -1.41741810e+04,\n         1.00040238e+04,  3.50671919e+04,  3.62991187e+04,\n         2.94645434e+04,  2.66796707e+04,  2.60886751e+04,\n         2.78872141e+04,  2.65673983e+04,  2.94202012e+04,\n         2.15059513e+04,  4.29327789e+03, -6.99260131e+03,\n        -1.29350082e+04, -1.21356464e+04, -9.63679687e+03,\n        -3.99062189e+03, -1.21073235e+03, -2.41469724e+04,\n        -3.49995291e+04, -4.46667304e+04],\n       [ 2.28820531e-02,  1.34709017e-03,  3.12930885e-02,\n         8.98042023e-01,  1.23363231e+05, -8.40520054e+01,\n        -3.88532804e+04, -2.57281498e+04, -1.43468660e+04,\n         1.04202224e+04,  3.44137444e+04,  3.67956433e+04,\n         2.94663774e+04,  2.69333507e+04,  2.59652714e+04,\n         2.82039633e+04,  2.67333645e+04,  2.91868638e+04,\n         2.16875935e+04,  4.18880730e+03, -7.00016622e+03,\n        -1.28357717e+04, -1.21595618e+04, -9.33735990e+03,\n        -3.90969568e+03, -1.27052486e+03, -2.42933318e+04,\n        -3.69259218e+04, -4.41040622e+04],\n       [ 2.38049951e-02,  1.39030825e-03,  3.06405290e-02,\n         8.92994409e-01,  1.24289098e+05, -8.40140819e+01,\n        -3.85337082e+04, -2.61684441e+04, -1.46993627e+04,\n         1.01634568e+04,  3.47937273e+04,  3.51012725e+04,\n         2.94226408e+04,  2.77672823e+04,  2.62946187e+04,\n         2.75564784e+04,  2.68482111e+04,  2.92367476e+04,\n         2.13296806e+04,  4.24587345e+03, -7.14882503e+03,\n        -1.30628882e+04, -1.22409211e+04, -9.52582674e+03,\n        -3.83384250e+03, -1.23631839e+03, -2.45485642e+04,\n        -3.59539920e+04, -4.45739752e+04],\n       [ 2.37778011e-02,  1.37778734e-03,  3.02599768e-02,\n         8.92655976e-01,  1.22304944e+05, -8.16288959e+01,\n        -3.89852188e+04, -2.65123655e+04, -1.43437856e+04,\n         1.02586834e+04,  3.55133375e+04,  3.57277510e+04,\n         2.93369637e+04,  2.69961210e+04,  2.64681386e+04,\n         2.77244225e+04,  2.68049260e+04,  2.95848184e+04,\n         2.10648643e+04,  4.03875969e+03, -7.09865270e+03,\n        -1.28342462e+04, -1.23781572e+04, -9.07575255e+03,\n        -4.05235581e+03, -1.28589801e+03, -2.43611569e+04,\n        -3.65053195e+04, -4.43511137e+04]]))\n\n\nLet us now visualize the fitted values of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\n-2012.193015\n\n\n1\n-699.563899\n\n\n2\n1246.127401\n\n\n...\n...\n\n\n213\n1355.434583\n\n\n214\n4926.937284\n\n\n215\n2467.814647\n\n\n\n\n216 rows √ó 1 columns\n\n\n\n\nimport scipy.stats as stats\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hours ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon, fitted=True)\nY_hat\n\n\n\n\n\n\n\n\nds\nAdd\nMulti\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n75020.843750\n75016.414062\n\n\n1\n2017-09-22 01:00:00\n73714.437500\n74739.921875\n\n\n1\n2017-09-22 02:00:00\n81196.570312\n80907.437500\n\n\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n92962.234375\n92432.453125\n\n\n1\n2017-09-23 04:00:00\n115738.320312\n112928.710938\n\n\n1\n2017-09-23 05:00:00\n113103.835938\n111366.179688\n\n\n\n\n30 rows √ó 3 columns\n\n\n\nWith the forecast method we can also extract the fitted values from the model and visualize it graphically, with the following instruction we can do it.\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nAdd\nMulti\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-13 00:00:00\n80115.0\n82127.195312\n77955.531250\n\n\n1\n2017-09-13 01:00:00\n79885.0\n80584.562500\n77934.929688\n\n\n1\n2017-09-13 02:00:00\n89325.0\n88078.875000\n84123.835938\n\n\n1\n2017-09-13 03:00:00\n101930.0\n99432.601562\n96152.687500\n\n\n1\n2017-09-13 04:00:00\n121630.0\n122571.562500\n117567.875000\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nAdd\nAdd-lo-95\nAdd-hi-95\nMulti\nMulti-lo-95\nMulti-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n75020.843750\n56622.953125\n93418.734375\n75016.414062\n62760.343750\n87272.476562\n\n\n1\n2017-09-22 01:00:00\n73714.437500\n55310.503906\n92118.367188\n74739.921875\n62483.855469\n86995.992188\n\n\n1\n2017-09-22 02:00:00\n81196.570312\n62786.074219\n99607.062500\n80907.437500\n68651.367188\n93163.500000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n92962.234375\n74304.234375\n111620.242188\n92432.453125\n79563.328125\n105301.585938\n\n\n1\n2017-09-23 04:00:00\n115738.320312\n97069.117188\n134407.531250\n112928.710938\n100059.578125\n125797.835938\n\n\n1\n2017-09-23 05:00:00\n113103.835938\n94423.414062\n131784.265625\n111366.179688\n98497.054688\n124235.304688\n\n\n\n\n30 rows √ó 7 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nAdd\nMulti\n\n\n\n\n0\n1\n2017-09-22 00:00:00\n75020.843750\n75016.414062\n\n\n1\n1\n2017-09-22 01:00:00\n73714.437500\n74739.921875\n\n\n2\n1\n2017-09-22 02:00:00\n81196.570312\n80907.437500\n\n\n...\n...\n...\n...\n...\n\n\n27\n1\n2017-09-23 03:00:00\n92962.234375\n92432.453125\n\n\n28\n1\n2017-09-23 04:00:00\n115738.320312\n112928.710938\n\n\n29\n1\n2017-09-23 05:00:00\n113103.835938\n111366.179688\n\n\n\n\n30 rows √ó 4 columns\n\n\n\n\nY_hat1 = pd.concat([df,Y_hat],  keys=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nAdd\nMulti\n\n\n\n\nunique_id\n0\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\n\n\n2\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nds\n27\n2017-09-23 03:00:00\nNaN\n1\n92962.234375\n92432.453125\n\n\n28\n2017-09-23 04:00:00\nNaN\n1\n115738.320312\n112928.710938\n\n\n29\n2017-09-23 05:00:00\nNaN\n1\n113103.835938\n111366.179688\n\n\n\n\n246 rows √ó 5 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds').tail(300)\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[ \"Add\"].plot(ax=ax, linewidth=2, color=\"yellow\")\nplot_df[ \"Multi\"].plot(ax=ax, linewidth=2, color=\"red\")\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Ads watched (hourly data)', fontsize=20)\nax.set_xlabel('Monthly [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hours ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nAdd\nMulti\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n75020.843750\n75016.414062\n\n\n1\n2017-09-22 01:00:00\n73714.437500\n74739.921875\n\n\n1\n2017-09-22 02:00:00\n81196.570312\n80907.437500\n\n\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n92962.234375\n92432.453125\n\n\n1\n2017-09-23 04:00:00\n115738.320312\n112928.710938\n\n\n1\n2017-09-23 05:00:00\n113103.835938\n111366.179688\n\n\n\n\n30 rows √ó 3 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \nforecast_df\n\n\n\n\n\n\n\n\nds\nAdd\nAdd-lo-95\nAdd-lo-80\nAdd-hi-80\nAdd-hi-95\nMulti\nMulti-lo-95\nMulti-lo-80\nMulti-hi-80\nMulti-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n75020.843750\n56622.953125\n62991.109375\n87050.578125\n93418.734375\n75016.414062\n62760.343750\n67002.601562\n83030.218750\n87272.476562\n\n\n1\n2017-09-22 01:00:00\n73714.437500\n55310.503906\n61680.750000\n85748.117188\n92118.367188\n74739.921875\n62483.855469\n66726.109375\n82753.734375\n86995.992188\n\n\n1\n2017-09-22 02:00:00\n81196.570312\n62786.074219\n69158.593750\n93234.546875\n99607.062500\n80907.437500\n68651.367188\n72893.625000\n88921.242188\n93163.500000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n92962.234375\n74304.234375\n80762.421875\n105162.054688\n111620.242188\n92432.453125\n79563.328125\n84017.789062\n100847.125000\n105301.585938\n\n\n1\n2017-09-23 04:00:00\n115738.320312\n97069.117188\n103531.187500\n127945.460938\n134407.531250\n112928.710938\n100059.578125\n104514.039062\n121343.375000\n125797.835938\n\n\n1\n2017-09-23 05:00:00\n113103.835938\n94423.414062\n100889.359375\n125318.312500\n131784.265625\n111366.179688\n98497.054688\n102951.507812\n119780.851562\n124235.304688\n\n\n\n\n30 rows √ó 11 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nAdd\nAdd-lo-95\nAdd-lo-80\nAdd-hi-80\nAdd-hi-95\nMulti\nMulti-lo-95\nMulti-lo-80\nMulti-hi-80\nMulti-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n92962.234375\n74304.234375\n80762.421875\n105162.054688\n111620.242188\n92432.453125\n79563.328125\n84017.789062\n100847.125000\n105301.585938\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n115738.320312\n97069.117188\n103531.187500\n127945.460938\n134407.531250\n112928.710938\n100059.578125\n104514.039062\n121343.375000\n125797.835938\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n113103.835938\n94423.414062\n100889.359375\n125318.312500\n131784.265625\n111366.179688\n98497.054688\n102951.507812\n119780.851562\n124235.304688\n\n\n\n\n246 rows √ó 12 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds')\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nAdd\nAdd-lo-95\nAdd-lo-80\nAdd-hi-80\nAdd-hi-95\nMulti\nMulti-lo-95\nMulti-lo-80\nMulti-hi-80\nMulti-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n92962.234375\n74304.234375\n80762.421875\n105162.054688\n111620.242188\n92432.453125\n79563.328125\n84017.789062\n100847.125000\n105301.585938\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n115738.320312\n97069.117188\n103531.187500\n127945.460938\n134407.531250\n112928.710938\n100059.578125\n104514.039062\n121343.375000\n125797.835938\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n113103.835938\n94423.414062\n100889.359375\n125318.312500\n131784.265625\n111366.179688\n98497.054688\n102951.507812\n119780.851562\n124235.304688\n\n\n\n\n246 rows √ó 12 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    df_plot = pd.concat([y_hist, y_pred]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=3 , )\n    colors = ['black', \"lime\"]\n    for model, color in zip(models, colors):\n        ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-95'], \n                        df_plot[f'{model}-hi-95'],\n                        alpha=.35,\n                        color=color,\n                        label=f'HoltWinter{model}-level-95')\n    for model, color in zip(models, colors):\n        ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-80'], \n                        df_plot[f'{model}-hi-80'],\n                        alpha=.35,\n                        color=color,\n                        label=f'HoltWinter{model}-level-80')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel(\"Ads watched (hourly data)\", fontsize=20)\n    ax.set_xlabel('Hourly', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid(True)\n    plt.show()\n\n\nplot_forecasts(df, forecast_df, models=[\"Add\"])\n\n\n\n\n\nplot_forecasts(df, forecast_df, models=[\"Multi\"])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/holtwinters.html#cross-validation",
    "href": "docs/models/holtwinters.html#cross-validation",
    "title": "Holt Winters Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=30,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAdd\nMulti\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n1\n2017-09-18 06:00:00\n2017-09-18 05:00:00\n99440.0\n134578.328125\n133820.109375\n\n\n1\n2017-09-18 07:00:00\n2017-09-18 05:00:00\n97655.0\n133548.781250\n133734.000000\n\n\n1\n2017-09-18 08:00:00\n2017-09-18 05:00:00\n97655.0\n134798.656250\n135216.046875\n\n\n...\n...\n...\n...\n...\n...\n\n\n1\n2017-09-21 21:00:00\n2017-09-20 17:00:00\n103080.0\n103021.726562\n103086.851562\n\n\n1\n2017-09-21 22:00:00\n2017-09-20 17:00:00\n95155.0\n89544.054688\n90028.406250\n\n\n1\n2017-09-21 23:00:00\n2017-09-20 17:00:00\n80285.0\n78090.210938\n78823.953125\n\n\n\n\n90 rows √ó 5 columns"
  },
  {
    "objectID": "docs/models/holtwinters.html#model-evaluation",
    "href": "docs/models/holtwinters.html#model-evaluation",
    "title": "Holt Winters Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Holt Winters Model.\n\n\nfrom datasetsforecast.losses import mse, mae, rmse\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['ds', 'cutoff', 'y']).columns.tolist()\n    evals = []\n    for model in models:\n        eval_ = df.groupby(['unique_id', 'cutoff']).apply(lambda x: metric(x['y'].values, x[model].values)).to_frame() # Calculate loss for every unique_id, model and cutoff.\n        eval_.columns = [model]\n        evals.append(eval_)\n    evals = pd.concat(evals, axis=1)\n    evals = evals.groupby(['unique_id']).mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n    evals['best_model'] = evals.idxmin(axis=1)\n    return evals\n\n\nevaluation_df = evaluate_cross_validation(crossvalidation_df, rmse)\n\nevaluation_df\n\n\n\n\n\n\n\n\nAdd\nMulti\nbest_model\n\n\nunique_id\n\n\n\n\n\n\n\n1\n12742.019531\n13012.641602\nAdd"
  },
  {
    "objectID": "docs/models/holtwinters.html#acknowledgements",
    "href": "docs/models/holtwinters.html#acknowledgements",
    "title": "Holt Winters Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/holtwinters.html#references",
    "href": "docs/models/holtwinters.html#references",
    "title": "Holt Winters Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/autoregressive.html",
    "href": "docs/models/autoregressive.html",
    "title": "AutoRegressive Model",
    "section": "",
    "text": "Introduction\nAutoregressive Models\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoRegressive with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/autoregressive.html#table-of-contents",
    "href": "docs/models/autoregressive.html#table-of-contents",
    "title": "AutoRegressive Model",
    "section": "",
    "text": "Introduction\nAutoregressive Models\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoRegressive with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/autoregressive.html#introduction",
    "href": "docs/models/autoregressive.html#introduction",
    "title": "AutoRegressive Model",
    "section": "Introduction** ",
    "text": "Introduction** \nThe autoregressive time series model (AutoRegressive) is a statistical technique used to analyze and predict univariate time series. In essence, the autoregressive model is based on the idea that previous values of the time series can be used to predict future values.\nIn this model, the dependent variable (the time series) returns to itself at different moments in time, creating a dependency relationship between past and present values. The idea is that past values can help us understand and predict future values of the series.\nThe autoregressive model can be fitted to different orders, which indicate how many past values are used to predict the present value. For example, an autoregressive model of order 1 \\((AR(1))\\) uses only the immediately previous value to predict the current value, while an autoregressive model of order \\(p (AR(p))\\) uses the \\(p\\) previous values.\nThe autoregressive model is one of the basic models of time series analysis and is widely used in a variety of fields, from finance and economics to meteorology and social sciences. The model‚Äôs ability to capture nonlinear dependencies in time series data makes it especially useful for forecasting and long-term trend analysis.\nIn a multiple regression model, we forecast the variable of interest using a linear combination of predictors. In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself."
  },
  {
    "objectID": "docs/models/autoregressive.html#definition-of-autoregressive-models",
    "href": "docs/models/autoregressive.html#definition-of-autoregressive-models",
    "title": "AutoRegressive Model",
    "section": "Definition of Autoregressive Models ",
    "text": "Definition of Autoregressive Models \nBefore giving a formal definition of the ARCH model, let‚Äôs define the components of an ARCH model in a general way: * Autoregressive, a concept that we have already known, is the construction of a univariate time series model using statistical methods, which means that the current value of a variable is influenced by past values of itself in different periods. * Heteroscedasticity means that the model can have different magnitudes or variability at different time points (variance changes over time). * Conditional, since volatility is not fixed, the reference here is the constant that we put in the model to limit heteroscedasticity and make it conditionally dependent on the previous value or values of the variable.\nThe AR model is the most basic building block of univariate time series. As you have seen before, univariate time series are a family of models that use only information about the target variable‚Äôs past to forecast its future, and do not rely on other explanatory variables.\nDefinition 1. (1) The following equation is called the autoregressive model of order \\(p\\) and denoted by \\(\\text{AR(p)}\\):\n\\[ Xt =\\varphi_0 +\\varphi_1X_{t‚àí1}+\\varphi_2X_{t‚àí2}+\\cdots+\\varphi_p X_{t‚àíp}+\\varepsilon_t  \\tag 1\\]\nwhere \\(\\{\\varepsilon_t \\} \\sim WN(0,\\sigma_{\\epsilon}^2)\\), \\(E(X_s \\varepsilon_t) = 0\\) if \\(s &lt; t\\) and \\(\\varphi_0,\\varphi_1,\\cdots ,\\varphi_p\\) are real-valued parameters (coefficients) with \\(\\varphi_p \\neq 0\\).\n\nIf a time series \\(\\{X_t \\}\\) is stationary and satisfies such an equation as (1), then we call it an \\(\\text{AR(p)}\\) process.\n\nNote the following remarks about this definition:\n\nFor simplicity, we often assume that the intercept (const term) \\(\\varphi_0 = 0\\); otherwise, we can consider \\(\\{X_t ‚àí\\mu \\}\\) where \\(\\mu =\\varphi_0 /(1‚àí\\varphi_1 ‚àí \\cdots ‚àí\\varphi_p)\\).\nWe distinguish the concept of \\(\\text{AR}\\) models from the concept of \\(\\text{AR}\\) processes. \\(\\text{AR}\\) models may or may not be stationary and \\(\\text{AR}\\) processes must be stationary.\n\\(E(X_s \\varepsilon_t) = 0(s &lt; t)\\) means that \\(X_s\\) in the past has nothing to do with \\(\\varepsilon_t\\) at the current time \\(t\\).\nLike the definition of MA models, sometimes Œµt in Eq.(1) is called the innovation or shock term.\n\nIn addition, using the backshift(see) operator \\(B\\), the \\(\\text{AR(p)}\\) model can be rewritten as\n\\[ \\varphi(B)X_t = \\varepsilon_t \\]\nwhere \\(\\varphi(z) = 1 ‚àí \\varphi_1z ‚àí \\cdots ‚àí \\varphi_p z^p\\) is called the (corresponding) \\(\\text{AR}\\) polynomial. Besides, in the Python package |StatsModels|, \\(\\varphi(B)\\) is called the \\(\\text{AR}\\) lag polynomial.\n\nDefinition of PACF\nLet \\(\\{X_t \\}\\) be a stationary time series with \\(E(X_t) = 0\\). Here the assumption \\(E(X_t ) = 0\\) is for conciseness only. If \\(E(X_t) = \\mu \\neq 0\\), it is okay to replace \\(\\{X_t \\}\\) by \\(\\{X_t ‚àí \\mu \\}\\). Now consider the linear regression (prediction) of \\(X_t\\) on \\(\\{X_{t‚àík+1:t‚àí1} \\}\\) for any integer \\(k ‚â• 2\\). We use \\(\\hat X_t\\) to denote this regression (prediction):\n\\[\\hat X_t =\\alpha_1 X_{t‚àí1}+ \\cdots +\\alpha_{k‚àí1} X_{t‚àík+1}\\]\nwhere \\(\\{\\alpha_1, \\cdots , \\alpha_{k‚àí1} \\}\\) satisfy\n\\[\\{\\alpha_1, \\cdots , \\alpha_{k‚àí1} \\}=\\argmin_{Œ≤1,¬∑¬∑¬∑,Œ≤k‚àí1} E[X_t ‚àí(\\beta_1 X_{t‚àí1} +\\cdots +\\beta_{k‚àí1}X_{t‚àík+1})]^2 \\]\nThat is, \\(\\{\\alpha_1, \\cdots , \\alpha_{k‚àí1} \\}\\) are chosen by minimizing the mean squared error of prediction. Similarly, let \\(\\hat X_{t ‚àík}\\) denote the regression (prediction) of \\(X_{t ‚àík}\\) on \\(\\{X_{t ‚àík+1:t ‚àí1} \\}\\):\n\\[\\hat X_{t‚àík} =\\eta_1 X_{t‚àí1}+ \\cdots +\\eta_{k‚àí1} X_{t‚àík+1}\\]\nNote that if \\(\\{X_t \\}\\) is stationary, then \\(\\{ \\alpha_{1:k‚àí1}\\} = \\{\\eta_{1:k‚àí1} \\}\\). Now let \\(\\hat Z_{t‚àík} = X_{t‚àík} ‚àí \\hat X_{t‚àík}\\) and \\(\\hat Z_t = X_t ‚àí \\hat X_t\\). Then \\(\\hat Z_{t‚àík}\\) is the residual of removing the effect of the intervening variables \\(\\{X_{t‚àík+1:t‚àí1} \\}\\) from \\(X_{t‚àík}\\), and \\(\\hat Z_t\\) is the residual of removing the effect of \\(\\{X_{t ‚àík+1:t ‚àí1} \\}\\) from \\(X_t\\).\nDefinition 2. The partial autocorrelation function(PACF) at lag \\(k\\) of astationary time series \\(\\{X_t \\}\\) with \\(E(X_t ) = 0\\) is\n\\[\\phi_{11} = Corr(X_{t‚àí1}, X_t ) = \\frac{Cov(X_{t‚àí1}, X_t )} {[Var(X_{t‚àí1})Var(X_t)]^1/2}=\\rho_1 \\]\nand\n\\[\\phi_{kk} = Corr(\\hat Z_{t‚àík},\\hat Z_t)=\\frac{Cov(\\hat Z_{t‚àík},\\hat Z_t)} {[Var(\\hat Z_{t‚àík})Var(\\hat Z_t)]^{1/2}}\\]\nAccording to the property of correlation coefficient (see, e.g., P172, Casella and Berger 2002), |œÜkk| ‚â§ 1. On the other hand, the following theorem paves the way to estimate the PACF of a stationary time series, and its proof can be seen in Fan and Yao (2003).\nOn the other hand, the following theorem paves the way to estimate the PACF of a stationary time series, and its proof can be seen in Fan and Yao (2003).\nTheorem 1. Let \\(\\{X_t \\}\\) be a stationary time series with \\(E(X_t) = 0\\), and \\(\\{a_{1k},\\cdots ,a_{kk} \\}\\) satisfy\n\\[\\{a_{1k},\\cdots,a_{kk} \\}=\\argmin_{a_1 ,\\cdots ,a_k} E(X_{t ‚àía1}X_{t‚àí1}‚àí\\cdots ‚àía_k X_{t‚àík})^2 \\]\nThen \\(\\phi_{kk}=a_{kk}\\) for \\(k‚â•1\\).\n\n\nProperties of Autoregressive Models\nFrom the \\(\\text{AR(p)}\\) model, namely, Eq. (1), we can see that it is in the same form as the multiple linear regression model. However, it explains current itself with its own past. Given the past\n\\[\\{X_{(t‚àíp):(t‚àí1)} \\} = \\{x_{(t‚àíp):(t‚àí1)} \\} \\]\nwe have \\[E(X_t |X_{(t‚àíp):(t‚àí1)}) = \\varphi_0 + \\varphi_1x_{t‚àí1} + \\varphi_2 x_{t‚àí2} + \\cdots + \\varphi_p x_{t‚àíp} \\]\nThis suggests that given the past, the right-hand side of this equation is a good estimate of \\(X_t\\) . Besides\n\\[Var(X_t |X_{(t ‚àíp):(t ‚àí1)}) = Var(\\varepsilon_t ) = \\sigma_{\\varepsilon}^2 \\]\nNow we suppose that the AR(p) model, namely, Eq. (1), is stationary; then we have\n\nThe model mean \\(E(_Xt)=\\mu =\\varphi_0 / (1‚àí\\varphi_1‚àí¬∑¬∑¬∑‚àí\\varphi_p)\\) .Thus,themodelmean \\(\\mu=0\\) if and only if \\(\\varphi_0 =0\\).\nIf the mean is zero or \\(\\varphi_0 = 0\\) ((3) and (4) below have the same assumption), noting that \\(E(X_t \\varepsilon_t ) = \\sigma_{\\varepsilon}^2\\) , we multiply Eq. (1) by \\(X_t\\) , take expectations, and then get\n\n\\[\\text {Var} (X_t) = \\gamma_0 = \\varphi_1 \\gamma_1 + \\varphi_2 \\gamma2 + \\cdots + \\varphi_p \\gamma_p + \\sigma_{\\varepsilon}^2\\]\nFurthermore\n\\[\\gamma_0 = \\sigma_{\\varepsilon}^2 / ( 1 ‚àí \\varphi_1 \\rho_1 ‚àí \\varphi_2 \\rho_2 ‚àí \\cdots ‚àí \\varphi_p \\rho_p )  \\]\n\nFor all \\(k &gt; p\\), the partial autocorrelation \\(\\phi_{kk} = 0\\), that is, the PACF of \\(\\text{AR(p)}\\) models cuts off after lag \\(p\\), which is very helpful in identifying an \\(\\text{AR}\\) model. In fact, at this point, the predictor or regression of \\(X_t\\) on \\(\\{X_{t‚àík+1:t‚àí1} \\}\\) is\n\n\\[\\hat X_t =\\varphi_1 X_{t‚àí1}+\\cdots +\\varphi_{k‚àí1} X_{t‚àík+1}\\]\nThus, \\(X_t ‚àí \\hat X_t = \\varepsilon_t\\). Moreover, \\(X_{t‚àík} ‚àí \\hat X_{t‚àík}\\) is a function of \\(\\{ X_{t‚àík:t‚àí1} \\}\\), and \\(\\varepsilon_t\\) is uncorrelated to everyone in \\(\\{X_{t‚àík:t‚àí1} \\}\\). Therefore\n\\[Cov(X_{t‚àík} ‚àí\\hat X_{t‚àík},X_t ‚àí\\hat X_t)=Cov(X_{t‚àík} ‚àí\\hat X_{t‚àík},\\varepsilon_t)=0.\\]\nBy Definition 2, \\(\\phi_{kk} = 0\\).\n\nWe multiply Eq.(1)by \\(X_{t‚àík}\\),take expectations,divide by \\(\\gamma_0\\),and then obtain the recursive relationship between the autocorrelations: \\[for \\ k ‚â• 1, \\rho_k = \\varphi_1 \\rho_{k‚àí1} + \\varphi_2 \\rho_{k‚àí2} + \\cdots + \\varphi_p \\rho_{k‚àíp} \\tag 2\\]\n\nFor Eq.(2), let \\(k = 1,2,¬∑¬∑¬∑ ,p\\). Then we arrive at a set of difference equations, which is known as the Yule-Walker equations. If the \\(\\text{ACF} \\{\\rho_{1:p} \\}\\) are given, then we can solve the Yule-Walker equations to obtain the estimates for \\(\\{\\varphi_{1:p} \\}\\), and the solutions are called the Yule-Walker estimates.\n\nSince the model is a stationary \\(\\text{AR(p)}\\) now, naturally it satisfies \\(X_t =\\varphi_1 X_{t‚àí1}+ \\varphi_2 X_{t‚àí2} + \\cdots + \\varphi_p X_{t‚àíp} + \\varepsilon_t\\). Hence \\(\\phi_{pp} = \\varphi_p\\). If the \\(\\text{AR(p)}\\) model is further Gaussian and a sample of size \\(\\text{T}\\) is given, then (a) \\(\\hat \\phi_{pp} ‚Üí \\varphi_p\\) as \\(T ‚Üí ‚àû\\); (b) according to Quenouille (1949), for \\(k &gt; p, \\sqrt{T} \\hat \\phi_{kk}\\) asymptotically follows the standard normal(Gaussian) distribution \\(\\text{N(0,1)}\\), or \\(\\phi_{kk}\\) is asymptotically distributed as \\(\\text{N(0, 1/T )}\\).\n\n\n\nStationarity and Causality of AR Models\nConsider the AR(1) model: \\[X_t = \\varphi X_{t ‚àí 1} + \\varepsilon_t , \\varepsilon_t \\sim W N( 0 , \\sigma_{\\varepsilon}^2 ) \\tag 3\\]\nFor \\(|\\varphi|&lt;1\\),let \\(X_{1t} =\\sum_{j=0}^{\\infty} \\varphi^j \\varepsilon_{t‚àíj}\\) and for \\(|\\varphi|&gt;1\\),let \\(X_{2t} =‚àí \\sum_{j=1}^{\\infty} \\varphi^{-j} \\varepsilon_{t+j}\\). It is easy to show that both \\(\\{X_{1t } \\}\\) and \\(\\{X_{2t } \\}\\) are stationary and satisfy Eq. (3). That is, both are the stationary solution of Eq. (3). This gives rise to a question: which one of both is preferable? Obviously, \\(\\{X_{2t } \\}\\) depends on future values of unobservable \\(\\{\\varepsilon_t \\}\\), and so it is unnatural. Hence we take \\(\\{X_{1t } \\}\\) and abandon \\(\\{X_{2t } \\}\\). In other words, we require that the coefficient \\(\\varphi\\) in Eq. (3) is less 1 in absolute value. At this point, the \\(\\text{AR}(1)\\) model is said to be causal and its causal expression is \\(X_t = \\sum_{j=0}^{\\infty} \\varphi^j \\varepsilon_{t‚àíj}\\). In general, the definition of causality is given below.\nDefinition 3 (1) A time series \\(\\{X_t \\}\\) is causal if there exist coefficients \\(\\psi_j\\) such that\n\\[X_t =\\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j}, \\ \\ \\sum_{j=0}^{\\infty} |\\psi_j |&lt; \\infty \\]\nwhere \\(\\psi_0 = 1, \\{\\varepsilon_t \\} \\sim WN(0, \\sigma_{\\varepsilon}^2 )\\). At this point, we say that the time series \\(\\{X_t \\}\\) has an \\(\\text{MA}(\\infty)\\) representation.\n\nWe say that a model is causal if the time series generated by it is causal.\n\nCausality suggests that the time series \\(\\{X_t\\}\\) is caused by the white noise (or innovations) from the past up to time t . Besides, the time series \\(\\{X_{2t } \\}\\) is an example that is stationary but not causal. In order to determine whether an \\(\\text{AR}\\) model is causal, similar to the invertibility for the \\(\\text{MA}\\) model, we have the following theorem.\nTheorem 2(CausalityTheorem) An \\(\\text{AR}\\) model defined by Eq.(1) is causal if and only if the roots of its \\(\\text{AR}\\) polynomial \\(\\varphi(z)=1‚àí\\varphi_1 z‚àí \\cdots ‚àí \\varphi_p z^p\\) exceed 1 in modulus or lie outside the unit circle on the complex plane.\nNote the following remarks: * In the light of the existence and uniqueness on page 75 of Brockwell and Davis (2016), an \\(\\text{AR}\\) model defined by Eq.(1) is stationary if and only if its \\(\\text{AR}\\) polynomial \\(\\varphi(z)=1‚àí\\varphi_1 z‚àí \\cdots ‚àí \\varphi_p z^p \\neq 0\\) for all \\(|z|=1\\) or all the roots of the \\(\\text{AR}\\) polynomial do not lie on the unit circle. Hence for the AR model defined by Eq. (1), its stationarity condition is weaker than its causality condition.\n\nA causal time series is surely a stationary one. So an \\(\\text{AR}\\) model that satisfies the causal condition is naturally stationary. But a stationary \\(\\text{AR}\\) model is not necessarily causal.\nIf the time series \\(\\{X_t \\}\\) generated by Eq. (1) is not from the remote past, namely, \\[t \\in T = {\\cdots ,‚àín,\\cdots ,‚àí1,0,1,\\cdots ,n,\\cdots}\\]\n\nbut starts from an initial value \\(X_0\\), then it may be nonstationary, not to mention causality.\n\nAccording to the relationship between the roots and the coefficients of the degree 2 polynomial \\(\\varphi(z) = 1 ‚àí \\varphi_1 z ‚àí \\varphi_2 z^2\\), it may be proved that both of the roots of the polynomial exceed 1 in modulus if and only if\n\n\\[\\begin{equation}\n    \\left\\{\n        \\begin{array}{ll}\n         |\\varphi_2| &lt; 1,      \\\\\n         \\varphi_2 + \\varphi_1 &lt; 1, \\\\\n         \\varphi_2 - \\varphi_1 &lt; 1,\\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nThus, we can conveniently use the three inequations to decide whether a \\(\\text{AR(2)}\\) model is causal or not.\n\nIt may be shown that for an \\(\\text{AR(p)}\\) model defined by Eq. (1), the coefficients \\(\\{\\psi_j \\}\\) in Definition 3 satisfy \\(\\psi_0=1\\) and\n\n\\[\\psi_j=\\sum_{k=1}^{j} \\varphi '_k \\psi_{j-k}, \\ \\ j \\geq 1 \\ where \\ \\ \\varphi '_k =\\varphi_k  \\ \\ if \\ \\ k \\leq p \\ \\ and \\ \\ \\varphi '_k =0 \\ \\ if \\ \\ k&gt;p \\]\n\n\nAutocorrelation: the past influences the present\nThe autoregressive model describes a relationship between the present of a variable and its past. Therefore, it is suitable for variables in which the past and present values are correlated.\nAs an intuitive example, consider the waiting line at the doctor. Imagine that the doctor has a plan in which each patient has 20 minutes with him. If each patient takes exactly 20 minutes, this works well. But what if a patient takes a little longer? An autocorrelation could be present if the duration of one query has an impact on the duration of the next query. So if the doctor needs to speed up an appointment because the previous appointment took too long, look at a correlation between the past and the present. Past values influence future values.\n\n\nPositive and negative autocorrelation\nLike ‚Äúregular‚Äù correlation, autocorrelation can be positive or negative. Positive autocorrelation means that a high value now is likely to give a high value in the next period. This can be observed, for example, in stock trading: as soon as a lot of people want to buy a stock, its price goes up. This positive trend makes people want to buy this stock even more as it has positive returns. The more people buy the stock, the higher it goes and the more people will want to buy it.\nA positive correlation also works in downtrends. If today‚Äôs stock value is low, tomorrow‚Äôs value is likely to be even lower as people start selling. When many people sell, the value falls, and even more people will want to sell. This is also a case of positive autocorrelation since the past and the present go in the same direction. If the past is low, the present is low; and if the past is high, the present is high.\nThere is negative autocorrelation if two trends are opposite. This is the case in the example of the duration of the doctor‚Äôs visit. If one query takes longer, the next one will be shorter. If one visit takes less time, the doctor may take a little longer for the next one.\n\n\nStationarity and the ADF test\nThe problem of having a trend in our data is general in univariate time series modeling. The stationarity of a time series means that a time series does not have a (long-term) trend: it is stable around the same average. Otherwise, a time series is said to be non-stationary.\nIn theory, AR models can have a trend coefficient in the model, but since stationarity is an important concept in general time series theory, it‚Äôs best to learn to deal with it right away. Many models can only work on stationary time series.\nA time series that is growing or falling strongly over time is obvious to spot. But sometimes it‚Äôs hard to tell if a time series is stationary. This is where the Augmented Dickey Fuller (ADF) test comes in handy."
  },
  {
    "objectID": "docs/models/autoregressive.html#loading-libraries-and-data",
    "href": "docs/models/autoregressive.html#loading-libraries-and-data",
    "title": "AutoRegressive Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport pandas as pd\n\nimport scipy.stats as stats\n\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\ndf= pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/catfish.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nDate\nTotal\n\n\n\n\n0\n1986-1-01\n9034\n\n\n1\n1986-2-01\n9596\n\n\n2\n1986-3-01\n10558\n\n\n3\n1986-4-01\n9002\n\n\n4\n1986-5-01\n9239\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1986-1-01\n9034\n1\n\n\n1\n1986-2-01\n9596\n1\n\n\n2\n1986-3-01\n10558\n1\n\n\n3\n1986-4-01\n9002\n1\n\n\n4\n1986-5-01\n9239\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable ds is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/autoregressive.html#explore-data-with-the-plot-method",
    "href": "docs/models/autoregressive.html#explore-data-with-the-plot-method",
    "title": "AutoRegressive Model",
    "section": "Explore data with the plot method ",
    "text": "Explore data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nThe Augmented Dickey-Fuller Test\nAn Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\nNull Hypothesis: Time Series is non-stationary. It gives a time-dependent trend. Alternate Hypothesis: Time Series is stationary. In another term, the series doesn‚Äôt depend on time.\nADF or t Statistic &lt; critical values: Reject the null hypothesis, time series is stationary. ADF or t Statistic &gt; critical values: Failed to reject the null hypothesis, time series is non-stationary.\nLet‚Äôs check if our series that we are analyzing is a stationary series. Let‚Äôs create a function to check, using the Dickey Fuller test\n\nfrom statsmodels.tsa.stattools import adfuller\n\n\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],'Sales')\n\nDickey-Fuller test results for columns: Sales\nTest Statistic          -1.589903\np-value                  0.488664\nNo Lags Used            14.000000\n                          ...    \nCritical Value (1%)     -3.451691\nCritical Value (5%)     -2.870939\nCritical Value (10%)    -2.571778\nLength: 7, dtype: float64\nConclusion:====&gt;\nThe null hypothesis cannot be rejected\nThe data is not stationary\n\n\nIn the previous result we can see that the Augmented_Dickey_Fuller test gives us a p-value of 0.488664, which tells us that the null hypothesis cannot be rejected, and on the other hand the data of our series are not stationary.\nWe need to differentiate our time series, in order to convert the data to stationary.\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"].diff().dropna(),\"Sales\")\n\nDickey-Fuller test results for columns: Sales\nTest Statistic          -4.310935\np-value                  0.000425\nNo Lags Used            17.000000\n                          ...    \nCritical Value (1%)     -3.451974\nCritical Value (5%)     -2.871063\nCritical Value (10%)    -2.571844\nLength: 7, dtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\nBy applying a differential, our time series now is stationary.\n\ndef tsplot(y, lags=None, figsize=(12, 7), style='bmh'): # [3]\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis plot\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()\n\n\ntsplot(df[\"y\"].diff().dropna(), lags=20);\n\n\n\n\nAs you can see, based on the blue background shaded area of the graph, the PACF shows the first, second, third, fourth, sixth, seventh, ninth, and tenth etc. delay outside the shaded area. This means that it would be interesting to also include these lags in the AR model.\n\n\nHow many lags should we include?\nNow, the big question in time series analysis is always how many lags to include. This is called the order of the time series. The notation is AR(1) for order 1 and AR(p) for order p.\nThe order is up to you. Theoretically speaking, you can base your order on the PACF chart. Theory tells you to take the number of lags before you get an autocorrelation of 0. All other lags should be 0.\nIn theory, you often see great charts where the first peak is very high and the rest equal zero. In those cases, the choice is easy: you are working with a very ‚Äúpure‚Äù example of AR(1). Another common case is when your autocorrelation starts high and slowly decreases to zero. In this case, you should use all delays where the PACF is not yet zero.\nHowever, in practice, it is not always that simple. Remember the famous saying ‚Äúall models are wrong, but some are useful‚Äù. It is very rare to find cases that fit an AR model perfectly. In general, the autoregression process can help explain part of the variation of a variable, but not all.\nIn practice, you will try to select the number of lags that gives your model the best predictive performance. The best predictive performance is often not defined by looking at autocorrelation plots: those plots give you a theoretical estimate. However, predictive performance is best defined by model evaluation and benchmarking, using the techniques you have seen in Module 2. Later in this module, we will see how to use model evaluation to choose a performance order for the AR model. But before we get into that, it‚Äôs time to dig into the exact definition of the AR model."
  },
  {
    "objectID": "docs/models/autoregressive.html#split-the-data-into-training-and-testing",
    "href": "docs/models/autoregressive.html#split-the-data-into-training-and-testing",
    "title": "AutoRegressive Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our AutoRegressive model 2. Data to test our model\nFor the test data we will use the last 12 months to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2011-12-01'] \ntest = df[df.ds&gt;'2011-12-01']\n\n\ntrain.shape, test.shape\n\n((312, 3), (12, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.show()"
  },
  {
    "objectID": "docs/models/autoregressive.html#cross-validation",
    "href": "docs/models/autoregressive.html#cross-validation",
    "title": "AutoRegressive Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=6,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoRegressive\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2009-01-01\n2008-12-01\n19262.0\n24295.751953\n\n\n1\n2009-02-01\n2008-12-01\n20658.0\n23993.804688\n\n\n1\n2009-03-01\n2008-12-01\n22660.0\n21200.978516\n\n\n...\n...\n...\n...\n...\n\n\n1\n2011-10-01\n2010-12-01\n12893.0\n19350.546875\n\n\n1\n2011-11-01\n2010-12-01\n11843.0\n16900.646484\n\n\n1\n2011-12-01\n2010-12-01\n11321.0\n18160.392578\n\n\n\n\n60 rows √ó 4 columns\n\n\n\nWe‚Äôll now plot the forecast for each cutoff period. To make the plots clearer, we‚Äôll rename the actual values in each period.\n\ncrossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True) # rename actual values \n\ncutoff = crossvalidation_df['cutoff'].unique()\n\nfor k in range(len(cutoff)): \n    cv = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff[k]]\n    StatsForecast.plot(df, cv.loc[:, cv.columns != 'cutoff'])"
  },
  {
    "objectID": "docs/models/autoregressive.html#model-evaluation",
    "href": "docs/models/autoregressive.html#model-evaluation",
    "title": "AutoRegressive Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, AutoRegressive.\n\n\nrmse = rmse(crossvalidation_df['actual'], crossvalidation_df[\"AutoRegressive\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  3566.5479\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, model):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=12)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, test, Y_hat, model=\"AutoRegressive\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nAutoRegressive\n961.773193\n7.271437\n0.601651\n1194.660588\n6.970027"
  },
  {
    "objectID": "docs/models/autoregressive.html#acknowledgements",
    "href": "docs/models/autoregressive.html#acknowledgements",
    "title": "AutoRegressive Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/autoregressive.html#references",
    "href": "docs/models/autoregressive.html#references",
    "title": "AutoRegressive Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/autoarima.html",
    "href": "docs/models/autoarima.html",
    "title": "AutoARIMA Model",
    "section": "",
    "text": "What is AutoArima with StatsForecast?\nDefinition of the Arima model\nAdvantages of using AutoArima\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoARIMA with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/autoarima.html#table-of-contents",
    "href": "docs/models/autoarima.html#table-of-contents",
    "title": "AutoARIMA Model",
    "section": "",
    "text": "What is AutoArima with StatsForecast?\nDefinition of the Arima model\nAdvantages of using AutoArima\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoARIMA with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/autoarima.html#what-is-autoarima-with-statsforecast",
    "href": "docs/models/autoarima.html#what-is-autoarima-with-statsforecast",
    "title": "AutoARIMA Model",
    "section": "What is AutoArima with StatsForecast? ",
    "text": "What is AutoArima with StatsForecast? \nAn autoARIMA is a time series model that uses an automatic process to select the optimal ARIMA (Autoregressive Integrated Moving Average) model parameters for a given time series. ARIMA is a widely used statistical model for modeling and predicting time series.\nThe process of automatic parameter selection in an autoARIMA model is performed using statistical and optimization techniques, such as the Akaike Information Criterion (AIC) and cross-validation, to identify optimal values for autoregression, integration, and moving average parameters. of the ARIMA model.\nAutomatic parameter selection is useful because it can be difficult to determine the optimal parameters of an ARIMA model for a given time series without a thorough understanding of the underlying stochastic process that generates the time series. The autoARIMA model automates the parameter selection process and can provide a fast and effective solution for time series modeling and forecasting.\nThe statsforecast.models library brings the AutoARIMA function from Python provides an implementation of autoARIMA that allows to automatically select the optimal parameters for an ARIMA model given a time series."
  },
  {
    "objectID": "docs/models/autoarima.html#definition-of-the-arima-model",
    "href": "docs/models/autoarima.html#definition-of-the-arima-model",
    "title": "AutoARIMA Model",
    "section": "Definition of the Arima model ",
    "text": "Definition of the Arima model \nAn Arima model (autoregressive integrated moving average) process is the combination of an autoregressive process AR(p), integration I(d), and the moving average process MA(q).\nJust like the ARMA process, the ARIMA process states that the present value is dependent on past values, coming from the AR(p) portion, and past errors, coming from the MA(q) portion. However, instead of using the original series, denoted as yt, the ARIMA process uses the differenced series, denoted as \\(y'_{t}\\). Note that \\(y'_{t}\\) can represent a series that has been differenced more than once.\nTherefore, the mathematical expression of the ARIMA(p,d,q) process states that the present value of the differenced series \\(y'_{t}\\) is equal to the sum of a constant \\(C\\), past values of the differenced series \\(\\phi_{p}y'_{t-p}\\), the mean of the differenced series \\(\\mu\\), past error terms \\(\\theta_{q}\\varepsilon_{t-q}\\), and a current error term \\(\\varepsilon_{t}\\), as shown in equation\n\\[\\begin{equation}\n  y'_{t} = c + \\phi_{1}y'_{t-1} + \\cdots + \\phi_{p}y'_{t-p}\n     + \\theta_{1}\\varepsilon_{t-1} + \\cdots + \\theta_{q}\\varepsilon_{t-q} + \\varepsilon_{t},  \\tag{1}\n\\end{equation}\\]\nwhere \\(y'_{t}\\) is the differenced series (it may have been differenced more than once). The ‚Äúpredictors‚Äù on the right hand side include both lagged values of \\(y_{t}\\) and lagged errors. We call this an ARIMA( p,d,q) model, where\n\n\n\np\norder of the autoregressive part\n\n\nd\ndegree of first differencing involved\n\n\nq\norder of the moving average part\n\n\n\nThe same stationarity and invertibility conditions that are used for autoregressive and moving average models also apply to an ARIMA model.\nMany of the models we have already discussed are special cases of the ARIMA model, as shown in Table\n\n\n\n\n\n\n\n\n\nModel\np d q\nDifferenced\nMethod\n\n\n\n\nArima(0,0,0)\n0 0 0\n\\(y_t=Y_t\\)\nWhite noise\n\n\nARIMA (0,1,0)\n0 1 0\n\\(y_t = Y_t - Y_{t-1}\\)\nRandom walk\n\n\nARIMA (0,2,0)\n0 2 0\n\\(y_t = Y_t - 2Y_{t-1} + Y_{t-2}\\)\nConstant\n\n\nARIMA (1,0,0)\n1 0 0\n$Y_t = + 1 Y{t-1} + \\(| AR(1): AR(1): First-order regression model| |ARIMA (2, 0, 0)|2 0 0 |\\)Y_t = 0 + 1 Y{t-1} + 2 Y{t-2} + \\(| AR(2): Second-order regression model| |ARIMA (1, 1, 0)|1 1 0 |\\)Y_t = + Y{t-1} + 1 (Y{t-1}- Y_{t-2})$\nDifferenced first-order autoregressive model\n\n\nARIMA (0, 1, 1)\n0 1 1\n\\(\\hat Y_t = Y_{t-1} - \\Phi_1 e^{t-1}\\)\nSimple exponential smoothing\n\n\nARIMA (0, 0, 1)\n0 0 1\n\\(\\hat Y_t = \\mu_0+ \\epsilon_t ‚Äì \\omega_1 \\epsilon_{t-1}\\)\nMA(1): First-order regression model\n\n\nARIMA (0, 0, 2)\n0 0 2\n\\(\\hat Y_t = \\mu_0+ \\epsilon_t ‚Äì \\omega_1 \\epsilon_{t-1} ‚Äì \\omega_2 \\epsilon_{t-2}\\)\nMA(2): Second-order regression model\n\n\nARIMA (1, 0, 1)\n1 0 1\n\\(\\hat Y_t = \\Phi_0 + \\Phi_1 Y_{t-1}+ \\epsilon_t ‚Äì \\omega_1 \\epsilon_{t-1}\\)\nARMA model\n\n\nARIMA (1, 1, 1)\n1 1 1\n\\(\\Delta Y_t = \\Phi_1 Y_{t-1} + \\epsilon_t - \\omega_1 \\epsilon_{t-1}\\)\nARIMA model\n\n\nARIMA (1, 1, 2)\n1 1 2\n\\(\\hat Y_t = Y_{t-1} + \\Phi_1 (Y_{t-1} - Y_{t-2} )- \\Theta_1 e_{t-1} - \\Theta_1 e_{t-1}\\)\nDamped-trend linear Exponential smoothing\n\n\nARIMA (0, 2, 1) OR (0,2,2)\n0 2 1\n\\(\\hat Y_t = 2 Y_{t-1} - Y_{t-2} - \\Theta_1 e_{t-1} - \\Theta_2 e_{t-2}\\)\nLinear exponential smoothing\n\n\n\nOnce we start combining components in this way to form more complicated models, it is much easier to work with the backshift notation. For example, Equation (1) can be written in backshift notation as:\n\\[\\begin{equation}\n\\tag{2}\n  \\begin{array}{c c c c}\n    (1-\\phi_1B - \\cdots - \\phi_p B^p) & (1-B)^d y_{t} &= &c + (1 + \\theta_1 B + \\cdots + \\theta_q B^q)\\varepsilon_t\\\\\n    {\\uparrow} & {\\uparrow} & &{\\uparrow}\\\\\n    \\text{AR($p$)} & \\text{$d$ differences} & & \\text{MA($q$)}\\\\\n  \\end{array}\n\\end{equation}\\]\nSelecting appropriate values for p, d and q can be difficult. However, the AutoARIMA() function from statsforecast will do it for you automatically.\nFor more information here"
  },
  {
    "objectID": "docs/models/autoarima.html#loading-libraries-and-data",
    "href": "docs/models/autoarima.html#loading-libraries-and-data",
    "title": "AutoARIMA Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \nUsing an AutoARIMA() model to model and predict time series has several advantages, including:\n\nAutomation of the parameter selection process: The AutoARIMA() function automates the ARIMA model parameter selection process, which can save the user time and effort by eliminating the need to manually try different combinations of parameters.\nReduction of prediction error: By automatically selecting optimal parameters, the ARIMA model can improve the accuracy of predictions compared to manually selected ARIMA models.\nIdentification of complex patterns: The AutoARIMA() function can identify complex patterns in the data that may be difficult to detect visually or with other time series modeling techniques.\nFlexibility in the choice of the parameter selection methodology: The ARIMA Model can use different methodologies to select the optimal parameters, such as the Akaike Information Criterion (AIC), cross-validation and others, which allows the user to choose the methodology that best suits their needs.\n\nIn general, using the AutoARIMA() function can help improve the efficiency and accuracy of time series modeling and forecasting, especially for users who are inexperienced with manual parameter selection for ARIMA models.\n\nMain results\nWe compared accuracy and speed against pmdarima, Rob Hyndman‚Äôs forecast package and Facebook‚Äôs Prophet. We used the Daily, Hourly and Weekly data from the M4 competition.\nThe following table summarizes the results. As can be seen, our auto_arima is the best model in accuracy (measured by the MASE loss) and time, even compared with the original implementation in R.\n\n\n\n\n\n\n\n\n\n\n\ndataset\nmetric\nauto_arima_nixtla\nauto_arima_pmdarima [1]\nauto_arima_r\nprophet\n\n\n\n\nDaily\nMASE\n3.26\n3.35\n4.46\n14.26\n\n\nDaily\ntime\n1.41\n27.61\n1.81\n514.33\n\n\nHourly\nMASE\n0.92\n‚Äî\n1.02\n1.78\n\n\nHourly\ntime\n12.92\n‚Äî\n23.95\n17.27\n\n\nWeekly\nMASE\n2.34\n2.47\n2.58\n7.29\n\n\nWeekly\ntime\n0.42\n2.92\n0.22\n19.82\n\n\n\n[1] The model auto_arima from pmdarima had a problem with Hourly data. An issue was opened.\nThe following table summarizes the data details.\n\n\n\n\n\n\n\n\n\n\n\ngroup\nn_series\nmean_length\nstd_length\nmin_length\nmax_length\n\n\n\n\nDaily\n4,227\n2,371\n1,756\n107\n9,933\n\n\nHourly\n414\n901\n127\n748\n1,008\n\n\nWeekly\n359\n1,035\n707\n93\n2,610"
  },
  {
    "objectID": "docs/models/autoarima.html#loading-libraries-and-data-1",
    "href": "docs/models/autoarima.html#loading-libraries-and-data-1",
    "title": "AutoARIMA Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport numpy as np\nimport pandas as pd\n\nimport scipy.stats as stats\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nLoading Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/candy_production.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nobservation_date\nIPG3113N\n\n\n\n\n0\n1972-01-01\n85.6945\n\n\n1\n1972-02-01\n71.8200\n\n\n2\n1972-03-01\n66.0229\n\n\n3\n1972-04-01\n64.5645\n\n\n4\n1972-05-01\n65.0100\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1972-01-01\n85.6945\n1\n\n\n1\n1972-02-01\n71.8200\n1\n\n\n2\n1972-03-01\n66.0229\n1\n\n\n3\n1972-04-01\n64.5645\n1\n\n\n4\n1972-05-01\n65.0100\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds            object\ny            float64\nunique_id     object\ndtype: object\n\n\nWe need to convert ds from the object type to datetime.\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/autoarima.html#explore-data-with-the-plot-method",
    "href": "docs/models/autoarima.html#explore-data-with-the-plot-method",
    "title": "AutoARIMA Model",
    "section": "Explore data with the plot method ",
    "text": "Explore data with the plot method \nPlot a series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df, engine=\"matplotlib\")\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=60, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=60, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * multiplicative\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]"
  },
  {
    "objectID": "docs/models/autoarima.html#multiplicative-time-series",
    "href": "docs/models/autoarima.html#multiplicative-time-series",
    "title": "AutoARIMA Model",
    "section": "Multiplicative time series",
    "text": "Multiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"add\", period=12)\na.plot();"
  },
  {
    "objectID": "docs/models/autoarima.html#split-the-data-into-training-and-testing",
    "href": "docs/models/autoarima.html#split-the-data-into-training-and-testing",
    "title": "AutoARIMA Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our AutoArima model 2. Data to test our model\nFor the test data we will use the last 12 months to test and evaluate the performance of our model.\n\nY_train_df = df[df.ds&lt;='2016-08-01'] \nY_test_df = df[df.ds&gt;'2016-08-01']\n\n\nY_train_df.shape, Y_test_df.shape\n\n((536, 3), (12, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(Y_train_df,x=\"ds\", y=\"y\", label=\"Train\")\nsns.lineplot(Y_test_df, x=\"ds\", y=\"y\", label=\"Test\")\nplt.show()"
  },
  {
    "objectID": "docs/models/autoarima.html#cross-validation",
    "href": "docs/models/autoarima.html#cross-validation",
    "title": "AutoARIMA Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=Y_train_df,\n                                         h=12,\n                                         step_size=12,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoARIMA\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2011-09-01\n2011-08-01\n93.906197\n104.758850\n\n\n1\n2011-10-01\n2011-08-01\n116.763397\n118.705879\n\n\n1\n2011-11-01\n2011-08-01\n116.825798\n116.834129\n\n\n1\n2011-12-01\n2011-08-01\n114.956299\n117.070084\n\n\n1\n2012-01-01\n2011-08-01\n99.966202\n103.552246"
  },
  {
    "objectID": "docs/models/autoarima.html#model-evaluation",
    "href": "docs/models/autoarima.html#model-evaluation",
    "title": "AutoARIMA Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, AutoArima.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"AutoARIMA\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  5.5258384\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import mae, mape, mase, rmse, smape\n\n\ndef evaluate_performace(y_hist, y_true, model):\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=12)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(Y_train_df, Y_hat_df, model='AutoARIMA')\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nAutoARIMA\n5.26042\n4.794312\n1.015379\n6.021264\n4.915602"
  },
  {
    "objectID": "docs/models/autoarima.html#acknowledgements",
    "href": "docs/models/autoarima.html#acknowledgements",
    "title": "AutoARIMA Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/autoarima.html#references",
    "href": "docs/models/autoarima.html#references",
    "title": "AutoARIMA Model",
    "section": "References ",
    "text": "References \n\nNixtla-Arima\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù."
  },
  {
    "objectID": "docs/models/autotheta.html",
    "href": "docs/models/autotheta.html",
    "title": "AutoTheta Model",
    "section": "",
    "text": "Introduction\nTheta Models\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/autotheta.html#table-of-contents",
    "href": "docs/models/autotheta.html#table-of-contents",
    "title": "AutoTheta Model",
    "section": "",
    "text": "Introduction\nTheta Models\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of AutoTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/autotheta.html#introduction",
    "href": "docs/models/autotheta.html#introduction",
    "title": "AutoTheta Model",
    "section": "Introduction",
    "text": "Introduction\nThe development of accurate, robust and reliable forecasting methods for univariate time series is very important when large numbers of time series are involved in the modelling and forecasting process. In industrial settings, it is very common to work with large lines of products; thus, efficient sales and operational planning (S&OP) depend heavily on accurate forecasting methods.\nThe Theta method (Assimakopoulos & Nikolopoulos, 2000, hereafter A&N) is applied to non-seasonal or deseasonalised time series, where the deseasonalisation is usually performed via the multiplicative classical decomposition. The method decomposes the original time series into two new lines through the so-called theta coefficients, denoted by \\({\\theta}_1\\) and \\({\\theta}_2\\) for \\({\\theta}_1, {\\theta}_2 \\in \\mathbb{R}\\), which are applied to the second difference of the data. The second differences are reduced when \\({\\theta}&lt;1\\), resulting in a better approximation of the long-term behaviour of the series (Assimakopoulos, 1995). If \\({\\theta}\\) is equal to zero, the new line is a straight line. When \\({\\theta}&gt;1\\) the local curvatures are increased, magnifying the short-term movements of the time series (A&N). The new lines produced are called theta lines, denoted here by \\(\\text{Z}(\\theta_1)\\) and \\(\\text{Z}(\\theta_2)\\). These lines have the same mean value and slope as the original data, but the local curvatures are either filtered out or enhanced, depending on the value of the \\(\\theta\\) coefficient.\nIn other words, the decomposition process has the advantage of exploiting information in the data that usually cannot be captured and modelled completely through the extrapolation of the original time series. The theta lines can be regarded as new time series and are extrapolated separately using an appropriate forecasting method. Once the extrapolation of each theta line has been completed, recomposition takes place through a combination scheme in order to calculate the point forecasts of the original time series. Combining has long been considered as a useful practice in the forecasting literature (for example, Clemen, 1989, Makridakis and Winkler, 1983, Petropoulos et al., 2014), and therefore its application to the Theta method is expected to result in more accurate and robust forecasts.\nThe Theta method is quite versatile in terms of choosing the number of theta lines, the theta coefficients and the extrapolation methods, and combining these to obtain robust forecasts. However, A&N proposed a simplified version involving the use of only two theta lines with prefixed \\(\\theta\\) coefficients that are extrapolated over time using a linear regression (LR) model for the theta line with \\({\\theta}_1 =0\\) and simple exponential smoothing (SES) for the theta line with \\({\\theta}_2 =2\\). The final forecasts are produced by combining the forecasts of the two theta lines with equal weights.\nThe performance of the Theta method has also been confirmed by other empirical studies (for example Nikolopoulos et al., 2012, Petropoulos and Nikolopoulos, 2013). Moreover, Hyndman and Billah (2003), hereafter H&B, showed that the simple exponential smoothing with drift model (SES-d) is a statistical model for the simplified version of the Theta method. More recently, Thomakos and Nikolopoulos (2014) provided additional theoretical insights, while Thomakos and Nikolopoulos (2015) derived new theoretical formulations for the application of the method to multivariate time series, and investigated the conditions under which the bivariate Theta method is expected to forecast better than the univariate one. Despite these advances, we believe that the Theta method deserves more attention from the forecasting community, given its simplicity and superior forecasting performance.\nOne key aspect of the Theta method is that, by definition, it is dynamic. One can choose different theta lines and combine the produced forecasts using either equal or unequal weights. However, AN limit this important property by fixing the theta coefficients to have predefined values.\nThe contributions of this work are fourfold. First, we extend the A&N method by the optimal selection of the theta line that describes the short-term movements of the series best, maintaining the long-term component. The forecasts derived from the two theta lines are combined using appropriate weights, which ensures the recomposition of the original time series. Second, we provide theoretical and practical links between the newly proposed model, the original Theta method and the SES-d model. Third, we also perform a further extension of the model that allows the regression line (the long term component) to be revised at every time period."
  },
  {
    "objectID": "docs/models/autotheta.html#theta-method",
    "href": "docs/models/autotheta.html#theta-method",
    "title": "AutoTheta Model",
    "section": "Theta method ",
    "text": "Theta method \n\nThe original Theta method\nOriginally, AN proposed the theta line as the solution of the equation\n\\[\\nabla^2 \\text{Z}_t (\\theta) =\\theta \\nabla^2 Y_t, \\ \\ t=3, \\cdots, n \\tag 1\\]\nwhere \\(Y_1, \\cdots Y_n\\) is the original time series (non-seasonal or deseasonalised) and \\(\\nabla\\) is the difference operator (i.e., \\(\\nabla X_t = X_t - X_{t-1}\\)). The initial values of \\(\\text{Z}_1\\) and \\(\\text{Z}_2\\) are obtained by minimising $_{t=1}^n [Y_t -_t()]^2 $. However, an analytical solution to compute the \\(\\text{Z}(\\theta)\\) was obtained by H&B, which is given by\n\\[\\text{Z}_t(\\theta)=\\theta Y_t +(1-\\theta)(A_n +B_n t) \\ \\ t=1, \\cdots n \\tag 2 \\]\nwhere \\(\\text{A}_n\\) and \\(\\text{B}_n\\) are the minimum square coefficients of a simple linear regression over \\(Y_1, \\cdots Y_n\\) against \\(1, \\cdots n \\ \\), given by\n\\[\\text{A}_n =\\frac{1}{n} \\sum_{t=1}^{n} Y_t -\\frac{n+1}{2} \\text{B}_n ; \\ \\ \\text{B}_n= \\frac{6}{n^2 -1} (\\frac{2}{n} \\sum_{t=1}^{n} tY_t -\\frac{1+n}{n} \\sum_{t=1}^{n} Y_t ) \\tag 3\\]\nFrom this point of view, the theta lines can be interpreted as functions of the linear regression model applied to the data directly. However, note that \\(\\text{A}_n\\) and \\(\\text{B}_n\\) are only functions of the original data, not parameters of the Theta method.\nFinally, the forecasts produced by the Theta method for \\(h\\) steps ahead of are an ad-hoc combination (50%‚Äì50%) of the extrapolations of $(0) $ and \\(\\text{Z}(2)\\) by the linear regression model and the simple exponential smoothing model respectively. We will refer to the above setup as the standard Theta method (STheta).\nThe steps for building the STheta method of AN are as follows: 1. Deseasonalisation: The time series is tested for statistically significant seasonal behaviour. A time series is seasonal if \\[|r_m|&gt;q_{1-\\alpha/2} \\sqrt{\\frac{1+2 \\sum_{i=1}^{m-1} r_{i}^2}{n} } \\]\nwhere \\(r_k\\) denotes the lag \\(k\\) autocorrelation function, \\(m\\) is the number of the periods within a seasonal cycle (for example, 12 for monthly data), \\(n\\) is the sample size, \\(q\\) is the quantile function of the standard normal distribution, and \\((1-\\alpha)\\%\\) is the confidence level. A&N opted for a 90% confidence level. If the time series is identified as seasonal, then it is deseasonalised via the classical decomposition method, assuming the seasonal component to have a multiplicative relationship.\n\nDecomposition: The seasonally adjusted time series is decomposed into two theta lines, the linear regression line \\(\\text{Z}(0)\\) and the theta line \\(\\text{Z}(2)\\).\nExtrapolation: \\(\\text{Z}(0)\\) is extrapolated as a normal linear regression line, while \\(\\text{Z}(2)\\) is extrapolated using SES.\nCombination: The final forecast is a combination of the forecasts of the two theta lines using equal weights.\nReseasonalisation: If the series was identified as seasonal in step 1, then the final forecasts are multiplied by the respective seasonal indices.\n\n\n\nModels for optimising the Theta method\nAssume that either the time series \\(Y_1, \\cdots Y_n\\) is non-seasonal or it has been seasonally adjusted using the multiplicative classical decomposition approach.\nLet \\(X_t\\) be the linear combination of two theta lines, \\[X_t=\\omega \\text{Z}_t (\\theta_1) +(1-\\omega) \\text{Z}_t (\\theta_2) \\tag 4\\]\nwhere \\(\\omega \\in [0,1]\\) is the weight parameter. Assuming that \\(\\theta_1 &lt;1\\) and \\(\\theta_2 \\geq 1\\), the weight \\(\\omega\\) can be derived as \\[\\omega:=\\omega(\\theta_1, \\theta_2)=\\frac{\\theta_2 -1}{\\theta_2 -\\theta_1} \\tag 5\\]\nIt is straightforward to see from Eqs. (4), (5) that \\(X_t=Y_t, \\ t=1, \\cdots n\\) i.e., the weights are calculated properly in such a way that Eq. (4) reproduces the original series. In Theorem 1 of Appendix A , we prove that the solution is unique and that the error from not choosing the optimal weights (\\(\\omega\\) and \\(1-\\omega\\)) s proportional to the error of a linear regression model. As a consequence, the STheta method is given simply by setting \\(\\theta_1=0\\) and \\(\\theta_2=2\\) while from Eq. (5) we get \\(\\omega=0.5\\). Thus, Eqs. (4), (5) allow us to construct a generalisation of the Theta model that maintains the re-composition propriety of the original time series for any theta lines \\(\\text{Z}_t (\\theta_1)\\) and \\(\\text{Z}_t (\\theta_2)\\).\nIn order to maintain the modelling of the long-term component and retain a fair comparison with the STheta method, in this work we fix \\(\\theta_1=0\\) and focus on the optimisation of the short-term component, \\(\\theta_2=0\\) with \\(\\theta \\geq 1\\). Thus, \\(\\theta\\) is the only parameter that requires estimation so far. The theta decomposition is now given by\n\\[Y_t=(1-\\frac{1}{\\theta}) (\\text{A}_n+\\text{B}_n t)+ \\frac{1}{\\theta} \\text{Z}_t (\\theta), \\ t=1, \\cdots , n \\]\nThe \\(h\\) -step-ahead forecasts calculated at origin are given by\n\\[\\hat Y_{n+h|n} = (1-\\frac{1}{\\theta}) [\\text{A}_n+\\text{B}_n (n+h)]+ \\frac{1}{\\theta} \\tilde {\\text{Z}}_{n+h|n} (\\theta) \\tag 6\\]\nwhere \\(\\tilde {\\text{Z}}_{n+h|n} (\\theta)=\\tilde {\\text{Z}}_{n+1|n} (\\theta)=\\alpha \\sum_{i=0}^{n-1}(1-\\alpha)^i \\text{Z}_{n-i}(\\theta)+(1-\\alpha)^n \\ell_{0}^{*}\\) is the extrapolation of \\(\\text{Z}_t(\\theta)\\) by an SES model with \\(\\ell_{0}^{*} \\in \\mathbb{R}\\) as the initial level parameter and \\(\\alpha \\in (0,1)\\) as the smoothing parameter. Note that for \\(\\theta=2\\) Eq. (6) corresponds to Step 4 of the STheta algorithm. After some algebra, we can write\n\\[\\tilde {\\text{Z}}_{n+1|n} (\\theta)=\\theta \\ell{n}+(1-\\theta) \\{ \\text{A}_n [1-(1-\\alpha)^n] + \\text{B}_n [n+(1-\\frac{1}{\\alpha}) [1-(1-\\alpha)^n] ]  \\}   \\tag 7\\]\nwhere \\(\\ell_{t}=\\alpha Y_t +(1-\\alpha) \\ell_{t-1}\\) for \\(t=1, \\cdots, n\\) and \\(\\ell_{0}=\\ell_{0}^{*}/\\theta\\).\nIn the light of Eqs. (6), (7), we suggest four stochastic approaches. These approaches differ due to the parameter \\(\\theta\\) which may be either fixed at two or optimised, and the coefficients \\(\\text{A}_n\\) and \\(\\text{B}_n\\), which can be either fixed or dynamic functions. To formulate the state space models, it is helpful to adopt \\(\\mu_{t}\\) as the one-step-ahead forecast at origin \\(t-1\\) and \\(\\varepsilon_{t}\\) as the respective additive error, i.e., \\(\\varepsilon_{t}=Y_t - \\mu_{t}\\) if \\(\\mu_{t}= \\hat Y_{t|t-1}\\). We assume \\(\\{ \\varepsilon_{t} \\}\\) to be a Gaussian white noise process with mean zero and variance \\(\\sigma^2\\).\n\n\nOptimised and standard Theta models\nLet \\(\\text{A}_n\\) and \\(\\text{B}_n\\) be fixed coefficients for all \\(t=1, \\cdots, n\\) so that Eqs. (6), (7) configure the state space model given by\n\\[Y_t=\\mu_{t}+\\varepsilon_{t} \\tag 8\\]\n\\[\\mu_{t}=\\ell_{t-1}+(1-\\frac{1}{\\theta}) \\{(1-\\alpha)^{t-1} \\text{A}_n +[\\frac{1-(1-\\alpha)^t}{\\alpha} \\text{B}_n]  \\tag 9   \\}\\]\n\\[\\ell_{t}=\\alpha Y_t +(1-\\alpha)\\ell_{t-1} \\tag{10} \\]\nwith parameters \\(\\ell_{0} \\in \\mathbb{R}\\), \\(\\alpha \\in (0,1)\\) and \\(\\theta \\in [1,\\infty)\\) . The parameter \\(\\theta\\) is to be estimated along with \\(\\alpha\\) and \\(\\ell_{0}\\) We call this the optimised Theta model (OTM).\nThe \\(h\\)-step-ahead forecast at origin \\(n\\) is given by\n\\[\\hat Y_{n+h|n}=E[Y_{n+h}|Y_1,\\cdots, Y_n]=\\ell_{n}+(1-\\frac{1}{\\theta}) \\{(1-\\alpha)^n \\text{A}_n +[(h-1) + \\frac{1-(1-\\alpha)^{n+1}}{\\alpha}] \\text{B}_n \\}   \\]\nwhich is equivalent to Eq. (6). The conditional variance $[Y_{n+h}|Y_1, , Y_n]=[1+(h-1)^2]^2 $ can be computed easily from the state space model. Thus, the \\((1-\\alpha)\\%\\) prediction interval for \\(Y_{n+h}\\) is given by \\[\\hat Y_{n+h|n} \\ \\pm  \\ q_{1-\\alpha/2} \\sqrt{[1+(h-1)\\alpha^2 ]\\sigma^2 } \\]\nFor \\(\\theta=2\\) OTM reproduces the forecasts of the STheta method; hereafter, we will refer to this particular case as the standard Theta model (STM). In Theorem 2 of Appendix A, we show that OTM is mathematically equivalent to the SES-d model. As a corollary of Theorem 2, STM is mathematically equivalent to SES-d with \\(b=\\frac{1}{2} \\text{B}_n\\). Therefore, for \\(\\theta=2\\) the corollary also re-confirms the H&B result on the relationship between STheta and the SES-d model."
  },
  {
    "objectID": "docs/models/autotheta.html#loading-libraries-and-data",
    "href": "docs/models/autotheta.html#loading-libraries-and-data",
    "title": "AutoTheta Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport pandas as pd\n\nimport scipy.stats as stats\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/candy_production.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nobservation_date\nIPG3113N\n\n\n\n\n0\n1972-01-01\n85.6945\n\n\n1\n1972-02-01\n71.8200\n\n\n2\n1972-03-01\n66.0229\n\n\n3\n1972-04-01\n64.5645\n\n\n4\n1972-05-01\n65.0100\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1972-01-01\n85.6945\n1\n\n\n1\n1972-02-01\n71.8200\n1\n\n\n2\n1972-03-01\n66.0229\n1\n\n\n3\n1972-04-01\n64.5645\n1\n\n\n4\n1972-05-01\n65.0100\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds            object\ny            float64\nunique_id     object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/autotheta.html#explore-data-with-the-plot-method",
    "href": "docs/models/autotheta.html#explore-data-with-the-plot-method",
    "title": "AutoTheta Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints aa random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df, engine=\"matplotlib\")\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=60, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=60, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();"
  },
  {
    "objectID": "docs/models/autotheta.html#split-the-data-into-training-and-testing",
    "href": "docs/models/autotheta.html#split-the-data-into-training-and-testing",
    "title": "AutoTheta Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our AutoTheta model 2. Data to test our model\nFor the test data we will use the last 12 months to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2016-08-01'] \ntest = df[df.ds&gt;'2016-08-01']\n\n\ntrain.shape, test.shape\n\n((536, 3), (12, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linewidth=3, linestyle=\":\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.ylabel(\"Candy Production\")\nplt.xlabel(\"Month\")\nplt.show()"
  },
  {
    "objectID": "docs/models/autotheta.html#implementation-of-autotheta-with-statsforecast",
    "href": "docs/models/autotheta.html#implementation-of-autotheta-with-statsforecast",
    "title": "AutoTheta Model",
    "section": "Implementation of AutoTheta with StatsForecast ",
    "text": "Implementation of AutoTheta with StatsForecast \nTo also know more about the parameters of the functions of the AutoTheta Model, they are listed below. For more information, visit the documentation\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\ndecomposition_type : str\n    Sesonal decomposition type, 'multiplicative' (default) or 'additive'.\nmodel : str\n    Controlling Theta Model. By default searchs the best model.\nalias : str\n    Custom name of the model.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoTheta\n\n\n\nInstantiate Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful.season_length.\nAutomatically selects the best Theta (Standard Theta Model (‚ÄòSTM‚Äô), Optimized Theta Model (‚ÄòOTM‚Äô), Dynamic Standard Theta Model (‚ÄòDSTM‚Äô), Dynamic Optimized Theta Model (‚ÄòDOTM‚Äô)) model using mse.\n\nseason_length = 12 # Monthly data \nhorizon = len(test) # number of predictions\n\n# We call the model that we are going to use\nmodels = [AutoTheta(season_length=season_length,\n                     decomposition_type=\"additive\",\n                     model=\"STM\")]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='MS', \n                   n_jobs=-1)\n\n\n\nFit Model\n\nsf.fit()\n\nStatsForecast(models=[AutoTheta])\n\n\nLet‚Äôs see the results of our Theta model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mse': 100.64880011735451,\n 'amse': array([19.1126859 , 31.8559999 , 38.50771628]),\n 'fit': results(x=array([258.45065328,   0.7664297 ]), fn=100.57831804495909, nit=32, simplex=array([[250.37338839,   0.76970741],\n        [232.0391584 ,   0.76429422],\n        [258.45065328,   0.7664297 ]])),\n 'residuals': array([ 2.10810474e+00, -1.13894116e+01, -9.49139749e+00, -9.82041058e+00,\n        -1.13263190e+01, -9.25797072e+00, -8.56619271e+00, -8.99232281e+00,\n        -1.98586635e+00,  3.14569304e+01,  1.98520670e+01,  2.04962612e+01,\n         4.98121093e+00, -1.08735318e+01, -1.12328034e+01, -8.08115311e+00,\n        -9.98197694e+00, -8.39937079e+00, -1.25789510e+01, -1.05952806e+01,\n         8.47229675e-01,  2.25644626e+01,  2.54401518e+01,  1.73989709e+01,\n         2.40287622e+00, -2.53475414e+00, -8.00590851e+00, -1.79241485e+01,\n        -6.36590415e+00, -5.76986711e+00, -2.26766747e+01, -8.95260757e+00,\n        -7.19719238e+00,  2.74032221e+01,  2.21368465e+01,  6.43171918e+00,\n        -3.51755110e+00, -1.31441914e+01, -6.13166000e+00,  1.51512233e+00,\n        -8.05776753e+00, -8.59603097e+00, -1.08617848e+01, -6.72940024e+00,\n        -6.24861877e+00,  2.85996831e+01,  2.98048000e+01,  1.90032272e+01,\n         5.07597932e+00, -9.59170287e+00, -1.64521040e+01, -7.52212480e+00,\n        -5.16540017e+00, -1.27924605e+01, -9.68434388e+00, -8.76758699e+00,\n        -8.27471729e-01,  3.08424000e+01,  2.47947373e+01,  2.35867234e+01,\n         3.75665137e+00, -4.47305376e+00, -1.48000353e+01, -1.08431533e+01,\n        -1.01249948e+01, -1.12379772e+01, -1.28624668e+01, -9.47780366e+00,\n        -2.17961621e-01,  2.49398623e+01,  1.66027796e+01,  2.62581226e+01,\n        -1.94878910e+00, -8.10877623e+00, -6.93183588e+00, -6.80707230e+00,\n        -1.17809900e+01, -1.05320691e+01, -1.59715820e+01, -9.07599954e+00,\n         6.11989624e-01,  2.24925204e+01,  2.57389548e+01,  2.38907624e+01,\n         4.99776573e+00, -1.07054647e+01, -7.24194335e+00, -1.17412089e+01,\n        -1.10031569e+01, -9.10138933e+00, -1.62277217e+01, -1.02585273e+01,\n        -2.79431182e+00,  1.96746062e+01,  2.40620697e+01,  2.00041945e+01,\n         8.38672864e-01, -3.01704956e-01, -1.10576385e+01, -1.76502427e+01,\n        -4.79852872e+00, -7.74057037e+00, -1.55628715e+01, -6.19663669e+00,\n        -4.85268042e+00,  2.17819362e+01,  2.48075797e+01,  2.16186239e+01,\n         9.21215945e+00, -1.71191246e+00, -1.38314202e+01, -9.44161648e+00,\n        -6.35863859e+00, -1.10470634e+01, -1.41408696e+01, -9.60040139e+00,\n        -4.80959573e+00,  3.41173932e+01,  2.02685750e+01,  1.65177476e+01,\n         1.45004512e+00, -6.65007227e-01, -1.11027908e+01, -1.82545880e+01,\n        -1.08637859e+01, -9.67573744e+00, -1.22946690e+01, -1.02064789e+01,\n        -2.94225752e+00,  3.21840486e+01,  2.21586037e+01,  2.09073985e+01,\n        -2.49861035e-01, -6.05605693e+00, -1.16741817e+01, -1.31096440e+01,\n        -1.07043792e+01, -1.25489002e+01, -9.16715967e+00, -7.70278796e+00,\n        -2.55656615e+00,  2.69936390e+01,  1.62042825e+01,  1.67614454e+01,\n         8.62186772e+00, -3.51518872e+00, -9.27420876e+00, -1.15442800e+01,\n        -9.96136035e+00, -1.17898547e+01, -1.13147631e+01, -7.10440289e+00,\n        -1.10170687e+00,  2.60646470e+01,  2.32687962e+01,  1.82272075e+01,\n         3.98792429e+00, -7.64233456e+00, -1.07945924e+01, -1.16024033e+01,\n        -1.10645356e+01, -1.33282252e+01, -1.15534869e+01, -6.76285862e+00,\n         3.93787288e+00,  2.37018466e+01,  2.07922165e+01,  2.37645525e+01,\n         7.00181250e-01, -1.59605643e+00, -1.62277551e+01, -1.51068272e+01,\n        -1.01377617e+01, -1.13639555e+01, -1.38275875e+01, -5.87092406e+00,\n         3.43469843e+00,  2.82932152e+01,  2.39510183e+01,  1.71053567e+01,\n         6.00991052e-01, -7.61227344e-01, -1.18686652e+01, -1.51989720e+01,\n        -1.23352889e+01, -1.09931328e+01, -1.34086750e+01, -4.52127936e+00,\n         2.09363456e+00,  3.13825833e+01,  2.43980039e+01,  1.89899596e+01,\n        -7.55701831e+00, -2.76896118e-01, -6.52574213e+00, -1.67167273e+01,\n        -1.17498904e+01, -7.68050375e+00, -5.60844064e+00, -2.79087748e+00,\n        -2.92096472e-01,  2.31896512e+01,  1.70158839e+01,  1.84177146e+01,\n        -3.39879932e-01,  1.31241678e+00, -9.65552310e+00, -1.30840504e+01,\n        -1.33540007e+01, -9.72077532e+00, -1.09022915e+01, -4.49636293e+00,\n        -6.88545386e-01,  1.88878505e+01,  2.15227108e+01,  2.32009713e+01,\n        -5.72605410e+00,  1.87746304e+00, -6.95944417e+00, -1.41944243e+01,\n        -1.25398538e+01, -8.09461371e+00, -5.46316779e+00, -4.73324801e+00,\n         1.12162623e+00,  1.61183549e+01,  2.63470348e+01,  2.28827932e+01,\n        -6.75326627e+00,  4.34024335e+00, -6.61711421e+00, -1.64533692e+01,\n        -1.44473755e+01, -4.85575266e+00, -1.14659676e+01, -1.83411691e+00,\n        -3.17491990e+00,  1.22586080e+01,  2.19162170e+01,  1.62630875e+01,\n        -1.99943217e+00,  2.59143066e-03, -8.89995746e+00, -1.10976710e+01,\n        -1.43864442e+01, -9.48222086e+00, -1.06785679e+01, -7.24340992e+00,\n         2.15093059e+00,  1.53607646e+01,  2.06126872e+01,  1.96076193e+01,\n         3.03104513e+00, -8.52381714e-02, -8.52357291e+00, -1.33461539e+01,\n        -1.37600223e+01, -6.08840914e+00, -8.32367614e+00, -3.02117355e+00,\n         4.08613501e-01,  1.63346128e+01,  1.76259441e+01,  1.75724074e+01,\n         1.52688580e+00, -2.23616547e+00, -3.82137115e+00, -1.61943645e+01,\n        -1.55739800e+01, -6.10489281e+00, -6.56542552e+00, -3.79160289e+00,\n         1.79366877e+00,  1.37690217e+01,  1.71703998e+01,  2.12968995e+01,\n         2.55881699e+00, -5.89333546e+00, -5.43867789e+00, -9.34441587e+00,\n        -1.23296390e+01, -7.43701646e+00, -9.59827596e+00, -6.98198281e+00,\n        -7.94913086e-01,  1.30601029e+01,  2.03392159e+01,  2.52824428e+01,\n        -3.95418141e+00,  2.43162639e+00, -3.09610920e+00, -1.49779650e+01,\n        -1.07287620e+01, -8.40149865e+00, -1.18887461e+01, -1.74756743e+00,\n         2.17909199e+00,  1.20038424e+01,  2.42508105e+01,  2.34572744e+01,\n        -5.17568502e+00, -1.96581848e-01, -4.18458181e+00, -1.55119014e+01,\n        -1.38833806e+01, -8.29522400e+00, -1.30003229e+01, -1.67000232e-01,\n         9.35163928e-01,  1.47273988e+01,  2.29308492e+01,  2.17103709e+01,\n         3.68218624e+00,  2.64748987e-01, -7.34442764e+00, -1.25122409e+01,\n        -1.14503466e+01, -8.19533607e+00, -1.15456959e+01, -2.81694144e+00,\n        -1.50157905e+00,  1.14252476e+01,  2.08253666e+01,  1.93274968e+01,\n         7.94221350e-01, -5.10391223e-01, -8.74258215e+00, -9.01560883e+00,\n        -1.00192385e+01, -1.10908755e+01, -1.09129047e+01, -6.64424323e+00,\n        -1.50482360e+00,  1.46897903e+01,  1.73829646e+01,  2.23508538e+01,\n         8.64908630e+00,  6.22671094e-01, -6.68012804e+00, -5.70808134e+00,\n        -1.80391994e+01, -7.97570029e+00, -1.19962951e+01, -5.55858845e+00,\n         2.35415415e+00,  1.17526341e+01,  1.54009299e+01,  2.21564078e+01,\n         3.90927294e+00,  2.21699393e+00, -3.80724429e+00, -1.09345637e+01,\n        -1.37938445e+01, -1.00726067e+01, -1.19963695e+01, -5.40000947e+00,\n        -1.51911075e+00,  1.69895894e+00,  1.74367915e+01,  2.04883209e+01,\n         7.55305128e+00,  7.29570557e-01, -5.09536064e+00, -1.29493288e+01,\n        -1.53454354e+01, -2.46711146e+00, -1.01903530e+01, -4.03697109e+00,\n        -3.08084248e+00,  3.86928003e+00,  1.92764138e+01,  1.55958083e+01,\n         7.35560975e+00,  1.85905807e+00, -5.61642383e-01, -1.23394878e+01,\n        -9.90369395e+00, -7.50968535e+00, -1.83651427e+01, -2.77916462e+00,\n        -1.07805579e+00,  8.15877069e+00,  2.33477145e+01,  1.69720399e+01,\n         6.19355475e+00,  4.92033569e+00, -1.36452248e+01, -1.10382213e+01,\n        -4.45625718e+00, -1.37976279e+01, -1.12070256e+01, -1.28293835e+00,\n         1.02619080e-01,  1.16373460e+01,  1.73964054e+01,  1.64050950e+01,\n         1.32632343e+01,  4.44789636e+00, -1.66636731e+01, -1.04932464e+01,\n        -7.27536875e+00, -1.52095873e+01, -8.33331642e+00, -6.12562858e+00,\n        -6.19892773e-01,  1.73375832e+01,  1.71076149e+01,  2.30092407e+01,\n        -1.39793491e+00,  1.20108252e+00, -1.01506302e+01, -9.35708666e+00,\n        -1.72524976e+01, -1.33257495e+01, -1.11436034e+01, -1.07822300e+00,\n         2.29722831e+00,  1.15489407e+01,  1.72661569e+01,  2.11762694e+01,\n         9.51783992e+00, -1.02191544e+00, -5.14895468e+00, -2.05301462e+01,\n        -1.56429874e+01, -1.60412132e+01, -1.50915598e+01, -2.94815391e+00,\n         4.61947153e+00,  6.94204539e+00,  1.79378224e+01,  2.19333492e+01,\n         8.01926611e+00, -3.09873730e+00, -6.33384070e+00, -1.29667977e+01,\n        -1.54450147e+01, -1.27736740e+01, -1.46733540e+01, -8.76926667e+00,\n         8.56843032e+00,  1.28259081e+01,  1.86473179e+01,  5.73666716e+00,\n         4.33460410e+00,  2.08833633e+00, -3.96959197e+00, -1.29223836e+01,\n        -1.19550430e+01, -1.27279222e+01, -8.02537455e+00, -3.92330203e+00,\n         7.09140653e+00,  2.42153186e+01,  1.28924490e+01,  1.79712039e+01,\n         2.89523345e+00,  1.30474561e+00, -7.77941964e+00, -1.04361436e+01,\n        -1.14357282e+01, -1.23868253e+01, -3.73409886e+00,  6.47317969e-01,\n         5.14176714e+00,  1.16621419e+01,  8.00349658e+00,  1.83900836e+01,\n         3.46846826e+00,  2.29413411e+00, -4.06962429e+00, -8.55164816e+00,\n        -1.76399687e+01, -1.50423464e+01, -1.13765493e+01, -9.17973348e+00,\n        -4.22253840e+00,  2.19090146e+01,  1.90170613e+01,  1.80606320e+01,\n         4.08981908e+00,  2.02346575e+00, -5.45474178e+00, -1.38725735e+01,\n        -1.50622767e+01, -1.15367785e+01, -7.55445662e+00, -1.77510786e+00,\n         9.46336210e+00,  4.88813442e+00,  1.61490921e+01,  1.93212523e+01,\n         1.03075606e+01, -6.46757556e-01, -5.79533411e-01, -1.35917688e+01,\n        -1.62148895e+01, -1.29823914e+01, -1.02149059e+01, -3.24210991e+00,\n         3.05411548e-01,  1.19385124e+01,  2.08979522e+01,  2.19927488e+01,\n         1.32364111e+00,  1.68626682e+00, -3.52030260e+00, -1.50337393e+01,\n        -1.75865889e+01, -1.23980830e+01, -1.19670278e+01, -1.59575155e+00,\n         4.32015198e+00,  1.39461341e+01,  2.63901700e+01,  2.11431702e+01,\n         1.19960846e+00,  1.22769642e+00, -3.12850968e+00, -1.23388318e+01,\n        -1.66429415e+01, -9.08277175e+00, -7.92637109e+00,  2.43702710e+00,\n        -3.53211257e+00,  1.00606809e+01,  1.39608449e+01,  1.44689445e+01,\n         6.50770870e+00,  3.13941359e+00, -4.89894849e-01, -1.05833253e+01,\n        -1.34863092e+01, -1.20763816e+01, -1.00738931e+01, -9.39207509e+00]),\n 'm': 12,\n 'states': array([[7.33548965e+01, 8.30544205e+01, 8.40569077e+01, 6.14692122e-02,\n         8.35863953e+01],\n        [7.31818390e+01, 7.80917587e+01, 8.40569077e+01, 6.14692122e-02,\n         8.32094116e+01],\n        [7.38093872e+01, 7.67280502e+01, 8.40569077e+01, 6.14692122e-02,\n         7.55142975e+01],\n        ...,\n        [1.12984993e+02, 1.00517860e+02, 8.40569077e+01, 6.14692122e-02,\n         1.14480782e+02],\n        [1.14049675e+02, 1.00543762e+02, 8.40569077e+01, 6.14692122e-02,\n         1.13025093e+02],\n        [1.10946037e+02, 1.00561401e+02, 8.40569077e+01, 6.14692122e-02,\n         1.14089775e+02]], dtype=float32),\n 'par': {'initial_smoothed': 41.527209666340006,\n  'alpha': 0.7664297044277077,\n  'theta': 2.0},\n 'n': 536,\n 'modeltype': 'STM',\n 'mean_y': 100.56138830499272,\n 'decompose': True,\n 'decomposition_type': 'additive',\n 'seas_forecast': {'mean': array([  0.08977811,  18.09442   ,  20.248487  ,  19.430647  ,\n           2.6400807 ,  -1.3090991 ,  -7.977731  , -12.3264065 ,\n         -12.027774  , -10.136967  , -11.4229355 ,  -5.3025    ],\n        dtype=float32)},\n 'fitted': array([ 83.58639526,  83.20941162,  75.51429749,  74.38491058,\n         76.33631897,  76.90467072,  77.60909271,  79.82932281,\n         77.03206635,  75.4719696 ,  85.744133  ,  85.47103882,\n         86.31848907,  88.1435318 ,  80.84380341,  78.37975311,\n         81.66417694,  83.26287079,  84.62535095,  83.77008057,\n         79.74427032,  80.35553741,  83.81224823,  87.82202911,\n         86.29562378,  86.14455414,  85.23590851,  85.24504852,\n         80.98550415,  85.35566711,  88.73347473,  80.13900757,\n         77.37219238,  71.81797791,  78.98325348,  80.46128082,\n         70.5292511 ,  65.84059143,  56.80056   ,  58.24617767,\n         68.88546753,  71.95893097,  73.17068481,  73.63150024,\n         72.56861877,  67.74141693,  75.82369995,  83.17867279,\n         82.88182068,  84.77950287,  78.46220398,  71.9979248 ,\n         75.71080017,  81.00106049,  78.99654388,  80.35978699,\n         77.73477173,  77.0625    ,  86.86366272,  90.37877655,\n         93.59484863,  94.48135376,  92.08713531,  86.88905334,\n         88.05659485,  89.54567719,  88.73256683,  87.66000366,\n         84.49066162,  84.28553772,  89.56282043,  86.79937744,\n         92.0628891 ,  88.57657623,  83.39583588,  84.2281723 ,\n         88.48908997,  88.70896912,  88.43688202,  84.98139954,\n         82.12001038,  82.55097961,  85.95254517,  90.19133759,\n         93.64043427,  95.47816467,  88.30724335,  88.90190887,\n         89.38115692,  90.19718933,  91.0216217 ,  87.36982727,\n         83.60211182,  81.4223938 ,  82.66423035,  85.61780548,\n         86.08812714,  84.73820496,  85.54103851,  83.21124268,\n         79.16162872,  84.73307037,  86.6004715 ,  83.45823669,\n         82.80368042,  79.04636383,  81.90332031,  85.42827606,\n         87.13594055,  92.20371246,  91.92572021,  87.47001648,\n         89.71173859,  94.08746338,  93.42066956,  91.36830139,\n         88.10499573,  84.38070679,  96.69192505,  96.73805237,\n         94.53625488,  93.65490723,  94.17929077,  91.814888  ,\n         87.30208588,  88.22493744,  88.60916901,  87.97177887,\n         84.24395752,  81.95085144,  92.78029633,  94.27500153,\n         95.43756104,  93.25335693,  89.64588165,  86.84354401,\n         86.27397919,  87.31900024,  85.50115967,  87.26078796,\n         85.45186615,  83.45436096,  90.30571747,  87.23685455,\n         85.22183228,  89.83718872,  88.17710876,  87.21417999,\n         87.84436035,  89.45885468,  88.22276306,  88.33640289,\n         86.98610687,  86.10365295,  92.24300385,  94.58859253,\n         93.69697571,  94.76073456,  89.93749237,  87.80930328,\n         88.39493561,  89.16392517,  86.74878693,  86.67945862,\n         85.59092712,  88.57095337,  92.89938354,  93.34684753,\n         96.69921875,  95.24315643,  95.05395508,  88.7616272 ,\n         86.66136169,  88.14065552,  87.23098755,  85.41872406,\n         85.01380157,  87.60818481,  95.45558167,  98.32404327,\n         96.57260895,  95.04052734,  95.49116516,  92.53977203,\n         90.36888885,  90.1639328 ,  89.53847504,  88.04727936,\n         88.67676544,  90.24331665, 100.45849609, 103.66954041,\n        103.36251831,  95.57789612,  96.39974213,  97.54332733,\n         94.20919037,  94.45290375,  96.36634064, 100.85347748,\n        102.80919647, 102.54724884, 106.48311615, 104.0362854 ,\n        103.29067993, 101.03748322, 103.0774231 , 101.82225037,\n        101.27230072, 100.28657532, 100.6362915 , 101.06606293,\n        101.71464539, 101.14884949, 101.78768921, 102.79502869,\n        105.7154541 ,  99.33413696, 101.80714417, 102.61832428,\n        101.21735382, 100.85561371, 102.45166779, 107.05014801,\n        107.51717377, 108.33874512, 106.85496521, 111.55980682,\n        114.23636627, 107.06775665, 111.42831421, 112.5018692 ,\n        109.3695755 , 107.54585266, 111.62426758, 111.62201691,\n        114.3110199 , 111.83959198, 107.39758301, 108.70651245,\n        106.30953217, 102.78440857, 103.82045746, 103.14437103,\n        104.11684418, 102.33982086, 102.87236786, 103.47360992,\n        102.01676941, 103.62723541, 101.56281281, 101.87268066,\n        102.03905487, 102.36943817, 103.33817291, 102.95055389,\n        102.19972229, 100.90280914, 104.03647614, 106.44257355,\n        108.2217865 , 108.49688721, 107.1788559 , 105.19959259,\n        103.8061142 , 102.98366547, 102.30387115, 105.52016449,\n        102.58638   ,  99.89919281, 103.02022552, 106.77390289,\n        107.96263123, 109.29927826, 106.01490021, 103.68650055,\n        105.14758301, 105.11603546, 101.63327789, 103.61001587,\n        105.92623901, 105.72561646, 107.82567596, 109.25488281,\n        107.99841309, 107.35109711, 103.52338409, 103.62365723,\n        108.13938141, 103.11607361, 106.0138092 , 109.78596497,\n        107.78446198, 108.81079865, 110.17164612, 109.84536743,\n        112.60070801, 114.23275757, 109.5954895 , 112.69372559,\n        115.81058502, 109.85108185, 110.73448181, 113.67240143,\n        111.2616806 , 109.870224  , 111.31252289, 110.13430023,\n        114.10103607, 114.77970123, 112.22985077, 114.31642914,\n        116.09441376, 116.92385101, 118.16082764, 118.67694092,\n        118.56524658, 119.03853607, 120.55739594, 120.49404144,\n        122.42977905, 121.24085236, 116.16013336, 116.63300323,\n        116.58467865, 115.20069122, 115.84358215, 115.28810883,\n        117.8563385 , 119.42647552, 118.72610474, 119.14774323,\n        118.1501236 , 116.95870972, 114.3800354 , 112.2145462 ,\n        114.4834137 , 119.11962891, 120.63092804, 121.65618134,\n        126.75939941, 122.18280029, 123.86999512, 123.46128845,\n        123.29574585, 125.06196594, 120.2321701 , 116.54759216,\n        118.66742706, 119.67090607, 122.40414429, 125.63126373,\n        126.72874451, 125.40590668, 125.48596954, 125.07720947,\n        125.03321075, 123.83084106, 111.29560852, 109.17137909,\n        110.01274872, 113.80892944, 115.40216064, 117.64202881,\n        117.19533539, 114.68331146, 120.592453  , 121.56787109,\n        122.56854248, 120.16921997, 109.29738617, 108.58309174,\n        105.67469025, 109.31954193, 111.77844238, 117.49308777,\n        117.51379395, 119.17248535, 121.21684265, 115.92686462,\n        117.89155579, 117.02722931, 109.44298553, 111.84906006,\n        109.99544525, 112.74966431, 117.55482483, 113.24182129,\n        114.25985718, 120.09362793, 117.31872559, 117.51493835,\n        120.62638092, 120.66695404, 115.74879456, 113.59360504,\n        111.3054657 , 119.47810364, 123.9211731 , 117.2947464 ,\n        118.73046875, 122.40358734, 118.54651642, 120.94522858,\n        120.34509277, 119.83191681, 119.28258514, 116.90605927,\n        119.67953491, 116.61541748, 118.57003021, 116.93538666,\n        119.24189758, 115.26824951, 112.85500336, 113.099823  ,\n        116.36817169, 118.09075928, 113.10484314, 110.84983063,\n        112.21846008, 117.52051544, 117.77135468, 119.97014618,\n        113.71328735, 110.9732132 , 106.47875977, 103.69775391,\n        105.53292847, 109.03535461, 100.51857758,  98.77835083,\n        100.72723389, 104.8807373 , 103.5398407 , 104.83049774,\n        104.37041473, 101.78207397,  99.79195404,  97.33146667,\n         94.70516968, 101.23419189,  97.22698212,  96.03053284,\n         85.5657959 ,  86.89526367,  89.52989197,  92.63258362,\n         92.20654297,  92.29302216,  90.33797455,  92.97270203,\n         94.06049347,  99.45748138, 104.17945099,  98.57229614,\n         97.48446655,  97.71075439,  99.74481964,  99.92754364,\n        101.4070282 , 101.89152527, 100.19789886, 106.12158203,\n        110.71243286, 114.61515808, 109.71600342, 100.36181641,\n         99.59503174, 100.26066589, 103.05302429, 106.07904816,\n        109.00286865, 104.7322464 , 101.0033493 , 101.06963348,\n         98.1287384 ,  94.85438538,  97.80873871,  96.89566803,\n         95.87638092,  97.01823425,  99.60314178, 101.56757355,\n        100.41327667,  98.1182785 ,  97.07615662, 100.07180786,\n        102.8060379 , 110.02096558,  99.93000793,  96.81884766,\n         96.76573944, 102.67305756, 103.21143341, 108.91236877,\n        107.97328949, 104.79489136, 102.64480591, 103.60140991,\n        105.21128845, 105.4072876 , 100.71994781, 101.24845123,\n        103.24285889, 102.26463318, 104.5911026 , 108.03813934,\n        105.99388885, 101.76418304, 100.0619278 ,  99.67565155,\n        102.54734802, 105.82036591, 102.67173004, 107.40962982,\n        108.75289154, 107.67960358, 109.65460968, 113.40193176,\n        113.42314148, 109.91667175, 110.75537109, 113.4659729 ,\n        119.42851257, 116.68331909, 110.55675507, 105.76845551,\n        101.9963913 , 104.99138641, 108.43159485, 114.20122528,\n        115.56790924, 114.48078156, 113.02509308, 114.08977509])}\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\n2.108105\n\n\n1\n-11.389412\n\n\n2\n-9.491397\n\n\n...\n...\n\n\n533\n-12.076382\n\n\n534\n-10.073893\n\n\n535\n-9.392075\n\n\n\n\n536 rows √ó 1 columns\n\n\n\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n# Prediction\nY_hat = sf.forecast(horizon, fitted=True)\n\nY_hat\n\n\n\n\n\n\n\n\nds\nAutoTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n2016-09-01\n111.075912\n\n\n1\n2016-10-01\n129.111282\n\n\n1\n2016-11-01\n131.296082\n\n\n...\n...\n...\n\n\n1\n2017-06-01\n101.125748\n\n\n1\n2017-07-01\n99.870514\n\n\n1\n2017-08-01\n106.021683\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nAutoTheta\n\n\nunique_id\n\n\n\n\n\n\n\n1\n1972-01-01\n85.694504\n83.586395\n\n\n1\n1972-02-01\n71.820000\n83.209412\n\n\n1\n1972-03-01\n66.022903\n75.514297\n\n\n1\n1972-04-01\n64.564499\n74.384911\n\n\n1\n1972-05-01\n65.010002\n76.336319\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nAutoTheta\nAutoTheta-lo-95\nAutoTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2016-09-01\n111.075912\n90.148819\n135.999680\n\n\n1\n2016-10-01\n129.111282\n94.811134\n160.372803\n\n\n1\n2016-11-01\n131.296082\n90.598457\n168.251602\n\n\n...\n...\n...\n...\n...\n\n\n1\n2017-06-01\n101.125748\n41.213715\n159.133316\n\n\n1\n2017-07-01\n99.870514\n35.173969\n152.843002\n\n\n1\n2017-08-01\n106.021683\n38.784256\n166.021072\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoTheta\n\n\n\n\n0\n1\n2016-09-01\n111.075912\n\n\n1\n1\n2016-10-01\n129.111282\n\n\n2\n1\n2016-11-01\n131.296082\n\n\n...\n...\n...\n...\n\n\n9\n1\n2017-06-01\n101.125748\n\n\n10\n1\n2017-07-01\n99.870514\n\n\n11\n1\n2017-08-01\n106.021683\n\n\n\n\n12 rows √ó 3 columns\n\n\n\n\n# Merge the forecasts with the true values\ntest['unique_id'] = test['unique_id'].astype(int)\nY_hat1 = test.merge(Y_hat, how='left', on=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nAutoTheta\n\n\n\n\n0\n2016-09-01\n109.3191\n1\n111.075912\n\n\n1\n2016-10-01\n119.0502\n1\n129.111282\n\n\n2\n2016-11-01\n116.8431\n1\n131.296082\n\n\n...\n...\n...\n...\n...\n\n\n9\n2017-06-01\n104.2022\n1\n101.125748\n\n\n10\n2017-07-01\n102.5861\n1\n99.870514\n\n\n11\n2017-08-01\n114.0613\n1\n106.021683\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds')\nplot_df[['y', \"AutoTheta\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Year ', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nAutoTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n2016-09-01\n111.075912\n\n\n1\n2016-10-01\n129.111282\n\n\n1\n2016-11-01\n131.296082\n\n\n...\n...\n...\n\n\n1\n2017-06-01\n101.125748\n\n\n1\n2017-07-01\n99.870514\n\n\n1\n2017-08-01\n106.021683\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[95]) \n\nforecast_df\n\n\n\n\n\n\n\n\nds\nAutoTheta\nAutoTheta-lo-95\nAutoTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2016-09-01\n111.075912\n90.148819\n135.999680\n\n\n1\n2016-10-01\n129.111282\n94.811134\n160.372803\n\n\n1\n2016-11-01\n131.296082\n90.598457\n168.251602\n\n\n...\n...\n...\n...\n...\n\n\n1\n2017-06-01\n101.125748\n41.213715\n159.133316\n\n\n1\n2017-07-01\n99.870514\n35.173969\n152.843002\n\n\n1\n2017-08-01\n106.021683\n38.784256\n166.021072\n\n\n\n\n12 rows √ó 4 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nAutoTheta\nAutoTheta-lo-95\nAutoTheta-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n1972-01-01\n85.6945\n1\nNaN\nNaN\nNaN\n\n\n1972-02-01\n71.8200\n1\nNaN\nNaN\nNaN\n\n\n1972-03-01\n66.0229\n1\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n2017-06-01\nNaN\nNaN\n101.125748\n41.213715\n159.133316\n\n\n2017-07-01\nNaN\nNaN\n99.870514\n35.173969\n152.843002\n\n\n2017-08-01\nNaN\nNaN\n106.021683\n38.784256\n166.021072\n\n\n\n\n560 rows √ó 5 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2)\n    colors = ['orange', 'black', 'green']\n    for model, color in zip(models, colors):\n        ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-95'], \n                        df_plot[f'{model}-hi-95'],\n                        alpha=.35,\n                        color=color,\n                        label=f'{model}-level-95')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel('', fontsize=20)\n    ax.set_xlabel('Timestamp [t]', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid(True)\n\n\nplot_forecasts(train, test, forecast_df, models=['AutoTheta'])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df, level=[95])"
  },
  {
    "objectID": "docs/models/autotheta.html#cross-validation",
    "href": "docs/models/autotheta.html#cross-validation",
    "title": "AutoTheta Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=12,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoTheta\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2011-09-01\n2011-08-01\n93.906197\n98.167465\n\n\n1\n2011-10-01\n2011-08-01\n116.763397\n116.969933\n\n\n1\n2011-11-01\n2011-08-01\n116.825798\n119.135147\n\n\n...\n...\n...\n...\n...\n\n\n1\n2016-06-01\n2015-08-01\n102.404404\n109.600456\n\n\n1\n2016-07-01\n2015-08-01\n102.951202\n108.260147\n\n\n1\n2016-08-01\n2015-08-01\n104.697701\n114.248260\n\n\n\n\n60 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/autotheta.html#model-evaluation",
    "href": "docs/models/autotheta.html#model-evaluation",
    "title": "AutoTheta Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, AutoTheta.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"AutoTheta\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  6.9269824\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, model):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=12)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, test, Y_hat, model=\"AutoTheta\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nAutoTheta\n6.281525\n5.568355\n1.212475\n7.683672\n5.479727"
  },
  {
    "objectID": "docs/models/autotheta.html#acknowledgements",
    "href": "docs/models/autotheta.html#acknowledgements",
    "title": "AutoTheta Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/autotheta.html#references",
    "href": "docs/models/autotheta.html#references",
    "title": "AutoTheta Model",
    "section": "References ",
    "text": "References \n\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/optimizedtheta.html",
    "href": "docs/models/optimizedtheta.html",
    "title": "Optimized Theta Model",
    "section": "",
    "text": "Introduction\nOptimized Theta Model (OTM)\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of OptimizedTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/optimizedtheta.html#table-of-contents",
    "href": "docs/models/optimizedtheta.html#table-of-contents",
    "title": "Optimized Theta Model",
    "section": "",
    "text": "Introduction\nOptimized Theta Model (OTM)\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of OptimizedTheta with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/optimizedtheta.html#introduction",
    "href": "docs/models/optimizedtheta.html#introduction",
    "title": "Optimized Theta Model",
    "section": "Introduction ",
    "text": "Introduction \nThe optimized Theta model is a time series forecasting method that is based on the decomposition of the time series into three components: trend, seasonality and noise. The model then forecasts the long-term trend and seasonality, and uses the noise to adjust the short-term forecasts. The optimized Theta model has been shown to be more accurate than other time series forecasting methods, especially for time series with complex trends and seasonality.\nThe optimized Theta model was developed by Athanasios N. Antoniadis and Nikolaos D. Tsonis in 2013. The model is based on the Theta forecasting method, which was developed by George E. P. Box and Gwilym M. Jenkins in 1976. Theta method is a time series forecasting method that is based on the decomposition of the time series into three components: trend, seasonality, and noise. The Theta model then forecasts the long-term trend and seasonality, and uses the noise to adjust the short-term forecasts.\nThe Theta Optimized model improves on the Theta method by using an optimization algorithm to find the best parameters for the model. The optimization algorithm is based on the Akaike loss function (AIC), which is a measure of the goodness of fit of a model to the data. The optimization algorithm looks for the parameters that minimize the AIC function.\nThe optimized Theta model has been shown to be more accurate than other time series forecasting methods, especially for time series with complex trends and seasonality. The model has been used to forecast a variety of time series, including sales, production, prices, and weather.\nBelow are some of the benefits of the optimized Theta model:\n\nIt is more accurate than other time series forecasting methods.\nIt‚Äôs easy to use.\nCan be used to forecast a variety of time series.\nIt is flexible and can be adapted to different scenarios.\n\nIf you are looking for an easy-to-use and accurate time series forecasting method, the Optimized Theta model is a good choice.\nThe optimized Theta model can be applied in a variety of areas, including:\n\nSales: The optimized Theta model can be used to forecast sales of products or services. This can help companies make decisions about production, inventory, and marketing.\nProduction: The optimized Theta model can be used to forecast the production of goods or services. This can help companies ensure they have the capacity to meet demand and avoid overproduction.\nPrices: The optimized Theta model can be used to forecast the prices of goods or services. This can help companies make decisions about pricing and marketing strategy.\nWeather: The optimized Theta model can be used to forecast the weather. This can help companies make decisions about agricultural production, travel planning and risk management.\nOther: The optimized Theta model can also be used to forecast other types of time series, including traffic, energy demand, and population.\n\nThe Optimized Theta model is a powerful tool that can be used to improve the accuracy of time series forecasts. It is easy to use and can be applied to a variety of areas. If you are looking for a tool to improve your time series forecasts, the Optimized Theta model is a good choice."
  },
  {
    "objectID": "docs/models/optimizedtheta.html#optimized-theta-model-otm",
    "href": "docs/models/optimizedtheta.html#optimized-theta-model-otm",
    "title": "Optimized Theta Model",
    "section": "Optimized Theta Model (OTM) ",
    "text": "Optimized Theta Model (OTM) \nAssume that either the time series \\(Y_1, \\cdots Y_n\\) is non-seasonal or it has been seasonally adjusted using the multiplicative classical decomposition approach.\nLet \\(X_t\\) be the linear combination of two theta lines, \\[X_t=\\omega \\text{Z}_t (\\theta_1) +(1-\\omega) \\text{Z}_t (\\theta_2) \\tag 1\\]\nwhere \\(\\omega \\in [0,1]\\) is the weight parameter. Assuming that \\(\\theta_1 &lt;1\\) and \\(\\theta_2 \\geq 1\\), the weight \\(\\omega\\) can be derived as \\[\\omega:=\\omega(\\theta_1, \\theta_2)=\\frac{\\theta_2 -1}{\\theta_2 -\\theta_1} \\tag 2\\]\nIt is straightforward to see from Eqs. (1), (2) that \\(X_t=Y_t, \\ t=1, \\cdots n\\) i.e., the weights are calculated properly in such a way that Eq. (1) reproduces the original series.\nTheorem 1: Let \\(\\theta_1 &lt;1\\) and \\(\\theta_2 \\geq 1\\). We will prove that\n\nthe linear system given by \\(X_t=Y_t\\) for all \\(t=1, \\cdots, n\\), where \\(X_t\\) is given by Eq.(4), has the single solution\n\n\\[\\omega= (\\theta_2 -1)/(\\theta_2 - \\theta_1)\\]\n\nthe error of choosing a non-optimal weight \\(\\omega_{\\delta} =\\omega + \\delta\\) is proportional to the error for a simple linear regression model.\n\nIn Theorem 1 , we prove that the solution is unique and that the error from not choosing the optimal weights (\\(\\omega\\) and \\(1-\\omega\\)) s proportional to the error of a linear regression model. As a consequence, the STheta method is given simply by setting \\(\\theta_1=0\\) and \\(\\theta_2=2\\) while from Eq. (2) we get \\(\\omega=0.5\\). Thus, Eqs. (1), (2) allow us to construct a generalisation of the Theta model that maintains the re-composition propriety of the original time series for any theta lines \\(\\text{Z}_t (\\theta_1)\\) and \\(\\text{Z}_t (\\theta_2)\\).\nIn order to maintain the modelling of the long-term component and retain a fair comparison with the STheta method, in this work we fix \\(\\theta_1=0\\) and focus on the optimisation of the short-term component, \\(\\theta_2=0\\) with \\(\\theta \\geq 1\\). Thus, \\(\\theta\\) is the only parameter that requires estimation so far. The theta decomposition is now given by\n\\[Y_t=(1-\\frac{1}{\\theta}) (\\text{A}_n+\\text{B}_n t)+ \\frac{1}{\\theta} \\text{Z}_t (\\theta), \\ t=1, \\cdots , n \\]\nThe \\(h\\) -step-ahead forecasts calculated at origin are given by\n\\[\\hat Y_{n+h|n} = (1-\\frac{1}{\\theta}) [\\text{A}_n+\\text{B}_n (n+h)]+ \\frac{1}{\\theta} \\tilde {\\text{Z}}_{n+h|n} (\\theta) \\tag 3\\]\nwhere \\(\\tilde {\\text{Z}}_{n+h|n} (\\theta)=\\tilde {\\text{Z}}_{n+1|n} (\\theta)=\\alpha \\sum_{i=0}^{n-1}(1-\\alpha)^i \\text{Z}_{n-i}(\\theta)+(1-\\alpha)^n \\ell_{0}^{*}\\) is the extrapolation of \\(\\text{Z}_t(\\theta)\\) by an SES model with \\(\\ell_{0}^{*} \\in \\mathbb{R}\\) as the initial level parameter and \\(\\alpha \\in (0,1)\\) as the smoothing parameter. Note that for \\(\\theta=2\\) Eq. (3) corresponds to Step 4 of the STheta algorithm. After some algebra, we can write\n\\[\\tilde {\\text{Z}}_{n+1|n} (\\theta)=\\theta \\ell{n}+(1-\\theta) \\{ \\text{A}_n [1-(1-\\alpha)^n] + \\text{B}_n [n+(1-\\frac{1}{\\alpha}) [1-(1-\\alpha)^n] ]  \\}   \\tag 4\\]\nwhere ${t}=Y_t +(1-) {t-1} $ for \\(t=1, \\cdots, n\\) and \\(\\ell_{0}=\\ell_{0}^{*}/\\theta\\).\nIn the light of Eqs. (3), (4), we suggest four stochastic approaches. These approaches differ due to the parameter \\(\\theta\\) which may be either fixed at two or optimised, and the coefficients \\(\\text{A}_n\\) and \\(\\text{B}_n\\), which can be either fixed or dynamic functions. To formulate the state space models, it is helpful to adopt \\(\\mu_{t}\\) as the one-step-ahead forecast at origin \\(t-1\\) and \\(\\varepsilon_{t}\\) as the respective additive error, i.e., \\(\\varepsilon_{t}=Y_t - \\mu_{t}\\) if \\(\\mu_{t}= \\hat Y_{t|t-1}\\). We assume \\(\\{ \\varepsilon_{t} \\}\\) to be a Gaussian white noise process with mean zero and variance \\(\\sigma^2\\).\n\nMore on Optimised Theta models\nLet \\(\\text{A}_n\\) and \\(\\text{B}_n\\) be fixed coefficients for all \\(t=1, \\cdots, n\\) so that Eqs. (3), (4) configure the state space model given by\n\\[Y_t=\\mu_{t}+\\varepsilon_{t} \\tag 5\\]\n\\[\\mu_{t}=\\ell_{t-1}+(1-\\frac{1}{\\theta}) \\{(1-\\alpha)^{t-1} \\text{A}_n +[\\frac{1-(1-\\alpha)^t}{\\alpha} \\text{B}_n]  \\tag 6   \\}\\]\n\\[\\ell_{t}=\\alpha Y_t +(1-\\alpha)\\ell_{t-1} \\tag 7 \\]\nwith parameters \\(\\ell_{0} \\in \\mathbb{R}\\), \\(\\alpha \\in (0,1)\\) and \\(\\theta \\in [1,\\infty)\\) . The parameter \\(\\theta\\) is to be estimated along with \\(\\alpha\\) and \\(\\ell_{0}\\) We call this the optimised Theta model (OTM).\nThe \\(h\\)-step-ahead forecast at origin \\(n\\) is given by\n\\[\\hat Y_{n+h|n}=E[Y_{n+h}|Y_1,\\cdots, Y_n]=\\ell_{n}+(1-\\frac{1}{\\theta}) \\{(1-\\alpha)^n \\text{A}_n +[(h-1) + \\frac{1-(1-\\alpha)^{n+1}}{\\alpha}] \\text{B}_n \\}   \\]\nwhich is equivalent to Eq. (3). The conditional variance $[Y_{n+h}|Y_1, , Y_n]=[1+(h-1)^2]^2 $ can be computed easily from the state space model. Thus, the \\((1-\\alpha)\\%\\) prediction interval for \\(Y_{n+h}\\) is given by \\[\\hat Y_{n+h|n} \\ \\pm  \\ q_{1-\\alpha/2} \\sqrt{[1+(h-1)\\alpha^2 ]\\sigma^2 } \\]\nFor \\(\\theta=2\\) OTM reproduces the forecasts of the STheta method; hereafter, we will refer to this particular case as the standard Theta model (STM).\nTheorem 2: The SES-d \\((\\ell_{0}^{**}, \\alpha, b)\\) model, where \\(\\ell_{0}^{**} \\in \\mathbb{R}, \\alpha \\in (0,1)\\) and \\(b \\in \\mathbb{R}\\) is equivalent to \\(\\text{OTM} (\\ell_{0}, \\alpha, \\theta )\\) where \\(\\ell_{0} \\in \\mathbb{R}\\) and \\(\\theta \\geq 1\\), if\n\\[\\ell_{0}^{**} = \\ell_{0} + (1- \\frac{1}{\\theta} )A_n \\ \\ and \\ \\ b=(1-\\frac{1}{\\theta} )B_n \\]\nIn Theorem 2, we show that OTM is mathematically equivalent to the SES-d model. As a corollary of Theorem 2, STM is mathematically equivalent to SES-d with \\(b=\\frac{1}{2} \\text{B}_n\\). Therefore, for \\(\\theta=2\\) the corollary also re-confirms the H&B result on the relationship between STheta and the SES-d model."
  },
  {
    "objectID": "docs/models/optimizedtheta.html#loading-libraries-and-data",
    "href": "docs/models/optimizedtheta.html#loading-libraries-and-data",
    "title": "Optimized Theta Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\nimport pandas as pd\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/milk_production.csv\", usecols=[1,2])\ndf.head()\n\n\n\n\n\n\n\n\nmonth\nproduction\n\n\n\n\n0\n1962-01-01\n589\n\n\n1\n1962-02-01\n561\n\n\n2\n1962-03-01\n640\n\n\n3\n1962-04-01\n656\n\n\n4\n1962-05-01\n727\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n1962-01-01\n589\n1\n\n\n1\n1962-02-01\n561\n1\n\n\n2\n1962-03-01\n640\n1\n\n\n3\n1962-04-01\n656\n1\n\n\n4\n1962-05-01\n727\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/optimizedtheta.html#explore-data-with-the-plot-method",
    "href": "docs/models/optimizedtheta.html#explore-data-with-the-plot-method",
    "title": "Optimized Theta Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df, engine=\"matplotlib\")\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n###Additive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\n\nAdditive\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=12)\na.plot();\n\n\n\n\n\n\nMultiplicative\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=12)\na.plot();"
  },
  {
    "objectID": "docs/models/optimizedtheta.html#split-the-data-into-training-and-testing",
    "href": "docs/models/optimizedtheta.html#split-the-data-into-training-and-testing",
    "title": "Optimized Theta Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our Optimized Theta model. 2. Data to test our model\nFor the test data we will use the last 12 months to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='1974-12-01'] \ntest = df[df.ds&gt;'1974-12-01']\n\n\ntrain.shape, test.shape\n\n((156, 3), (12, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"\")\nplt.ylabel(\"Monthly Milk Production\")\nplt.xlabel(\"Monthly\")\nplt.show()"
  },
  {
    "objectID": "docs/models/optimizedtheta.html#implementation-of-optimizedtheta-with-statsforecast",
    "href": "docs/models/optimizedtheta.html#implementation-of-optimizedtheta-with-statsforecast",
    "title": "Optimized Theta Model",
    "section": "Implementation of OptimizedTheta with StatsForecast ",
    "text": "Implementation of OptimizedTheta with StatsForecast \nTo also know more about the parameters of the functions of the OptimizedTheta Model, they are listed below. For more information, visit the documentation.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\ndecomposition_type : str\n    Sesonal decomposition type, 'multiplicative' (default) or 'additive'.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import OptimizedTheta\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 12 # Monthly data \nhorizon = len(test) # number of predictions\n\nmodels = [OptimizedTheta(season_length=season_length, \n                decomposition_type=\"additive\")] # multiplicative   additive\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=train,\n                   models=models,\n                   freq='MS', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[OptimizedTheta])\n\n\nLet‚Äôs see the results of our Optimized Theta Model (OTM). We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nprint(result.keys())\nprint(result['fit'])\n\ndict_keys(['mse', 'amse', 'fit', 'residuals', 'm', 'states', 'par', 'n', 'modeltype', 'mean_y', 'decompose', 'decomposition_type', 'seas_forecast', 'fitted'])\nresults(x=array([-83.14191626,   0.73681394,  12.45013763]), fn=10.448217519858634, nit=47, simplex=array([[-58.73988124,   0.7441127 ,  11.69842922],\n       [-49.97233449,   0.73580297,  11.41787513],\n       [-83.14191626,   0.73681394,  12.45013763],\n       [-77.04867427,   0.73498431,  11.99254037]]))\n\n\nLet us now visualize the residuals of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nresidual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\nresidual\n\n\n\n\n\n\n\n\nresidual Model\n\n\n\n\n0\n-271.899414\n\n\n1\n-114.671692\n\n\n2\n4.768066\n\n\n...\n...\n\n\n153\n-60.233887\n\n\n154\n-92.472839\n\n\n155\n-44.143982\n\n\n\n\n156 rows √ó 1 columns\n\n\n\n\nimport scipy.stats as stats\n\nfig, axs = plt.subplots(nrows=2, ncols=2)\n\nresidual.plot(ax=axs[0,0])\naxs[0,0].set_title(\"Residuals\");\n\nsns.distplot(residual, ax=axs[0,1]);\naxs[0,1].set_title(\"Density plot - Residual\");\n\nstats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\naxs[1,0].set_title('Plot Q-Q')\n\nplot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\naxs[1,1].set_title(\"Autocorrelation\");\n\nplt.show();\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\nY_hat = sf.forecast(horizon, fitted=True)\nY_hat\n\n\n\n\n\n\n\n\nds\nOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n1975-01-01\n839.682800\n\n\n1\n1975-02-01\n802.071838\n\n\n1\n1975-03-01\n896.117126\n\n\n...\n...\n...\n\n\n1\n1975-10-01\n824.135498\n\n\n1\n1975-11-01\n795.691162\n\n\n1\n1975-12-01\n833.316345\n\n\n\n\n12 rows √ó 2 columns\n\n\n\nLet‚Äôs visualize the fitted values\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n\n1\n1962-01-01\n589.0\n860.899414\n\n\n1\n1962-02-01\n561.0\n675.671692\n\n\n1\n1962-03-01\n640.0\n635.231934\n\n\n1\n1962-04-01\n656.0\n614.731323\n\n\n1\n1962-05-01\n727.0\n609.770752\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\nAdding 95% confidence interval with the forecast method\n\nsf.forecast(h=horizon, level=[95])\n\n\n\n\n\n\n\n\nds\nOptimizedTheta\nOptimizedTheta-lo-95\nOptimizedTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n1975-01-01\n839.682800\n742.509583\n955.414307\n\n\n1\n1975-02-01\n802.071838\n643.581360\n945.119263\n\n\n1\n1975-03-01\n896.117126\n710.785217\n1065.057495\n\n\n...\n...\n...\n...\n...\n\n\n1\n1975-10-01\n824.135498\n555.948975\n1084.320312\n\n\n1\n1975-11-01\n795.691162\n503.148010\n1036.519531\n\n\n1\n1975-12-01\n833.316345\n530.259949\n1106.636963\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nOptimizedTheta\n\n\n\n\n0\n1\n1975-01-01\n839.682800\n\n\n1\n1\n1975-02-01\n802.071838\n\n\n2\n1\n1975-03-01\n896.117126\n\n\n...\n...\n...\n...\n\n\n9\n1\n1975-10-01\n824.135498\n\n\n10\n1\n1975-11-01\n795.691162\n\n\n11\n1\n1975-12-01\n833.316345\n\n\n\n\n12 rows √ó 3 columns\n\n\n\n\n# Merge the forecasts with the true values\ntest['unique_id'] = test['unique_id'].astype(int)\nY_hat1 = test.merge(Y_hat, how='left', on=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nOptimizedTheta\n\n\n\n\n0\n1975-01-01\n834\n1\n839.682800\n\n\n1\n1975-02-01\n782\n1\n802.071838\n\n\n2\n1975-03-01\n892\n1\n896.117126\n\n\n...\n...\n...\n...\n...\n\n\n9\n1975-10-01\n827\n1\n824.135498\n\n\n10\n1975-11-01\n797\n1\n795.691162\n\n\n11\n1975-12-01\n843\n1\n833.316345\n\n\n\n\n12 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([train, Y_hat1]).set_index('ds')\nplot_df[['y', \"OptimizedTheta\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Monthly Milk Production', fontsize=20)\nax.set_xlabel('Month [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[95] means that the model expects the real value to be inside that interval 95% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nsf.predict(h=horizon)\n\n\n\n\n\n\n\n\nds\nOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n1\n1975-01-01\n839.682800\n\n\n1\n1975-02-01\n802.071838\n\n\n1\n1975-03-01\n896.117126\n\n\n...\n...\n...\n\n\n1\n1975-10-01\n824.135498\n\n\n1\n1975-11-01\n795.691162\n\n\n1\n1975-12-01\n833.316345\n\n\n\n\n12 rows √ó 2 columns\n\n\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95]) \nforecast_df\n\n\n\n\n\n\n\n\nds\nOptimizedTheta\nOptimizedTheta-lo-80\nOptimizedTheta-hi-80\nOptimizedTheta-lo-95\nOptimizedTheta-hi-95\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n1\n1975-01-01\n839.682800\n766.665955\n928.326233\n742.509583\n955.414307\n\n\n1\n1975-02-01\n802.071838\n704.290100\n899.335876\n643.581360\n945.119263\n\n\n1\n1975-03-01\n896.117126\n761.334717\n1007.408630\n710.785217\n1065.057495\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1\n1975-10-01\n824.135498\n623.904114\n996.567322\n555.948975\n1084.320312\n\n\n1\n1975-11-01\n795.691162\n576.546753\n975.490967\n503.148010\n1036.519531\n\n\n1\n1975-12-01\n833.316345\n606.713989\n1033.886230\n530.259949\n1106.636963\n\n\n\n\n12 rows √ó 6 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nOptimizedTheta\nOptimizedTheta-lo-80\nOptimizedTheta-hi-80\nOptimizedTheta-lo-95\nOptimizedTheta-hi-95\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n1962-01-01\n589.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1962-02-01\n561.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1962-03-01\n640.0\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1975-10-01\nNaN\nNaN\n824.135498\n623.904114\n996.567322\n555.948975\n1084.320312\n\n\n1975-11-01\nNaN\nNaN\n795.691162\n576.546753\n975.490967\n503.148010\n1036.519531\n\n\n1975-12-01\nNaN\nNaN\n833.316345\n606.713989\n1033.886230\n530.259949\n1106.636963\n\n\n\n\n180 rows √ó 7 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series, also let‚Äôs draw the confidence interval that we have obtained when making the prediction with 95% confidence.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(12*10)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=3 , )\n    colors = ['green', \"lime\"]\n    ax.fill_between(df_plot.index, \n                df_plot['OptimizedTheta-lo-80'], \n                df_plot['OptimizedTheta-lo-80'],\n                alpha=.20,\n                color='orange',\n                label='OptimizedTheta_level_80')\n    ax.fill_between(df_plot.index, \n                df_plot['OptimizedTheta-lo-95'], \n                df_plot['OptimizedTheta-hi-95'],\n                alpha=.3,\n                color='lime',\n                label='OptimizedTheta_level_95')\n    ax.set_title('', fontsize=22)\n    ax.set_ylabel(\"Montly Mil Production\", fontsize=20)\n    ax.set_xlabel('Month', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid(True)\n    plt.show()\n\n\nplot_forecasts(train, test, forecast_df, models=['OptimizedTheta'])\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df, level=[95])"
  },
  {
    "objectID": "docs/models/optimizedtheta.html#cross-validation",
    "href": "docs/models/optimizedtheta.html#cross-validation",
    "title": "Optimized Theta Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=12,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n1972-01-01\n1971-12-01\n826.0\n828.836365\n\n\n1\n1972-02-01\n1971-12-01\n799.0\n792.592346\n\n\n1\n1972-03-01\n1971-12-01\n890.0\n883.269592\n\n\n...\n...\n...\n...\n...\n\n\n1\n1974-10-01\n1973-12-01\n812.0\n812.183838\n\n\n1\n1974-11-01\n1973-12-01\n773.0\n783.898376\n\n\n1\n1974-12-01\n1973-12-01\n813.0\n821.124329\n\n\n\n\n36 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/optimizedtheta.html#model-evaluation",
    "href": "docs/models/optimizedtheta.html#model-evaluation",
    "title": "Optimized Theta Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Optimized Theta Model (OTM).\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"OptimizedTheta\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  14.504839\n\n\nAs you have noticed, we have used the cross validation results to perform the evaluation of our model.\nNow we are going to evaluate our model with the results of the predictions, we will use different types of metrics MAE, MAPE, MASE, RMSE, SMAPE to evaluate the accuracy.\n\nfrom datasetsforecast.losses import (mae, mape, mase, rmse, smape)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, model):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    evaluation[model] = {}\n    for metric in [mase, mae, mape, rmse, smape]:\n        metric_name = metric.__name__\n        if metric_name == 'mase':\n            evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                y_true[model].values, \n                                                y_hist['y'].values, seasonality=12)\n        else:\n            evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(train, test, Y_hat, model=\"OptimizedTheta\")\n\n\n\n\n\n\n\n\nmae\nmape\nmase\nrmse\nsmape\n\n\n\n\nOptimizedTheta\n6.740209\n0.782753\n0.30312\n8.701501\n0.778689"
  },
  {
    "objectID": "docs/models/optimizedtheta.html#acknowledgements",
    "href": "docs/models/optimizedtheta.html#acknowledgements",
    "title": "Optimized Theta Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/optimizedtheta.html#references",
    "href": "docs/models/optimizedtheta.html#references",
    "title": "Optimized Theta Model",
    "section": "References ",
    "text": "References \n\nKostas I. Nikolopoulos, Dimitrios D. Thomakos. Forecasting with the Theta Method-Theory and Applications. 2019 John Wiley & Sons Ltd.\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/crostonclassic.html",
    "href": "docs/models/crostonclassic.html",
    "title": "CrostonClassic Model",
    "section": "",
    "text": "Introduction\nCroston Classic Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of CrostonClassic with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/crostonclassic.html#table-of-contents",
    "href": "docs/models/crostonclassic.html#table-of-contents",
    "title": "CrostonClassic Model",
    "section": "",
    "text": "Introduction\nCroston Classic Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of CrostonClassic with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/crostonclassic.html#introduction",
    "href": "docs/models/crostonclassic.html#introduction",
    "title": "CrostonClassic Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Croston model is a method used in time series analysis to forecast demand in situations where there are intermittent data or frequent zeros. It was developed by J.D. Croston in 1972 and is especially useful in industries such as inventory management, retail sales, and demand forecasting for products with low sales frequency.\nThe Croston model is based on two main components:\n\nIntermittent Demand Rate: Calculates the demand rate for periods in which sales or events occur, ignoring periods without sales. This rate is used to estimate the probability that a claim will occur in the future.\nDemand Interval: Calculates the time interval between sales or events occurring, again ignoring non-sales periods. This interval is used to estimate the probability that a demand will occur in the next period.\n\nThe Croston model combines these two estimates to generate a weighted forecast that takes into account both the rate of intermittent demand and the interval between demands. This approach helps address the challenge of forecasting demand in situations where the time series has many zeros or missing values.\nIt is important to note that the Croston model is a simplification and does not account for other possible sources of variability or patterns in the demand data. Therefore, its accuracy may be affected in situations where there are external factors or changes in demand behavior."
  },
  {
    "objectID": "docs/models/crostonclassic.html#croston-classic-model",
    "href": "docs/models/crostonclassic.html#croston-classic-model",
    "title": "CrostonClassic Model",
    "section": "Croston Classic Model ",
    "text": "Croston Classic Model \n\nWhat is intermittent demand?\nIntermittent demand is a demand pattern characterized by the irregular and sporadic occurrence of events or sales. In other words, it refers to situations in which the demand for a product or service occurs intermittently, with periods of time in which there are no sales or significant events.\nIntermittent demand differs from constant or regular demand, where sales occur in a predictable and consistent manner over time. In contrast, in intermittent demand, periods without sales may be long and there may not be a regular sequence of events.\nThis type of demand can occur in different industries and contexts, such as low consumption products, seasonal products, high variability products, products with short life cycles, or in situations where demand depends on specific events or external factors.\nIntermittent demand can pose challenges in forecasting and inventory management, as it is difficult to predict when sales will occur and in what quantity. Methods like the Croston model, which I mentioned earlier, are used to address intermittent demand and generate more accurate and appropriate forecasts for this type of demand pattern.\n\n\nProblem with intermittent demand\nIntermittent demand can present various challenges and issues in inventory management and demand forecasting. Some of the common problems associated with intermittent demand are as follows:\n\nUnpredictable variability: Intermittent demand can have unpredictable variability, making planning and forecasting difficult. Demand patterns can be irregular and fluctuate dramatically between periods with sales and periods without sales.\nLow frequency of sales: Intermittent demand is characterized by long periods without sales. This can lead to inventory management difficulties, as it is necessary to hold enough stock to meet demand when it occurs, while avoiding excess inventory during non-sales periods.\nForecast error: Forecasting intermittent demand can be more difficult to pin down than constant demand. Traditional forecast models may not be adequate to capture the variability and lack of patterns in intermittent demand, which can lead to significant errors in estimates of future demand.\nImpact on the supply chain: Intermittent demand can affect the efficiency of the supply chain and create difficulties in production planning, supplier management and logistics. Lead times and inventory levels must be adjusted to meet unpredictable demand.\nOperating costs: Managing inventory in situations of intermittent demand can increase operating costs. Maintaining adequate inventory during non-sales periods and managing stock levels may require additional investments in storage and logistics.\n\nTo address these issues, specific approaches to intermittent demand management are used, such as specialized forecasting models, product classification techniques, and tailored inventory strategies. These solutions seek to minimize the impacts of variability and lack of patterns in intermittent demand, optimizing inventory management and improving supply chain efficiency.\n\n\nCroston‚Äôs method(CR)\nCroston‚Äôs method(CR) is a classic method that specifically dealing with intermittent demand, it was developed base upon the Simple Exponential Smoothing method. When Croston dealing with the intermittent demand, he found out that by using the SES, the level of forecasting in each period‚Äôs demand are normally higher than it‚Äôs actual value, which lead to a very low accuracy. After a period of times of research, he came out a method that optimize the result of the intermittent demand forecasting.\nThis method basically decompose the intermittent demand into two parts: the size of non-zero demand and the time interval of those demand occurred, and then apply the simple exponential smoothing on both part. Where the formula is follow:\nif \\(Z_t=0\\) then:\n\\[Z'_t= Z'_{t-1}\\]\n\\[P'_t= P'_{t-1}\\]\nOtherwise\n\\[Z'_t=\\alpha Z_t +(1-\\alpha) Z'_{t-1}\\]\n\\[P'_t=\\alpha P_t +(1-\\alpha) P'_{t-1}\\]\nwhere \\(0&lt; \\alpha &lt; 1\\)\nAnd finally by combining these forecasts\n\\[{Y'}_t = \\frac{{Z'}_t}{{P'}_t}\\]\nWhere\n\n\\({Y'}_t:\\) Average demand per period.\n\\(Z_t:\\) Actual demand at period \\(t\\).\n\\(Z'_t:\\) Time between two positive demand.\n\\(P:\\) Demand size forecast for next period.\n\\(P_t:\\) Forecast of demand interval.\n\\(\\alpha :\\) Smoothing constant.\n\nCroston‚Äôs method converse the intermittent demand time series into a non-zero demand time series and a demand interval time series, many cases show that this method work quite well, but before apply Croston‚Äôs method, three assumptions should be made:\n\nThe non-zero demand are independent and obey normal distribution;\nThe demand intervals are independent and obey geometric distribution;\nThere are mutual independence between the demand size and demand intervals.\n\nAccording to many real cases show that, Croston‚Äôs method is suitable for the situation which the lead time obey normal distribution, for those demand series which contain large amount of zero values, Croston‚Äôs method did not shows a outstanding performance, sometimes even worse than SES method.\nAdditionally, Croston‚Äôs method can only provide the average demand for each period, it can not give a forecast of the demand size for each period, it can not forecast which period will occurred a demand, and it also can not come out a probability of whether a period will occurred a demand.\nAfter all, although Croston‚Äôs method is a very classic and wide use method, it still has a lots of limitations, but after years of research carried by statisticians and scholars, few variations of Croston‚Äôs method were brought up.\n\n\nCroston‚Äôs variations\nCroston‚Äôs method is the main model used in demand forecasting area, most of the works are based upon this model. However, in 2001 Syntetos and Boylan proposed that Croston‚Äôs method is no a unbiased method, while some empirical evidence also showed that the losses in performance which use the Croston‚Äôs method (Sani and Kingsman, 1997). Plenty of further research is done in improving the Croston‚Äôs method. Syntetos and Boylan (2005) proposed an approximate unbiased procedure that provide less variance in the result of estimate, which is known as SBA (Syntetos and Boylan Approximate). Recently, Teunter et al.¬†(2011) also proposed a intermittent forecasting method that can deal with obsolescence, which is based on Croston‚Äôs method known as TSB method (Teunter, Syntetos and Babai).\n\n\nArea of application of the Croston method\nThe Croston method is commonly applied in the field of inventory management and demand forecasting in situations of intermittent demand. Some specific areas where the Croston model can be applied are:\n\nInventory management: The Croston model is used to forecast demand for products with sporadic or intermittent sales. Helps determine optimal inventory levels and replenishment policies, minimizing inventory costs and ensuring adequate availability to meet intermittent demand.\nRetail sales: In the retail sector, especially in products with low sales frequency or irregular sales, the Croston model can be useful for forecasting demand and optimizing inventory planning in stores or warehouses.\nDemand forecasting: In general, the Croston model is applied in demand forecasting when there is a lack of clear patterns or high variability in the time series. It can be used in various industries, such as the pharmaceutical industry, the automotive industry, the perishable goods industry, and other sectors where intermittent demand is common.\nSupply Chain Planning: The Croston model can be used in supply chain planning and management to improve the accuracy of intermittent demand forecasts. This helps streamline production, inventory management, supplier order scheduling, and other aspects of the supply chain.\n\nIt is important to note that Croston‚Äôs model is just one of many approaches available to address intermittent demand. Depending on the context and the specific characteristics of the time series, there may be other more appropriate methods and techniques.\n\n\nCroston Method for Stationary Time Series\nNo, the time series in the Croston method does not have to be stationary. The Croston method is an effective forecasting method for intermittent time series, even if they are not stationary. However, if the time series is stationary, the Croston method may be more accurate.\nThe Croston method is based on the idea that intermittent time series can be decomposed into two components: a demand component and a time between demands component. The demand component is forecast using a standard time series forecasting method, such as single or double exponential smoothing. The time component between demands is forecast using a probability distribution function, such as a Poisson distribution or a Weibull distribution.\nThe Croston method then combines the forecasts for the two components to obtain a total demand forecast for the next period.\nIf the time series is stationary, the two components of the time series will be stationary as well. This means that the Croston method will be able to forecast the two components more accurately.\nHowever, even if the time series is not stationary, the Croston method can still be an effective forecasting method. The Croston method is a robust method that can handle time series with irregular demand patterns.\nIf you are using the Croston method to forecast an intermittent time series that is not stationary, it is important to choose a standard time series forecast method that is effective for nonstationary time series. Double exponential smoothing is an effective forecasting method for non-stationary time series."
  },
  {
    "objectID": "docs/models/crostonclassic.html#loading-libraries-and-data",
    "href": "docs/models/crostonclassic.html#loading-libraries-and-data",
    "title": "CrostonClassic Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nimport pandas as pd\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/intermittend_demand2\")\ndf.head()\n\n\n\n\n\n\n\n\ndate\nsales\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n\n\n1\n2022-01-01 01:00:00\n10\n\n\n2\n2022-01-01 02:00:00\n0\n\n\n3\n2022-01-01 03:00:00\n0\n\n\n4\n2022-01-01 04:00:00\n100\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n1\n\n\n1\n2022-01-01 01:00:00\n10\n1\n\n\n2\n2022-01-01 02:00:00\n0\n1\n\n\n3\n2022-01-01 03:00:00\n0\n1\n\n\n4\n2022-01-01 04:00:00\n100\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/crostonclassic.html#explore-data-with-the-plot-method",
    "href": "docs/models/crostonclassic.html#explore-data-with-the-plot-method",
    "title": "CrostonClassic Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=24,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "docs/models/crostonclassic.html#split-the-data-into-training-and-testing",
    "href": "docs/models/crostonclassic.html#split-the-data-into-training-and-testing",
    "title": "CrostonClassic Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our Croston Classic Model.\nData to test our model\n\nFor the test data we will use the last 500 hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2023-01-31 19:00:00'] \ntest = df[df.ds&gt;'2023-01-31 19:00:00']\n\n\ntrain.shape, test.shape\n\n((9500, 3), (500, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\",linewidth=2)\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\", linewidth=2, color=\"yellow\")\nplt.title(\"Store visit\");\nplt.show()"
  },
  {
    "objectID": "docs/models/crostonclassic.html#implementation-of-crostonclassic-with-statsforecast",
    "href": "docs/models/crostonclassic.html#implementation-of-crostonclassic-with-statsforecast",
    "title": "CrostonClassic Model",
    "section": "Implementation of CrostonClassic with StatsForecast ",
    "text": "Implementation of CrostonClassic with StatsForecast \nTo also know more about the parameters of the functions of the CrostonClassic Model, they are listed below. For more information, visit the documentation.\nalias : str\n    Custom name of the model.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import CrostonClassic\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\nmodels = [CrostonClassic()]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[CrostonClassic])\n\n\nLet‚Äôs see the results of our Croston Classic Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([23.606695], dtype=float32)}\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 25 week ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon)\nY_hat\n\n\n\n\n\n\n\n\nds\nCrostonClassic\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n23.606695\n\n\n1\n2023-02-21 17:00:00\n23.606695\n\n\n1\n2023-02-21 18:00:00\n23.606695\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n23.606695\n\n\n1\n2023-03-14 10:00:00\n23.606695\n\n\n1\n2023-03-14 11:00:00\n23.606695\n\n\n\n\n500 rows √ó 2 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nCrostonClassic\n\n\n\n\n0\n1\n2023-02-21 16:00:00\n23.606695\n\n\n1\n1\n2023-02-21 17:00:00\n23.606695\n\n\n2\n1\n2023-02-21 18:00:00\n23.606695\n\n\n...\n...\n...\n...\n\n\n497\n1\n2023-03-14 09:00:00\n23.606695\n\n\n498\n1\n2023-03-14 10:00:00\n23.606695\n\n\n499\n1\n2023-03-14 11:00:00\n23.606695\n\n\n\n\n500 rows √ó 3 columns\n\n\n\n\nY_hat1 = pd.concat([df,Y_hat])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nCrostonClassic\n\n\n\n\n0\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n1\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n497\n2023-03-14 09:00:00\nNaN\n1\n23.606695\n\n\n498\n2023-03-14 10:00:00\nNaN\n1\n23.606695\n\n\n499\n2023-03-14 11:00:00\nNaN\n1\n23.606695\n\n\n\n\n10500 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df, Y_hat1]).set_index('ds')\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[ \"CrostonClassic\"].plot(ax=ax, linewidth=2, color=\"yellow\")\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Store visit (Hourly data)', fontsize=20)\nax.set_xlabel('Hourly', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \nforecast_df\n\n\n\n\n\n\n\n\nds\nCrostonClassic\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n23.606695\n\n\n1\n2023-02-21 17:00:00\n23.606695\n\n\n1\n2023-02-21 18:00:00\n23.606695\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n23.606695\n\n\n1\n2023-03-14 10:00:00\n23.606695\n\n\n1\n2023-03-14 11:00:00\n23.606695\n\n\n\n\n500 rows √ó 2 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nCrostonClassic\n\n\nds\n\n\n\n\n\n\n\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n23.606695\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n23.606695\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n23.606695\n\n\n\n\n10500 rows √ó 3 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds').tail(5000)\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nCrostonClassic\n\n\nds\n\n\n\n\n\n\n\n2022-08-18 04:00:00\n0.0\n1\nNaN\n\n\n2022-08-18 05:00:00\n80.0\n1\nNaN\n\n\n2022-08-18 06:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n23.606695\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n23.606695\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n23.606695\n\n\n\n\n5000 rows √ó 3 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series.\n\nplt.plot(df_plot['y'],label=\"Actual\", linewidth=2.5)\nplt.plot(df_plot['CrostonClassic'], label=\"CrostonClassic\", color=\"yellow\") # '-', '--', '-.', ':',\n\nplt.title(\"Store visit (Hourly data)\");\nplt.xlabel(\"Hourly\")\nplt.ylabel(\"\")\n\nText(0, 0.5, '')\n\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/crostonclassic.html#cross-validation",
    "href": "docs/models/crostonclassic.html#cross-validation",
    "title": "CrostonClassic Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second hour (step_size=50). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents \\(h\\) steps into the future that are being forecasted. In this case, 500 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=50,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nCrostonClassic\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2023-01-23 12:00:00\n2023-01-23 11:00:00\n0.0\n23.655830\n\n\n1\n2023-01-23 13:00:00\n2023-01-23 11:00:00\n0.0\n23.655830\n\n\n1\n2023-01-23 14:00:00\n2023-01-23 11:00:00\n0.0\n23.655830\n\n\n...\n...\n...\n...\n...\n\n\n1\n2023-02-21 13:00:00\n2023-01-31 19:00:00\n60.0\n27.418417\n\n\n1\n2023-02-21 14:00:00\n2023-01-31 19:00:00\n20.0\n27.418417\n\n\n1\n2023-02-21 15:00:00\n2023-01-31 19:00:00\n20.0\n27.418417\n\n\n\n\n2500 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/crostonclassic.html#model-evaluation",
    "href": "docs/models/crostonclassic.html#model-evaluation",
    "title": "CrostonClassic Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Croston Classic Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"CrostonClassic\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  48.08823"
  },
  {
    "objectID": "docs/models/crostonclassic.html#acknowledgements",
    "href": "docs/models/crostonclassic.html#acknowledgements",
    "title": "CrostonClassic Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/adida.html",
    "href": "docs/models/adida.html",
    "title": "ADIDA Model",
    "section": "",
    "text": "Introduction\nADIDA Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of ADIDA with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/adida.html#table-of-contents",
    "href": "docs/models/adida.html#table-of-contents",
    "title": "ADIDA Model",
    "section": "",
    "text": "Introduction\nADIDA Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of ADIDA with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/adida.html#introduction",
    "href": "docs/models/adida.html#introduction",
    "title": "ADIDA Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Aggregate-Disaggregate Intermittent Demand Approach (ADIDA) is a forecasting method that is used to predict the demand for products that exhibit intermittent demand patterns. Intermittent demand patterns are characterized by a large number of zero observations, which can make forecasting challenging.\nThe ADIDA method uses temporal aggregation to reduce the number of zero observations and mitigate the effect of the variance observed in the intervals. The method uses equally sized time buckets to perform non-overlapping temporal aggregation and predict the demand over a pre-specified lead time. The time bucket is set equal to the mean inter-demand interval, which is the average time between two consecutive non-zero observations.\nThe method uses the Simple Exponential Smoothing (SES) technique to obtain the forecasts. SES is a popular time series forecasting technique that is commonly used for its simplicity and effectiveness in producing accurate forecasts.\nThe ADIDA method has several advantages. It is easy to implement and can be used for a wide range of intermittent demand patterns. The method also provides accurate forecasts and can be used to predict the demand over a pre-specified lead time.\nHowever, the ADIDA method has some limitations. The method assumes that the time buckets are equally sized, which may not be the case for all intermittent demand patterns. Additionally, the method may not be suitable for time series data with complex patterns or trends.\nOverall, the ADIDA method is a useful forecasting technique for intermittent demand patterns that can help mitigate the effect of zero observations and produce accurate demand forecasts."
  },
  {
    "objectID": "docs/models/adida.html#adida-model",
    "href": "docs/models/adida.html#adida-model",
    "title": "ADIDA Model",
    "section": "ADIDA Model ",
    "text": "ADIDA Model \n\nWhat is intermittent demand?\nIntermittent demand is a demand pattern characterized by the irregular and sporadic occurrence of events or sales. In other words, it refers to situations in which the demand for a product or service occurs intermittently, with periods of time in which there are no sales or significant events.\nIntermittent demand differs from constant or regular demand, where sales occur in a predictable and consistent manner over time. In contrast, in intermittent demand, periods without sales may be long and there may not be a regular sequence of events.\nThis type of demand can occur in different industries and contexts, such as low consumption products, seasonal products, high variability products, products with short life cycles, or in situations where demand depends on specific events or external factors.\nIntermittent demand can pose challenges in forecasting and inventory management, as it is difficult to predict when sales will occur and in what quantity. Methods like the Croston model, which I mentioned earlier, are used to address intermittent demand and generate more accurate and appropriate forecasts for this type of demand pattern.\n\n\nProblem with intermittent demand\nIntermittent demand can present various challenges and issues in inventory management and demand forecasting. Some of the common problems associated with intermittent demand are as follows:\n\nUnpredictable variability: Intermittent demand can have unpredictable variability, making planning and forecasting difficult. Demand patterns can be irregular and fluctuate dramatically between periods with sales and periods without sales.\nLow frequency of sales: Intermittent demand is characterized by long periods without sales. This can lead to inventory management difficulties, as it is necessary to hold enough stock to meet demand when it occurs, while avoiding excess inventory during non-sales periods.\nForecast error: Forecasting intermittent demand can be more difficult to pin down than constant demand. Traditional forecast models may not be adequate to capture the variability and lack of patterns in intermittent demand, which can lead to significant errors in estimates of future demand.\nImpact on the supply chain: Intermittent demand can affect the efficiency of the supply chain and create difficulties in production planning, supplier management and logistics. Lead times and inventory levels must be adjusted to meet unpredictable demand.\nOperating costs: Managing inventory in situations of intermittent demand can increase operating costs. Maintaining adequate inventory during non-sales periods and managing stock levels may require additional investments in storage and logistics.\n\nTo address these issues, specific approaches to intermittent demand management are used, such as specialized forecasting models, product classification techniques, and tailored inventory strategies. These solutions seek to minimize the impacts of variability and lack of patterns in intermittent demand, optimizing inventory management and improving supply chain efficiency.\n\n\nADIDA Model\nThe ADIDA model is based on the Simple Exponential Smoothing (SES) method and uses temporal aggregation to handle the problem of intermittent demand. The mathematical development of the model can be summarized as follows:\nLet St be the demand at time \\(t\\), where \\(t = 1, 2, ..., T\\). The mean inter-demand interval is denoted as MI, which is the average time between two consecutive non-zero demands. The time bucket size is set equal to MI.\nThe demand data is then aggregated into non-overlapping time buckets of size MI. Let Bt be the demand in bucket \\(t\\), where \\(t = 1, 2, ..., T/MI\\). The aggregated demand data can be represented as:\n\\[B_t = \\sum S_t, for (t-1)*MI + 1 ‚â§ j ‚â§ t*MI\\]\nThe SES method is then applied to the aggregated demand data to obtain the forecasts. The forecast for bucket \\(t\\) is denoted as \\(F_t\\). The SES method involves estimating the level \\(L_t\\) at time t based on the actual demand \\(D_t\\) at time t and the estimated level at the previous time period, \\(L_{t-1}\\), using the following equation:\n\\[L_t = \\alpha * D_t + (1 - Œ±) * L_{t-1}\\]\nwhere \\(\\alpha\\) is the smoothing parameter that controls the weight given to the current demand value.\nThe forecast for bucket \\(t\\) is then obtained by using the estimated level at the previous time period, \\(L_{t-1}\\), as follows:\n\\[F_t = L_{t-1}\\]\nThe forecasts are then disaggregated to obtain the demand predictions for the original time period. Let \\(Y_t\\) be the demand prediction at time \\(t\\). The disaggregation can be performed using the following equation:\n\\[Y_t = F_t / MI, for (t-1)*MI + 1 ‚â§ j ‚â§ t*MI\\]\n\n\nHow can you determine if the ADIDA model is suitable for a specific data set?\nTo determine if the ADIDA model is suitable for a specific data set, the following steps can be followed:\n\nAnalyze the demand pattern: Examine the demand pattern of the data to determine if it fits an intermittent pattern. Intermittent data is characterized by a high proportion of zeros and sporadic demands in specific periods.\nEvaluate seasonality: Check if there is a clear seasonality in the data. The ADIDA model assumes that there is no seasonality or that it can be handled by temporal aggregation. If the data show complex seasonality or cannot be handled by temporal aggregation, the ADIDA model may not be suitable.\nData requirements: Consider the data requirements of the ADIDA model. The model requires historical demand data and the ability to calculate the mean interval between non-zero demands. Make sure you have enough data to estimate the parameters and that the data is available at a frequency suitable for temporal aggregation.\nPerformance evaluation: Perform a performance evaluation of the ADIDA model on the specific data set. Compare model-generated forecasts with actual demand values and use evaluation metrics such as mean absolute error (MAE) or mean square error (MSE). If the model performs well and produces accurate forecasts on the data set, this is an indication that it is suitable for that data set.\nComparison with other models: Compare the performance of the ADIDA model with other forecast models suitable for intermittent data. Consider models like Croston, Syntetos-Boylan Approximation (SBA), or models based on exponential smoothing techniques that have been developed specifically for intermittent data. If the ADIDA model shows similar or better performance than other models, it can be considered suitable.\n\nRemember that the adequacy of the ADIDA model may depend on the specific nature of the data and the context of the forecasting problem. It is advisable to carry out a thorough analysis and experiment with different models to determine the most appropriate approach for the data set in question."
  },
  {
    "objectID": "docs/models/adida.html#loading-libraries-and-data",
    "href": "docs/models/adida.html#loading-libraries-and-data",
    "title": "ADIDA Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nimport plotly.graph_objects as go\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/tipos_malarias_choco_colombia.csv\", sep=\";\", usecols=[0,4])\ndf = df.dropna()\ndf.head()\n\n\n\n\n\n\n\n\nsemanas\nmalaria_falciparum\n\n\n\n\n0\n2007-12-31\n50.0\n\n\n1\n2008-01-07\n62.0\n\n\n2\n2008-01-14\n76.0\n\n\n3\n2008-01-21\n64.0\n\n\n4\n2008-01-28\n38.0\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2007-12-31\n50.0\n1\n\n\n1\n2008-01-07\n62.0\n1\n\n\n2\n2008-01-14\n76.0\n1\n\n\n3\n2008-01-21\n64.0\n1\n\n\n4\n2008-01-28\n38.0\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds            object\ny            float64\nunique_id     object\ndtype: object\n\n\nWe need to convert the object types to datetime and numeric.\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])\ndf[\"y\"] = df[\"y\"].astype(float).astype(\"int64\")"
  },
  {
    "objectID": "docs/models/adida.html#explore-data-with-the-plot-method",
    "href": "docs/models/adida.html#explore-data-with-the-plot-method",
    "title": "ADIDA Model",
    "section": "Explore data with the plot method ",
    "text": "Explore data with the plot method \nPlot a series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plot_seasonal_decompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplot_seasonal_decompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=52,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "docs/models/adida.html#split-the-data-into-training-and-testing",
    "href": "docs/models/adida.html#split-the-data-into-training-and-testing",
    "title": "ADIDA Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our ADIDA Model. 2. Data to test our model\nFor the test data we will use the last 25 week to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2022-07-04'] \ntest = df[df.ds&gt;'2022-07-04']\n\n\ntrain.shape, test.shape\n\n((758, 3), (25, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\",linewidth=2)\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\", linewidth=2, color=\"yellow\")\nplt.title(\"Falciparum Malaria\");\nplt.show()"
  },
  {
    "objectID": "docs/models/adida.html#implementation-of-adida-model-with-statsforecast",
    "href": "docs/models/adida.html#implementation-of-adida-model-with-statsforecast",
    "title": "ADIDA Model",
    "section": "Implementation of ADIDA Model with StatsForecast ",
    "text": "Implementation of ADIDA Model with StatsForecast \nTo also know more about the parameters of the functions of the ADIDA Model, they are listed below. For more information, visit the documentation.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import ADIDA\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 52 # Hourly data \nhorizon = len(test) # number of predictions\n\n# We call the model that we are going to use\nmodels = [ADIDA()]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='W', \n                   n_jobs=-1)\n\n\n\nFit the Model\nHere, we call the fit() method to fit the model.\n\nsf.fit()\n\nStatsForecast(models=[ADIDA])\n\n\nLet‚Äôs see the results of our ADIDA Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([132.52887], dtype=float32)}\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the forecast() method does not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 25 week ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon)\nY_hat\n\n\n\n\n\n\n\n\nds\nADIDA\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-01-01\n132.52887\n\n\n1\n2023-01-08\n132.52887\n\n\n1\n2023-01-15\n132.52887\n\n\n...\n...\n...\n\n\n1\n2023-06-04\n132.52887\n\n\n1\n2023-06-11\n132.52887\n\n\n1\n2023-06-18\n132.52887\n\n\n\n\n25 rows √ó 2 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nADIDA\n\n\n\n\n0\n1\n2023-01-01\n132.52887\n\n\n1\n1\n2023-01-08\n132.52887\n\n\n2\n1\n2023-01-15\n132.52887\n\n\n...\n...\n...\n...\n\n\n22\n1\n2023-06-04\n132.52887\n\n\n23\n1\n2023-06-11\n132.52887\n\n\n24\n1\n2023-06-18\n132.52887\n\n\n\n\n25 rows √ó 3 columns\n\n\n\nMerge the forecasts with the true values\n\nY_hat1 = pd.concat([df,Y_hat],  keys=['unique_id', 'ds'])\nY_hat1\n\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nADIDA\n\n\n\n\nunique_id\n0\n2007-12-31\n50.0\n1\nNaN\n\n\n1\n2008-01-07\n62.0\n1\nNaN\n\n\n2\n2008-01-14\n76.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\nds\n22\n2023-06-04\nNaN\n1\n132.52887\n\n\n23\n2023-06-11\nNaN\n1\n132.52887\n\n\n24\n2023-06-18\nNaN\n1\n132.52887\n\n\n\n\n808 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df, Y_hat1]).set_index('ds').tail(300)\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[ \"ADIDA\"].plot(ax=ax, linewidth=2, color=\"yellow\")\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Falciparum Malaria (Weekly data)', fontsize=20)\nax.set_xlabel('Monthly [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 25 week ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \n\nforecast_df.head()\n\n\n\n\n\n\n\n\nds\nADIDA\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-01-01\n132.52887\n\n\n1\n2023-01-08\n132.52887\n\n\n1\n2023-01-15\n132.52887\n\n\n1\n2023-01-22\n132.52887\n\n\n1\n2023-01-29\n132.52887\n\n\n\n\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nADIDA\n\n\nds\n\n\n\n\n\n\n\n2007-12-31\n50.0\n1\nNaN\n\n\n2008-01-07\n62.0\n1\nNaN\n\n\n2008-01-14\n76.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-06-04\nNaN\nNaN\n132.52887\n\n\n2023-06-11\nNaN\nNaN\n132.52887\n\n\n2023-06-18\nNaN\nNaN\n132.52887\n\n\n\n\n808 rows √ó 3 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds').tail(200)\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nADIDA\n\n\nds\n\n\n\n\n\n\n\n2019-08-26\n191.0\n1\nNaN\n\n\n2019-09-02\n205.0\n1\nNaN\n\n\n2019-09-09\n200.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-06-04\nNaN\nNaN\n132.52887\n\n\n2023-06-11\nNaN\nNaN\n132.52887\n\n\n2023-06-18\nNaN\nNaN\n132.52887\n\n\n\n\n200 rows √ó 3 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series.\n\nplt.plot(df_plot['y'],label=\"Actual\", linewidth=2.5)\nplt.plot(df_plot['ADIDA'], label=\"ADIDA\", color=\"yellow\") # '-', '--', '-.', ':',\n\nplt.title(\"Falciparum Malaria (Weekly data)\");\nplt.xlabel(\"Weekly\")\nplt.ylabel(\"\")\n\nText(0, 0.5, '')\n\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/adida.html#cross-validation",
    "href": "docs/models/adida.html#cross-validation",
    "title": "ADIDA Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\n\nimg\n\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=30,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nADIDA\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2020-03-22\n2020-03-15\n317.0\n251.901505\n\n\n1\n2020-03-29\n2020-03-15\n332.0\n251.901505\n\n\n1\n2020-04-05\n2020-03-15\n306.0\n251.901505\n\n\n...\n...\n...\n...\n...\n\n\n1\n2022-12-11\n2022-07-03\n151.0\n336.747375\n\n\n1\n2022-12-18\n2022-07-03\n97.0\n336.747375\n\n\n1\n2022-12-25\n2022-07-03\n42.0\n336.747375\n\n\n\n\n125 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/adida.html#model-evaluation",
    "href": "docs/models/adida.html#model-evaluation",
    "title": "ADIDA Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, ADIDA Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"ADIDA\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  84.45186"
  },
  {
    "objectID": "docs/models/adida.html#acknowledgements",
    "href": "docs/models/adida.html#acknowledgements",
    "title": "ADIDA Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/adida.html#references",
    "href": "docs/models/adida.html#references",
    "title": "ADIDA Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/crostonsba.html",
    "href": "docs/models/crostonsba.html",
    "title": "CrostonSBA Model",
    "section": "",
    "text": "Introduction\nCroston SBA Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of CrostonSBA with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/crostonsba.html#table-of-contents",
    "href": "docs/models/crostonsba.html#table-of-contents",
    "title": "CrostonSBA Model",
    "section": "",
    "text": "Introduction\nCroston SBA Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of CrostonSBA with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/crostonsba.html#introduction",
    "href": "docs/models/crostonsba.html#introduction",
    "title": "CrostonSBA Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Croston model is a method used to forecast time series with intermittent demand data, that is, data that has many periods of zero demand and only a few periods of non-zero demand. Croston‚Äôs approach was originally proposed by J.D. Croston in 1972. Subsequently, Syntetos and Boylan proposed an improvement to the original model in 2001, known as the Croston-SBA (Syntetos and Boylan Approximation).\nThe Croston-SBA model is based on the assumption that intermittent demand follows a binomial process. Instead of directly modeling demand, the focus is on modeling the intervals between demand periods. The model has two main components: one to model the intervals between demand periods (which are assumed to follow a Poisson distribution), and another to model the demands when they occur.\nIt is important to note that the Croston-SBA model assumes that the intervals between the non-zero demand periods are independent and follow a Poisson distribution. However, this model is an approximation and may not work well in all situations. It is advisable to evaluate its performance on historical data before using it in practice."
  },
  {
    "objectID": "docs/models/crostonsba.html#croston-sba-model",
    "href": "docs/models/crostonsba.html#croston-sba-model",
    "title": "CrostonSBA Model",
    "section": "Croston SBA Model ",
    "text": "Croston SBA Model \nThe formula of SBA is very similar to the original Croston‚Äôs method, however, it apply a correction factor which reduce the error in the final estimate result.\nif \\(Z_t=0\\) then\n\\[Z'_t=Z'_{t-1}\\]\n\\[P'_t=P'_{t-1}\\]\nOtherwise\n\\[Z'_t=\\alpha Z_t +(1-\\alpha)Z'_{t-1}\\]\n\\[P'_t=\\alpha P_t +(1- \\alpha) P'_{t-1}\\ where \\ 0&lt;\\alpha &lt; 1\\]\n\\[Y'_t=(1-\\frac{\\alpha}{2}) \\frac{Z'_t}{P'_t}\\]\nwhere\n\n\\(Y'_t:\\) Average demand per period\n\\(Z_t:\\) Actual demand at period \\(t\\)\n\\(Z'_t:\\) Time between two positive demand\n\\(P:\\) Demand size forecast for next period\n\\(P'_t:\\) Forecast of demand interval\n\\(\\alpha:\\) Smoothing constant\n\nNote: In Croston‚Äôs method, result often will present a considerable positive bias, whereas in SBA the bias is reduced, and sometimes will appear slightly negative bias.\n\nPrincipals of the Croston SBA method\nThe Croston SBA (Syntetos and Boylan Approximate) method is a technique used for forecasting time series with intermittent or sporadic data. This methodology is based on the original Croston method, which was developed to forecast inventory demand in situations where data is sparse or not available at regular intervals.\nThe main properties of the Croston SBA method are the following:\n\nSuitable for intermittent data: The Croston SBA method is especially useful when the data exhibits intermittent patterns, that is, periods of demand followed by periods of non-demand. Instead of treating the data as zero for non-demand periods, the Croston SBA method estimates demand occurrence rates and conditional demand rates.\nSeparation of frequency and level: One of the key features of the Croston SBA method is that it separates the frequency and level information in the demand data. This allows these two components to be modeled and forecasted separately, which can result in better predictions.\nEstimation of occurrence and demand rates: The Croston SBA method uses a simple exponential smoothing technique to estimate conditional occurrence and demand rates. These rates are then used to forecast future demand.\nDoes not assume distribution of the data: Unlike some forecasting techniques that assume a specific distribution of the data, the Croston SBA method makes no assumptions about the distribution of demand. This makes it more flexible and applicable to a wide range of situations.\nDoes not require complete historical data: The Croston SBA method can work even when historical data is sparse or not available at regular intervals. This makes it an attractive option when it comes to forecasting intermittent demand with limited data.\n\nIt is important to note that the Croston SBA method is an approximation and may not be suitable for all cases. It is recommended to evaluate its performance in conjunction with other forecasting techniques and adapt it according to the specific characteristics of the data and the context of the problem.\nIn the Croston SBA method, the data series need not be stationary. The Croston SBA approach is suitable for forecasting time series with intermittent data, where periods of demand are interspersed with periods of non-demand.\nThe Croston SBA method is based on the estimation of occurrence rates and conditional demand rates, using simple exponential smoothing techniques. These rates are used to forecast future demand.\nIn the context of time series, stationarity refers to the property that the statistical properties of the series, such as the mean and variance, are constant over time. However, in the case of intermittent data, it is common for the series not to meet the assumptions of stationarity, since the demand can vary considerably in different periods of time.\nThe Croston SBA method is not based on the assumption of stationarity of the data series. Instead, it focuses on modeling the frequency and level of intermittent demand separately, using simple exponential smoothing techniques. This makes it possible to capture demand occurrence patterns and estimate conditional demand rates, without requiring the stationarity of the series."
  },
  {
    "objectID": "docs/models/crostonsba.html#loading-libraries-and-data",
    "href": "docs/models/crostonsba.html#loading-libraries-and-data",
    "title": "CrostonSBA Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nimport pandas as pd\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/intermittend_demand2\")\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nsales\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n\n\n1\n2022-01-01 01:00:00\n10\n\n\n2\n2022-01-01 02:00:00\n0\n\n\n3\n2022-01-01 03:00:00\n0\n\n\n4\n2022-01-01 04:00:00\n100\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n1\n\n\n1\n2022-01-01 01:00:00\n10\n1\n\n\n2\n2022-01-01 02:00:00\n0\n1\n\n\n3\n2022-01-01 03:00:00\n0\n1\n\n\n4\n2022-01-01 04:00:00\n100\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/crostonsba.html#explore-data-with-the-plot-method",
    "href": "docs/models/crostonsba.html#explore-data-with-the-plot-method",
    "title": "CrostonSBA Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\nAutocorrelation (ACF) and partial autocorrelation (PACF) plots are statistical tools used to analyze time series. ACF charts show the correlation between the values of a time series and their lagged values, while PACF charts show the correlation between the values of a time series and their lagged values, after the effect of previous lagged values has been removed.\nACF and PACF charts can be used to identify the structure of a time series, which can be helpful in choosing a suitable model for the time series. For example, if the ACF chart shows a repeating peak and valley pattern, this indicates that the time series is stationary, meaning that it has the same statistical properties over time. If the PACF chart shows a pattern of rapidly decreasing spikes, this indicates that the time series is invertible, meaning it can be reversed to get a stationary time series.\nThe importance of the ACF and PACF charts is that they can help analysts better understand the structure of a time series. This understanding can be helpful in choosing a suitable model for the time series, which can improve the ability to predict future values of the time series.\nTo analyze ACF and PACF charts:\n\nLook for patterns in charts. Common patterns include repeating peaks and valleys, sawtooth patterns, and plateau patterns.\nCompare ACF and PACF charts. The PACF chart generally has fewer spikes than the ACF chart.\nConsider the length of the time series. ACF and PACF charts for longer time series will have more spikes.\nUse a confidence interval. The ACF and PACF plots also show confidence intervals for the autocorrelation values. If an autocorrelation value is outside the confidence interval, it is likely to be significant.\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=24,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "docs/models/crostonsba.html#split-the-data-into-training-and-testing",
    "href": "docs/models/crostonsba.html#split-the-data-into-training-and-testing",
    "title": "CrostonSBA Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets 1. Data to train our Croston SBA Model. 2. Data to test our model\nFor the test data we will use the last 500 Hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2023-01-31 19:00:00'] \ntest = df[df.ds&gt;'2023-01-31 19:00:00']\n\n\ntrain.shape, test.shape\n\n((9500, 3), (500, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\",linewidth=2)\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\", linewidth=2, color=\"yellow\")\nplt.title(\"Store visit\");\nplt.xlabel(\"Hours\")\nplt.show()"
  },
  {
    "objectID": "docs/models/crostonsba.html#implementation-of-crostonsba-with-statsforecast",
    "href": "docs/models/crostonsba.html#implementation-of-crostonsba-with-statsforecast",
    "title": "CrostonSBA Model",
    "section": "Implementation of CrostonSBA with StatsForecast ",
    "text": "Implementation of CrostonSBA with StatsForecast \nTo also know more about the parameters of the functions of the CrostonSBA Model, they are listed below. For more information, visit the documentation.\nalias : str\n    Custom name of the model.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import CrostonSBA\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\n# We call the model that we are going to use\nmodels = [CrostonSBA()]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[CrostonSBA])\n\n\nLet‚Äôs see the results of our Croston SBA Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([22.426361], dtype=float32)}\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\nY_hat = sf.forecast(horizon)\nY_hat\n\n\n\n\n\n\n\n\nds\nCrostonSBA\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n22.426361\n\n\n1\n2023-02-21 17:00:00\n22.426361\n\n\n1\n2023-02-21 18:00:00\n22.426361\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n22.426361\n\n\n1\n2023-03-14 10:00:00\n22.426361\n\n\n1\n2023-03-14 11:00:00\n22.426361\n\n\n\n\n500 rows √ó 2 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nCrostonSBA\n\n\n\n\n0\n1\n2023-02-21 16:00:00\n22.426361\n\n\n1\n1\n2023-02-21 17:00:00\n22.426361\n\n\n2\n1\n2023-02-21 18:00:00\n22.426361\n\n\n...\n...\n...\n...\n\n\n497\n1\n2023-03-14 09:00:00\n22.426361\n\n\n498\n1\n2023-03-14 10:00:00\n22.426361\n\n\n499\n1\n2023-03-14 11:00:00\n22.426361\n\n\n\n\n500 rows √ó 3 columns\n\n\n\n\nY_hat1 = pd.concat([df,Y_hat])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nCrostonSBA\n\n\n\n\n0\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n1\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n497\n2023-03-14 09:00:00\nNaN\n1\n22.426361\n\n\n498\n2023-03-14 10:00:00\nNaN\n1\n22.426361\n\n\n499\n2023-03-14 11:00:00\nNaN\n1\n22.426361\n\n\n\n\n10500 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df, Y_hat1]).set_index('ds')\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[\"CrostonSBA\"].plot(ax=ax, linewidth=2, color=\"yellow\")\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel(\"Store visit (Hourly data)\", fontsize=20)\nax.set_xlabel('Hours', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \nforecast_df\n\n\n\n\n\n\n\n\nds\nCrostonSBA\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n22.426361\n\n\n1\n2023-02-21 17:00:00\n22.426361\n\n\n1\n2023-02-21 18:00:00\n22.426361\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n22.426361\n\n\n1\n2023-03-14 10:00:00\n22.426361\n\n\n1\n2023-03-14 11:00:00\n22.426361\n\n\n\n\n500 rows √ó 2 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nCrostonSBA\n\n\nds\n\n\n\n\n\n\n\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n22.426361\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n22.426361\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n22.426361\n\n\n\n\n10500 rows √ó 3 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds').tail(5000)\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nCrostonSBA\n\n\nds\n\n\n\n\n\n\n\n2022-08-18 04:00:00\n0.0\n1\nNaN\n\n\n2022-08-18 05:00:00\n80.0\n1\nNaN\n\n\n2022-08-18 06:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n22.426361\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n22.426361\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n22.426361\n\n\n\n\n5000 rows √ó 3 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series.\n\nplt.plot(df_plot['y'],label=\"Actual\", linewidth=2.5)\nplt.plot(df_plot['CrostonSBA'], label=\"Croston SBA\", color=\"yellow\") # '-', '--', '-.', ':',\n\nplt.title(\"Store visit (Hourly data)\");\nplt.xlabel(\"Hourly\")\nplt.ylabel(\"Store visit\")\nplt.legend()\nplt.show();\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/crostonsba.html#cross-validation",
    "href": "docs/models/crostonsba.html#cross-validation",
    "title": "CrostonSBA Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=50). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 500 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=50,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nCrostonSBA\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2023-01-23 12:00:00\n2023-01-23 11:00:00\n0.0\n22.473040\n\n\n1\n2023-01-23 13:00:00\n2023-01-23 11:00:00\n0.0\n22.473040\n\n\n1\n2023-01-23 14:00:00\n2023-01-23 11:00:00\n0.0\n22.473040\n\n\n...\n...\n...\n...\n...\n\n\n1\n2023-02-21 13:00:00\n2023-01-31 19:00:00\n60.0\n26.047497\n\n\n1\n2023-02-21 14:00:00\n2023-01-31 19:00:00\n20.0\n26.047497\n\n\n1\n2023-02-21 15:00:00\n2023-01-31 19:00:00\n20.0\n26.047497\n\n\n\n\n2500 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/crostonsba.html#model-evaluation",
    "href": "docs/models/crostonsba.html#model-evaluation",
    "title": "CrostonSBA Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Croston SBA Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"CrostonSBA\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  47.809525"
  },
  {
    "objectID": "docs/models/crostonsba.html#acknowledgements",
    "href": "docs/models/crostonsba.html#acknowledgements",
    "title": "CrostonSBA Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/imapa.html",
    "href": "docs/models/imapa.html",
    "title": "IMAPA Model",
    "section": "",
    "text": "Introduction\nIMAPA Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of IMAPA with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/imapa.html#table-of-contents",
    "href": "docs/models/imapa.html#table-of-contents",
    "title": "IMAPA Model",
    "section": "",
    "text": "Introduction\nIMAPA Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of IMAPA with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/imapa.html#introduction",
    "href": "docs/models/imapa.html#introduction",
    "title": "IMAPA Model",
    "section": "Introduction",
    "text": "Introduction\nIMAPA is an algorithm that uses multiple models to forecast the future values of an intermittent time series. The algorithm starts by adding the time series values at regular intervals. It then uses a forecast model to forecast the added values.\nIMAPA is a good choice for intermittent time series because it is robust to missing values and is computationally efficient. IMAPA is also easy to implement.\nIMAPA has been tested on a variety of intermittent time series and has been shown to be effective in forecasting future values."
  },
  {
    "objectID": "docs/models/imapa.html#imapa-method",
    "href": "docs/models/imapa.html#imapa-method",
    "title": "IMAPA Model",
    "section": "IMAPA Method ",
    "text": "IMAPA Method \nThe Intermittent Multiple Aggregation Prediction Algorithm (IMAPA) model is a time series model for forecasting future values for time series that are intermittent. The IMAPA model is based on the idea of aggregating the time series values at regular intervals and then using a forecast model to forecast the aggregated values. The aggregated values can be forecast using any forecast model. It uses the optimized SES to generate the forecasts at the new levels and then combines them using a simple average.\nThe IMAPA model can be defined mathematically as follows:\n\\[\\hat{y}_{t+1} = f(\\hat{y}_{t-\\tau}, \\hat{y}_{t-2\\tau}, ..., \\hat{ y}_{t-m\\tau})\\]\nwhere \\(\\hat{y}_{t+1}\\) is the forecast time value \\(t+1\\), \\(f\\) is the forecast model, \\(\\hat{y}_{t-\\tau} , \\hat{y}_{t-2\\tau}, ..., \\hat{y}_{t-m\\tau}\\) are the forecasts of the added values at times \\(t-\\tau, t-2 \\tau, ..., t-m\\tau\\), and \\(\\tau\\) is the time interval over which the time series values are aggregated.\nIMAPA is a good choice for intermittent time series because it is robust to missing values and is computationally efficient. IMAPA is also easy to implement.\nIMAPA has been tested on a variety of intermittent time series and has been shown to be effective in forecasting future values.\n\nIMAPA General Properties\n\nMultiple Aggregation: IMAPA uses multiple levels of aggregation to analyze and predict intermittent time series. This involves decomposing the original series into components of different time scales.\nIntermittency: IMAPA focuses on handling intermittent time series, which are those that exhibit irregular and non-stationary patterns with periods of activity and periods of inactivity.\nAdaptive Prediction: IMAPA uses an adaptive approach to adjust prediction models as new data is collected. This allows the algorithm to adapt to changes in the time series behavior over time.\nRobust to Missing Values: IMAPA can handle missing values in the data without sacrificing accuracy. This is important for intermittent time series, which often have missing values.\nComputationally Efficient: IMAPA is computationally efficient, meaning it can forecast future values quickly. This is important for large time series, which can take a long time to forecast using other methods.\nDecomposition Property: Time series can be decomposed into components such as trend, seasonality, and residual components."
  },
  {
    "objectID": "docs/models/imapa.html#loading-libraries-and-data",
    "href": "docs/models/imapa.html#loading-libraries-and-data",
    "title": "IMAPA Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nimport pandas as pd\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/intermittend_demand2\")\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nsales\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n\n\n1\n2022-01-01 01:00:00\n10\n\n\n2\n2022-01-01 02:00:00\n0\n\n\n3\n2022-01-01 03:00:00\n0\n\n\n4\n2022-01-01 04:00:00\n100\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2022-01-01 00:00:00\n0\n1\n\n\n1\n2022-01-01 01:00:00\n10\n1\n\n\n2\n2022-01-01 02:00:00\n0\n1\n\n\n3\n2022-01-01 03:00:00\n0\n1\n\n\n4\n2022-01-01 04:00:00\n100\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/imapa.html#explore-data-with-the-plot-method",
    "href": "docs/models/imapa.html#explore-data-with-the-plot-method",
    "title": "IMAPA Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nAutocorrelation plots\nAutocorrelation (ACF) and partial autocorrelation (PACF) plots are statistical tools used to analyze time series. ACF charts show the correlation between the values of a time series and their lagged values, while PACF charts show the correlation between the values of a time series and their lagged values, after the effect of previous lagged values has been removed.\nACF and PACF charts can be used to identify the structure of a time series, which can be helpful in choosing a suitable model for the time series. For example, if the ACF chart shows a repeating peak and valley pattern, this indicates that the time series is stationary, meaning that it has the same statistical properties over time. If the PACF chart shows a pattern of rapidly decreasing spikes, this indicates that the time series is invertible, meaning it can be reversed to get a stationary time series.\nThe importance of the ACF and PACF charts is that they can help analysts better understand the structure of a time series. This understanding can be helpful in choosing a suitable model for the time series, which can improve the ability to predict future values of the time series.\nTo analyze ACF and PACF charts:\n\nLook for patterns in charts. Common patterns include repeating peaks and valleys, sawtooth patterns, and plateau patterns.\nCompare ACF and PACF charts. The PACF chart generally has fewer spikes than the ACF chart.\nConsider the length of the time series. ACF and PACF charts for longer time series will have more spikes.\nUse a confidence interval. The ACF and PACF plots also show confidence intervals for the autocorrelation values. If an autocorrelation value is outside the confidence interval, it is likely to be significant.\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\n# Grafico\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\n\nplotSeasonalDecompose(\n    df[\"y\"],\n    model=\"additive\",\n    period=24,\n    title=\"Seasonal Decomposition\")"
  },
  {
    "objectID": "docs/models/imapa.html#split-the-data-into-training-and-testing",
    "href": "docs/models/imapa.html#split-the-data-into-training-and-testing",
    "title": "IMAPA Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our IMAPA Model.\nData to test our model\n\nFor the test data we will use the last 500 Hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2023-01-31 19:00:00'] \ntest = df[df.ds&gt;'2023-01-31 19:00:00']\n\n\ntrain.shape, test.shape\n\n((9500, 3), (500, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\",linewidth=2)\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\", linewidth=2, color=\"yellow\")\nplt.title(\"Store visit\");\nplt.xlabel(\"Hours\")\nplt.show()"
  },
  {
    "objectID": "docs/models/imapa.html#implementation-of-imapa-method-with-statsforecast",
    "href": "docs/models/imapa.html#implementation-of-imapa-method-with-statsforecast",
    "title": "IMAPA Model",
    "section": "Implementation of IMAPA Method with StatsForecast ",
    "text": "Implementation of IMAPA Method with StatsForecast \nTo also know more about the parameters of the functions of the IMAPA Model, they are listed below. For more information, visit the documentation.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import IMAPA\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\nmodels = [IMAPA()]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[IMAPA])\n\n\nLet‚Äôs see the results of our IMAPA Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([27.116224], dtype=float32)}\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon)\nY_hat\n\n\n\n\n\n\n\n\nds\nIMAPA\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n27.116224\n\n\n1\n2023-02-21 17:00:00\n27.116224\n\n\n1\n2023-02-21 18:00:00\n27.116224\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n27.116224\n\n\n1\n2023-03-14 10:00:00\n27.116224\n\n\n1\n2023-03-14 11:00:00\n27.116224\n\n\n\n\n500 rows √ó 2 columns\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nIMAPA\n\n\n\n\n0\n1\n2023-02-21 16:00:00\n27.116224\n\n\n1\n1\n2023-02-21 17:00:00\n27.116224\n\n\n2\n1\n2023-02-21 18:00:00\n27.116224\n\n\n...\n...\n...\n...\n\n\n497\n1\n2023-03-14 09:00:00\n27.116224\n\n\n498\n1\n2023-03-14 10:00:00\n27.116224\n\n\n499\n1\n2023-03-14 11:00:00\n27.116224\n\n\n\n\n500 rows √ó 3 columns\n\n\n\n\n# Concat the forecasts with the true values\nY_hat1 = pd.concat([df,Y_hat])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nIMAPA\n\n\n\n\n0\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n1\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n497\n2023-03-14 09:00:00\nNaN\n1\n27.116224\n\n\n498\n2023-03-14 10:00:00\nNaN\n1\n27.116224\n\n\n499\n2023-03-14 11:00:00\nNaN\n1\n27.116224\n\n\n\n\n10500 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df, Y_hat1]).set_index('ds')\nplot_df['y'].plot(ax=ax, linewidth=2)\nplot_df[\"IMAPA\"].plot(ax=ax, linewidth=2, color=\"yellow\")\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel(\"Store visit (Hourly data)\", fontsize=20)\nax.set_xlabel('Hours', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 500 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \nforecast_df\n\n\n\n\n\n\n\n\nds\nIMAPA\n\n\nunique_id\n\n\n\n\n\n\n1\n2023-02-21 16:00:00\n27.116224\n\n\n1\n2023-02-21 17:00:00\n27.116224\n\n\n1\n2023-02-21 18:00:00\n27.116224\n\n\n...\n...\n...\n\n\n1\n2023-03-14 09:00:00\n27.116224\n\n\n1\n2023-03-14 10:00:00\n27.116224\n\n\n1\n2023-03-14 11:00:00\n27.116224\n\n\n\n\n500 rows √ó 2 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\npd.concat([df, forecast_df]).set_index('ds')\n\n\n\n\n\n\n\n\ny\nunique_id\nIMAPA\n\n\nds\n\n\n\n\n\n\n\n2022-01-01 00:00:00\n0.0\n1\nNaN\n\n\n2022-01-01 01:00:00\n10.0\n1\nNaN\n\n\n2022-01-01 02:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n27.116224\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n27.116224\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n27.116224\n\n\n\n\n10500 rows √ó 3 columns\n\n\n\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds').tail(5000)\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nIMAPA\n\n\nds\n\n\n\n\n\n\n\n2022-08-18 04:00:00\n0.0\n1\nNaN\n\n\n2022-08-18 05:00:00\n80.0\n1\nNaN\n\n\n2022-08-18 06:00:00\n0.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-03-14 09:00:00\nNaN\nNaN\n27.116224\n\n\n2023-03-14 10:00:00\nNaN\nNaN\n27.116224\n\n\n2023-03-14 11:00:00\nNaN\nNaN\n27.116224\n\n\n\n\n5000 rows √ó 3 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series.\n\nplt.plot(df_plot['y'],label=\"Actual\", linewidth=2.5)\nplt.plot(df_plot['IMAPA'], label=\"IMAPA\", color=\"yellow\") # '-', '--', '-.', ':',\n\nplt.title(\"Store visit (Hourly data)\");\nplt.xlabel(\"Hourly\")\nplt.ylabel(\"Store visit\")\nplt.legend()\nplt.show();\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/imapa.html#cross-validation",
    "href": "docs/models/imapa.html#cross-validation",
    "title": "IMAPA Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=50). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 500 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=50,\n                                         n_windows=5)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nIMAPA\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2023-01-23 12:00:00\n2023-01-23 11:00:00\n0.0\n15.134251\n\n\n1\n2023-01-23 13:00:00\n2023-01-23 11:00:00\n0.0\n15.134251\n\n\n1\n2023-01-23 14:00:00\n2023-01-23 11:00:00\n0.0\n15.134251\n\n\n...\n...\n...\n...\n...\n\n\n1\n2023-02-21 13:00:00\n2023-01-31 19:00:00\n60.0\n28.579695\n\n\n1\n2023-02-21 14:00:00\n2023-01-31 19:00:00\n20.0\n28.579695\n\n\n1\n2023-02-21 15:00:00\n2023-01-31 19:00:00\n20.0\n28.579695\n\n\n\n\n2500 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/imapa.html#model-evaluation",
    "href": "docs/models/imapa.html#model-evaluation",
    "title": "IMAPA Model",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, IMAPA Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"IMAPA\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  48.62734"
  },
  {
    "objectID": "docs/models/imapa.html#acknowledgements",
    "href": "docs/models/imapa.html#acknowledgements",
    "title": "IMAPA Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/imapa.html#references",
    "href": "docs/models/imapa.html#references",
    "title": "IMAPA Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html",
    "href": "docs/models/seasonalexponentialsmoothing.html",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "",
    "text": "Introduction\nSeasonal Exponential Smoothing\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of SeasonalExponentialSmoothing with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#table-of-contents",
    "href": "docs/models/seasonalexponentialsmoothing.html#table-of-contents",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "",
    "text": "Introduction\nSeasonal Exponential Smoothing\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of SeasonalExponentialSmoothing with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#introduction",
    "href": "docs/models/seasonalexponentialsmoothing.html#introduction",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Introduction ",
    "text": "Introduction \nSimple Exponential Smoothing (SES) is a forecasting method that uses a weighted average of historical values to predict the next value. The weight is assigned to the most recent values, and the oldest values receive a lower weight. This is because SES assumes that more recent values are more relevant to predicting the future than older values.\nSES is implemented by a simple formula:\n\\[\\hat{y}_{T+1|T} = \\alpha y_T + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^2 y_{T-2}+ \\cdots, \\]\nThe smoothing factor controls the amount of weight that is assigned to the most recent values. A higher Œ± value means more weight will be assigned to newer values, while a lower Œ± value means more weight will be assigned to older values.\nSeasonality in time series refers to the regular, repeating pattern of variation in a time series over a specified period of time.\nSeasonality can be a challenge to deal with in time series analysis, as it can obscure the underlying trend in the data.\nSeasonality is an important factor to consider when analyzing time series data. By understanding the seasonal patterns in the data, it is possible to make more accurate forecasts and better decisions."
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#seasonal-exponential-smoothing-model",
    "href": "docs/models/seasonalexponentialsmoothing.html#seasonal-exponential-smoothing-model",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Seasonal Exponential Smoothing Model ",
    "text": "Seasonal Exponential Smoothing Model \nThe simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern.\nUsing the na√Øve method, all forecasts for the future are equal to the last observed value of the series, \\[\\hat{y}_{T+h|T} = y_{T},\\]\nfor $h=1,2,$. Hence, the na√Øve method assumes that the most recent observation is the only important one, and all previous observations provide no information for the future. This can be thought of as a weighted average where all of the weight is given to the last observation.\nUsing the average method, all future forecasts are equal to a simple average of the observed data, \\[\\hat{y}_{T+h|T} = \\frac1T \\sum_{t=1}^T y_t, \\]\nfor $h=1,2,$ Hence, the average method assumes that all observations are of equal importance, and gives them equal weights when generating forecasts.\nWe often want something between these two extremes. For example, it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past ‚Äî the smallest weights are associated with the oldest observations:\n\\[\\begin{equation}\n  \\hat{y}_{T+1|T} = \\alpha y_T + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^2 y_{T-2}+ \\cdots,   \\tag{1}\n\\end{equation}\\]\nwhere \\(0 \\le \\alpha \\le 1\\) is the smoothing parameter. The one-step-ahead forecast for time \\(T+1\\) is a weighted average of all of the observations in the series \\(y_1,\\dots,y_T\\). The rate at which the weights decrease is controlled by the parameter \\(\\alpha\\).\nFor any \\(\\alpha\\) between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name ‚Äúexponential smoothing‚Äù. If \\(\\alpha\\) is small (i.e., close to 0), more weight is given to observations from the more distant past. If \\(\\alpha\\) is large (i.e., close to 1), more weight is given to the more recent observations. For the extreme case where \\(\\alpha=1\\), \\(\\hat{y}_{T+1|T}=y_T\\) and the forecasts are equal to the na√Øve forecasts.\n\nHow do you know the value of the seasonal parameters?\nTo determine the value of the seasonal parameter s in the Seasonally Adjusted Simple Exponential Smoothing (SES Seasonally Adjusted) model, different methods can be used, depending on the nature of the data and the objective of the analysis.\nHere are some common methods to determine the value of the seasonal parameter \\(s\\):\n\nVisual Analysis: A visual analysis of the time series data can be performed to identify any seasonal patterns. If a clear seasonal pattern is observed in the data, the length of the seasonal period can be used as the value of \\(s\\).\nStatistical methods: Statistical techniques, such as autocorrelation, can be used to identify seasonal patterns in the data. The value of \\(s\\) can be the number of periods in which a significant peak in the autocorrelation function is observed.\nFrequency Analysis: A frequency analysis of the data can be performed to identify seasonal patterns. The value of \\(s\\) can be the number of periods in which a significant peak in the frequency spectrum is observed. see\nTrial and error: You can try different values of \\(s\\) and select the value that results in the best fit of the model to the data.\n\nIt is important to note that the choice of the value of \\(s\\) can significantly affect the accuracy of the seasonally adjusted SES model predictions. Therefore, it is recommended to test different values of \\(s\\) and evaluate the performance of the model using appropriate evaluation measures before selecting the final value of \\(s\\).\n\n\nHow can we validate the simple exponential smoothing model with seasonal adjustment?\nTo validate the Seasonally Adjusted Simple Exponential Smoothing (SES Seasonally Adjusted) model, different theorems and evaluation measures can be used, depending on the objective of the analysis and the nature of the data.\nHere are some common theorems used to validate the seasonally adjusted SES model:\n\nGauss-Markov Theorem: This theorem states that, if certain conditions are met, the least squares estimator is the best linear unbiased estimator. In the case of the seasonally adjusted SES, the model parameters are estimated using least squares, so the Gauss-Markov theorem can be used to assess the quality of model fit.\nUnit Root Theorem: This theorem is used to determine if a time series is stationary or not. If a time series is non-stationary, the seasonally adjusted SES model is not appropriate, since it assumes that the time series is stationary. Therefore, the unit root theorem is used to assess the stationarity of the time series and determine whether the seasonally adjusted SES model is appropriate.\nLjung-Box Theorem: This theorem is used to assess the goodness of fit of the model and to determine if the model residuals are white noise. If the residuals are white noise, the model fits the data well and the model predictions are accurate. The Ljung-Box theorem is used to test whether the model residuals are independent and uncorrelated.\n\nIn addition to these theorems, various evaluation measures, such as root mean square error (MSE), mean absolute error (MAE), and coefficient of determination (R¬≤), can be used to evaluate the performance of the seasonally adjusted SES model and compare it with other forecast models."
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#loading-libraries-and-data",
    "href": "docs/models/seasonalexponentialsmoothing.html#loading-libraries-and-data",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\nimport pandas as pd\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/ads.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nTime\nAds\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n\n\n1\n2017-09-13T01:00:00\n79885\n\n\n2\n2017-09-13T02:00:00\n89325\n\n\n3\n2017-09-13T03:00:00\n101930\n\n\n4\n2017-09-13T04:00:00\n121630\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n1\n\n\n1\n2017-09-13T01:00:00\n79885\n1\n\n\n2\n2017-09-13T02:00:00\n89325\n1\n\n\n3\n2017-09-13T03:00:00\n101930\n1\n\n\n4\n2017-09-13T04:00:00\n121630\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#explore-data-with-the-plot-method",
    "href": "docs/models/seasonalexponentialsmoothing.html#explore-data-with-the-plot-method",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nThe Augmented Dickey-Fuller Test\nAn Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\nNull Hypothesis: Time Series is non-stationary. It gives a time-dependent trend. Alternate Hypothesis: Time Series is stationary. In another term, the series doesn‚Äôt depend on time.\nADF or t Statistic &lt; critical values: Reject the null hypothesis, time series is stationary. ADF or t Statistic &gt; critical values: Failed to reject the null hypothesis, time series is non-stationary.\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],'Ads')\n\nDickey-Fuller test results for columns: Ads\nTest Statistic         -7.089634e+00\np-value                 4.444804e-10\nNo Lags Used            9.000000e+00\n                            ...     \nCritical Value (1%)    -3.462499e+00\nCritical Value (5%)    -2.875675e+00\nCritical Value (10%)   -2.574304e+00\nLength: 7, dtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\n\n\nAutocorrelation plots\nThe important characteristics of Autocorrelation (ACF) and Partial Autocorrelation (PACF) are as follows:\nAutocorrelation (ACF): 1. Identify patterns of temporal dependence: The ACF shows the correlation between an observation and its lagged values at different time intervals. Helps identify patterns of temporal dependency in a time series, such as the presence of trends or seasonality.\n\nIndicates the ‚Äúmemory‚Äù of the series: The ACF allows us to determine how much past observations influence future ones. If the ACF shows significant autocorrelations in several lags, it indicates that the series has a long-term memory and that past observations are relevant to predict future ones.\nHelps identify MA (moving average) models: The shape of the ACF can reveal the presence of moving average components in the time series. Lags where the ACF shows a significant correlation may indicate the order of an MA model.\n\nPartial Autocorrelation (PACF): 1. Identify direct dependence: Unlike the ACF, the PACF eliminates the indirect effects of intermediate lags and measures the direct correlation between an observation and its lagged values. It helps to identify the direct dependence between an observation and its lag values, without the influence of intermediate lags.\n\nHelps to identify AR (autoregressive) models: The shape of the PACF can reveal the presence of autoregressive components in the time series. Lags in which the PACF shows a significant correlation may indicate the order of an AR model.\nUsed in conjunction with the ACF: The PACF is used in conjunction with the ACF to determine the order of an AR or MA model. By analyzing both the ACF and the PACF, significant lags can be identified and a model suitable for time series analysis and forecasting can be built.\n\nIn summary, the ACF and the PACF are complementary tools in time series analysis that provide information on time dependence and help identify the appropriate components to build forecast models.\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\n\nAdditive\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=12)\na.plot();\n\n\n\n\n\n\nMultiplicative\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=12)\na.plot();"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#split-the-data-into-training-and-testing",
    "href": "docs/models/seasonalexponentialsmoothing.html#split-the-data-into-training-and-testing",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our Seasonal Exponential Smoothing Model.\nData to test our model\n\nFor the test data we will use the last 30 hourly to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2017-09-20 17:00:00'] \ntest = df[df.ds&gt;'2017-09-20 17:00:00']\n\n\ntrain.shape, test.shape\n\n((186, 3), (30, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"Ads watched (hourly data)\");\nplt.show()"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#implementation-of-seasonalexponentialsmoothing-with-statsforecast",
    "href": "docs/models/seasonalexponentialsmoothing.html#implementation-of-seasonalexponentialsmoothing-with-statsforecast",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Implementation of SeasonalExponentialSmoothing with StatsForecast ",
    "text": "Implementation of SeasonalExponentialSmoothing with StatsForecast \nTo also know more about the parameters of the functions of the SeasonalExponentialSmoothing Model, they are listed below. For more information, visit the documentation.\nalpha : float\n    Smoothing parameter.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import SeasonalExponentialSmoothing\n\n\n\nInstantiating Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\nmodels = [SeasonalExponentialSmoothing(alpha=0.8, season_length=season_length)]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See pandas‚Äô available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[SeasonalES])\n\n\nLet‚Äôs see the results of our Seasonal Exponential Smoothing Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([ 72221.8  ,  73250.41 ,  81213.68 ,  95889.06 , 122141.46 ,\n        114311.9  , 105907.24 ,  97934.02 ,  98570.34 , 106042.58 ,\n        115734.17 , 133876.25 , 145319.06 , 141776.67 , 142775.06 ,\n        145288.16 , 150219.11 , 149963.72 , 154389.11 , 154027.47 ,\n        123923.64 , 102927.42 ,  93966.52 ,  79575.586], dtype=float32),\n 'fitted': array([       nan,        nan,        nan,        nan,        nan,\n               nan,        nan,        nan,        nan,        nan,\n               nan,        nan,        nan,        nan,        nan,\n               nan,        nan,        nan,        nan,        nan,\n               nan,        nan,        nan,        nan,  80115.   ,\n         79885.   ,  89325.   , 101930.   , 121630.   , 116475.   ,\n        106495.   , 102795.   , 108055.   , 116125.   , 131030.   ,\n        149020.   , 157590.   , 150715.   , 149295.   , 150100.   ,\n        144780.   , 150690.   , 163840.   , 166235.   , 139520.   ,\n        105895.   ,  96780.   ,  82520.   ,  80123.   ,  76245.   ,\n         85949.   , 102050.   , 124434.   , 117719.   , 108679.   ,\n        102539.   , 103403.   , 115897.   , 130638.   , 145264.   ,\n        150694.   , 149463.   , 148291.   , 149068.   , 148820.   ,\n        150594.   , 152320.   , 153663.   , 131208.   , 104231.   ,\n         93096.   ,  82716.   ,  77076.6  ,  75353.   ,  83301.8  ,\n         91446.   , 119630.8  , 115695.8  , 110487.8  ,  99595.8  ,\n        104028.6  , 110111.4  , 127439.6  , 141400.8  , 152114.8  ,\n        146912.6  , 148074.2  , 148001.6  , 146364.   , 149546.8  ,\n        158244.   , 159600.6  , 134657.6  , 111202.2  ,  98779.2  ,\n         86635.2  ,  85683.32 ,  86146.6  ,  90540.36 , 101861.2  ,\n        116678.16 , 126299.16 , 135205.56 , 135471.16 , 135405.72 ,\n        128574.28 , 130479.92 , 142264.16 , 156322.95 , 151382.52 ,\n        152602.84 , 150556.31 , 149788.8  , 147857.36 , 153668.8  ,\n        149420.12 , 127127.52 , 116580.44 ,  97115.84 ,  92215.04 ,\n         88384.664,  88705.32 ,  90568.07 ,  99004.24 , 113391.63 ,\n        128835.83 , 140165.11 , 149142.23 , 149145.14 , 138650.86 ,\n        144135.98 , 157340.83 , 164332.6  , 163700.5  , 161032.56 ,\n        155955.27 , 157201.77 , 157587.47 , 165409.77 , 165804.03 ,\n        139593.5  , 113680.086,  97299.17 ,  83783.01 ,  81284.93 ,\n         80421.06 ,  88549.62 ,  99632.85 , 121702.33 , 114827.164,\n        107585.02 , 107952.445, 107953.03 , 109782.17 , 124771.195,\n        140072.17 , 144962.52 , 146124.1  , 145982.52 , 147479.05 ,\n        147708.36 , 151845.5  , 162297.95 , 155892.81 , 135694.7  ,\n        108388.016,  95495.836,  80368.6  ,  78924.984,  75820.21 ,\n         83301.92 ,  98286.57 , 119816.47 , 113457.43 , 100621.01 ,\n         96790.49 ,  96518.61 , 105304.44 , 120754.24 , 136806.44 ,\n        146156.5  , 140556.81 , 146976.5  , 145443.81 , 150637.67 ,\n        155233.1  , 161567.6  , 163186.56 , 134410.94 , 106145.6  ,\n         93383.164,  79489.72 ,  79769.   ,  77652.04 ,  85288.38 ,\n         99665.31 , 123067.3  , 115759.484, 103556.2  , 100510.1  ,\n         97411.72 , 107672.89 , 121150.85 , 140041.28 , 140075.3  ,\n        140903.36 , 142615.3  , 142360.77 , 142615.53 , 142658.62 ,\n        149285.52 , 146577.31 , 126038.19 , 102317.12 ,  89212.63 ,\n         76737.945], dtype=float32)}\n\n\nLet us now visualize the fitted values of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nfitted=pd.DataFrame(result.get(\"fitted\"), columns=[\"fitted\"])\nfitted[\"ds\"]=df[\"ds\"]\nfitted\n\n\n\n\n\n\n\n\nfitted\nds\n\n\n\n\n0\nNaN\n2017-09-13 00:00:00\n\n\n1\nNaN\n2017-09-13 01:00:00\n\n\n2\nNaN\n2017-09-13 02:00:00\n\n\n...\n...\n...\n\n\n213\n102317.117188\n2017-09-21 21:00:00\n\n\n214\n89212.632812\n2017-09-21 22:00:00\n\n\n215\n76737.945312\n2017-09-21 23:00:00\n\n\n\n\n216 rows √ó 2 columns\n\n\n\n\nsns.lineplot(df, x=\"ds\", y=\"y\", label=\"Actual\", linewidth=2)\nsns.lineplot(fitted,x=\"ds\", y=\"fitted\", label=\"Fitted\", linestyle=\"--\" )\n\nplt.title(\"Ads watched (hourly data)\");\nplt.show()\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\nY_hat = sf.forecast(horizon, fitted=True)\nY_hat\n\n\n\n\n\n\n\n\nds\nSeasonalES\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n72221.796875\n\n\n1\n2017-09-22 01:00:00\n73250.406250\n\n\n1\n2017-09-22 02:00:00\n81213.679688\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n95889.062500\n\n\n1\n2017-09-23 04:00:00\n122141.460938\n\n\n1\n2017-09-23 05:00:00\n114311.898438\n\n\n\n\n30 rows √ó 2 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nSeasonalES\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2017-09-13 00:00:00\n80115.0\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\nNaN\n\n\n1\n2017-09-13 02:00:00\n89325.0\nNaN\n\n\n1\n2017-09-13 03:00:00\n101930.0\nNaN\n\n\n1\n2017-09-13 04:00:00\n121630.0\nNaN\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nSeasonalES\n\n\n\n\n0\n1\n2017-09-22 00:00:00\n72221.796875\n\n\n1\n1\n2017-09-22 01:00:00\n73250.406250\n\n\n2\n1\n2017-09-22 02:00:00\n81213.679688\n\n\n...\n...\n...\n...\n\n\n27\n1\n2017-09-23 03:00:00\n95889.062500\n\n\n28\n1\n2017-09-23 04:00:00\n122141.460938\n\n\n29\n1\n2017-09-23 05:00:00\n114311.898438\n\n\n\n\n30 rows √ó 3 columns\n\n\n\n\nY_hat1= pd.concat([df,Y_hat])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nSeasonalES\n\n\n\n\n0\n2017-09-13 00:00:00\n80115.0\n1\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\n1\nNaN\n\n\n2\n2017-09-13 02:00:00\n89325.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n27\n2017-09-23 03:00:00\nNaN\n1\n95889.062500\n\n\n28\n2017-09-23 04:00:00\nNaN\n1\n122141.460938\n\n\n29\n2017-09-23 05:00:00\nNaN\n1\n114311.898438\n\n\n\n\n246 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df,Y_hat]).set_index('ds')\nplot_df[['y', \"SeasonalES\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel(\"Ads watched (hourly data)\", fontsize=20)\nax.set_xlabel('Hourly', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hourly ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \nforecast_df\n\n\n\n\n\n\n\n\nds\nSeasonalES\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n72221.796875\n\n\n1\n2017-09-22 01:00:00\n73250.406250\n\n\n1\n2017-09-22 02:00:00\n81213.679688\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n95889.062500\n\n\n1\n2017-09-23 04:00:00\n122141.460938\n\n\n1\n2017-09-23 05:00:00\n114311.898438\n\n\n\n\n30 rows √ó 2 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds')\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nSeasonalES\n\n\nds\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n95889.062500\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n122141.460938\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n114311.898438\n\n\n\n\n246 rows √ó 3 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series.\n\nsns.lineplot(df_plot,x=\"ds\", y=\"y\", label=\"Actual\")\nsns.lineplot(df_plot, x=\"ds\", y=\"SeasonalES\", label=\"SeasonalES\")\nplt.title(\"Ads watched (hourly data)\")\nplt.show()"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#cross-validation",
    "href": "docs/models/seasonalexponentialsmoothing.html#cross-validation",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=5), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 30 hourly ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=12,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nSeasonalES\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-19 18:00:00\n2017-09-19 17:00:00\n161385.0\n162297.953125\n\n\n1\n2017-09-19 19:00:00\n2017-09-19 17:00:00\n165010.0\n155892.812500\n\n\n1\n2017-09-19 20:00:00\n2017-09-19 17:00:00\n134090.0\n135694.703125\n\n\n...\n...\n...\n...\n...\n\n\n1\n2017-09-21 21:00:00\n2017-09-20 17:00:00\n103080.0\n106145.601562\n\n\n1\n2017-09-21 22:00:00\n2017-09-20 17:00:00\n95155.0\n93383.164062\n\n\n1\n2017-09-21 23:00:00\n2017-09-20 17:00:00\n80285.0\n79489.718750\n\n\n\n\n90 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#evaluate-model",
    "href": "docs/models/seasonalexponentialsmoothing.html#evaluate-model",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Evaluate Model ",
    "text": "Evaluate Model \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Seasonal Exponential Smoothing Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"SeasonalES\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  7127.6504"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#acknowledgements",
    "href": "docs/models/seasonalexponentialsmoothing.html#acknowledgements",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothing.html#references",
    "href": "docs/models/seasonalexponentialsmoothing.html#references",
    "title": "Seasonal Exponential Smoothing Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "",
    "text": "Introduction\nSeasonal Exponential Smoothing Optimized Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of SeasonalExponentialSmoothingOptimized with StatsForecast\nCross-validation\nModel evaluation\nReferences\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#table-of-contents",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#table-of-contents",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "",
    "text": "Introduction\nSeasonal Exponential Smoothing Optimized Model\nLoading libraries and data\nExplore data with the plot method\nSplit the data into training and testing\nImplementation of SeasonalExponentialSmoothingOptimized with StatsForecast\nCross-validation\nModel evaluation\nReferences"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#introduction",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#introduction",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Introduction ",
    "text": "Introduction \nThe Seasonal Exponential Smoothing Optimized (SESO) model is a forecasting technique used to predict future values of a time series that exhibits seasonal patterns. It is a variant of the exponential smoothing method, which uses a combination of past and predicted values to generate a prediction.\nThe SESO algorithm uses an optimization approach to find the optimal values of the seasonal exponential smoothing parameters. These parameters include the smoothing coefficients for the levels, trends, and seasonal components of the time series.\nThe SESO model is particularly useful for forecasting time series with pronounced seasonal patterns, such as seasonal product sales or seasonal temperatures, and many other areas. By using SESO, accurate and useful forecasts can be generated for business planning and decision making."
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#seasonal-exponential-smoothing-model",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#seasonal-exponential-smoothing-model",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Seasonal Exponential Smoothing Model ",
    "text": "Seasonal Exponential Smoothing Model \nThe SESO model is based on the exponential smoothing method, which uses a combination of past and predicted values to generate a prediction. The mathematical formula for the SESO model is as follows:\n\\[\\hat{y}{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}{t-1,s}\\]\nWhere: - \\(\\hat{y}{t+1,s}\\) is the forecast for the next period of the season \\(s\\). - \\(\\alpha\\) is the smoothing parameter that is optimized by minimizing the squared error. - \\(y_t\\) is the current observation of station \\(s\\) in period \\(t\\). - \\(\\hat{y}{t-1,s}\\) is the forecast for the previous period of the season \\(s\\).\nThe equation indicates that the forecast value for the next season period \\(s\\) is calculated as a weighted combination of the current observation and the previous forecast for the same station. The smoothing parameter \\(\\alpha\\) controls the relative influence of these two terms on the final prediction. A high value of Œ± gives more weight to the current observation and less weight to the previous forecast, making the model more sensitive to recent changes in the time series. A low value of \\(\\alpha\\), on the other hand, gives more weight to the previous forecast and less weight to the current observation, making the model more stable and smooth.\nThe optimal value of the smoothing parameter \\(\\alpha\\) is determined by minimizing the squared error between the forecasts generated by the model and the actual values of the time series.\n\nModel selection\nModel selection in the context of the SESO model refers to the process of choosing the optimal values of the smoothing parameters and the seasonal component for the model. The optimal values of these parameters are the ones that result in the best forecast performance for the given data set.\nA great advantage of the ETS statistical framework is that information criteria can be used for model selection. The \\(AIC, AIC_c\\) and \\(BIC\\), that also can be used here to determine which of the ETS models is most appropriate for a given time series.\nFor ETS models, Akaike‚Äôs Information Criterion (AIC) is defined as \\[\\text{AIC} = -2\\log(L) + 2k,\\]\nwhere \\(L\\) is the likelihood of the model and \\(k\\) is the total number of parameters and initial states that have been estimated (including the residual variance).\nThe AIC corrected for small sample bias (\\(AIC_c\\)) is defined as \\[\\text{AIC}_{\\text{c}} = \\text{AIC} + \\frac{2k(k+1)}{T-k-1},\\]\nand the Bayesian Information Criterion (BIC) is \\[\\text{BIC} = \\text{AIC} + k[\\log(T)-2].\\]\nThese criteria balance the goodness of fit with the complexity of the model and provide a way to choose the model that maximizes the likelihood of the data while minimizing the number of parameters.\nIn addition to these techniques, expert judgment and domain knowledge can also be used to select the optimal SESO model. This involves considering the underlying dynamics of the time series, the patterns of seasonality, and any other relevant factors that may influence the choice of the model.\nOverall, the process of model selection for the SESO model involves a combination of statistical techniques, information criteria, and expert judgment to identify the optimal values of the smoothing parameters and the seasonal component that result in the best forecast performance for the given data set."
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#loading-libraries-and-data",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#loading-libraries-and-data",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Loading libraries and data ",
    "text": "Loading libraries and data \n\n\n\n\n\n\nTip\n\n\n\nStatsforecast will be needed. To install, see instructions.\n\n\nNext, we import plotting libraries and configure the plotting style.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\n\n\nRead Data\n\nimport pandas as pd\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/ads.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nTime\nAds\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n\n\n1\n2017-09-13T01:00:00\n79885\n\n\n2\n2017-09-13T02:00:00\n89325\n\n\n3\n2017-09-13T03:00:00\n101930\n\n\n4\n2017-09-13T04:00:00\n121630\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\n\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\nunique_id\n\n\n\n\n0\n2017-09-13T00:00:00\n80115\n1\n\n\n1\n2017-09-13T01:00:00\n79885\n1\n\n\n2\n2017-09-13T02:00:00\n89325\n1\n\n\n3\n2017-09-13T03:00:00\n101930\n1\n\n\n4\n2017-09-13T04:00:00\n121630\n1\n\n\n\n\n\n\n\n\nprint(df.dtypes)\n\nds           object\ny             int64\nunique_id    object\ndtype: object\n\n\nWe can see that our time variable (ds) is in an object format, we need to convert to a date format\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#explore-data-with-the-plot-method",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#explore-data-with-the-plot-method",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Explore Data with the plot method ",
    "text": "Explore Data with the plot method \nPlot some series using the plot method from the StatsForecast class. This method prints a random series from the dataset and is useful for basic EDA.\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n\n\n\n\nThe Augmented Dickey-Fuller Test\nAn Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\nNull Hypothesis: Time Series is non-stationary. It gives a time-dependent trend. Alternate Hypothesis: Time Series is stationary. In another term, the series doesn‚Äôt depend on time.\nADF or t Statistic &lt; critical values: Reject the null hypothesis, time series is stationary. ADF or t Statistic &gt; critical values: Failed to reject the null hypothesis, time series is non-stationary.\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],'Ads')\n\nDickey-Fuller test results for columns: Ads\nTest Statistic         -7.089634e+00\np-value                 4.444804e-10\nNo Lags Used            9.000000e+00\n                            ...     \nCritical Value (1%)    -3.462499e+00\nCritical Value (5%)    -2.875675e+00\nCritical Value (10%)   -2.574304e+00\nLength: 7, dtype: float64\nConclusion:====&gt;\nReject the null hypothesis\nThe data is stationary\n\n\n\n\nAutocorrelation plots\nThe important characteristics of Autocorrelation (ACF) and Partial Autocorrelation (PACF) are as follows:\nAutocorrelation (ACF): 1. Identify patterns of temporal dependence: The ACF shows the correlation between an observation and its lagged values at different time intervals. Helps identify patterns of temporal dependency in a time series, such as the presence of trends or seasonality.\n\nIndicates the ‚Äúmemory‚Äù of the series: The ACF allows us to determine how much past observations influence future ones. If the ACF shows significant autocorrelations in several lags, it indicates that the series has a long-term memory and that past observations are relevant to predict future ones.\nHelps identify MA (moving average) models: The shape of the ACF can reveal the presence of moving average components in the time series. Lags where the ACF shows a significant correlation may indicate the order of an MA model.\n\nPartial Autocorrelation (PACF): 1. Identify direct dependence: Unlike the ACF, the PACF eliminates the indirect effects of intermediate lags and measures the direct correlation between an observation and its lagged values. It helps to identify the direct dependence between an observation and its lag values, without the influence of intermediate lags.\n\nHelps to identify AR (autoregressive) models: The shape of the PACF can reveal the presence of autoregressive components in the time series. Lags in which the PACF shows a significant correlation may indicate the order of an AR model.\nUsed in conjunction with the ACF: The PACF is used in conjunction with the ACF to determine the order of an AR or MA model. By analyzing both the ACF and the PACF, significant lags can be identified and a model suitable for time series analysis and forecasting can be built.\n\nIn summary, the ACF and the PACF are complementary tools in time series analysis that provide information on time dependence and help identify the appropriate components to build forecast models.\n\nfig, axs = plt.subplots(nrows=1, ncols=2)\n\nplot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation\");\n\n# Grafico\nplot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelation')\n\n#plt.savefig(\"Gr√°fico de Densidad y qq\")\nplt.show();\n\n\n\n\n\n\nDecomposition of the time series\nHow to decompose a time series and why?\nIn time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n\nLevel: This is the primary value that averages over time.\nTrend: The trend is the value that causes increasing or decreasing patterns in a time series.\nSeasonality: This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\nResidual/Noise: These are the random variations in the time series.\n\nCombining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\nIf seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\nThe combination of the components in time series can be of two types: * Additive * Multiplicative\n\n\nAdditive time series\nIf the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by: \\[y(t) = level + Trend + seasonality + noise\\]\n\n\nMultiplicative time series\nIf the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n\\[y(t) = Level * Trend * seasonality * Noise\\]\n\n\nAdditive\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"additive\", period=12)\na.plot();\n\n\n\n\n\n\nMultiplicative\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose \na = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=12)\na.plot();"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#split-the-data-into-training-and-testing",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#split-the-data-into-training-and-testing",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Split the data into training and testing",
    "text": "Split the data into training and testing\nLet‚Äôs divide our data into sets\n\nData to train our Seasonal Exponential Smoothing Optimized Model.\nData to test our model\n\nFor the test data we will use the last 30 hours to test and evaluate the performance of our model.\n\ntrain = df[df.ds&lt;='2017-09-20 17:00:00'] \ntest = df[df.ds&gt;'2017-09-20 17:00:00']\n\n\ntrain.shape, test.shape\n\n((186, 3), (30, 3))\n\n\nNow let‚Äôs plot the training data and the test data.\n\nsns.lineplot(train,x=\"ds\", y=\"y\", label=\"Train\", linestyle=\"--\")\nsns.lineplot(test, x=\"ds\", y=\"y\", label=\"Test\")\nplt.title(\"Ads watched (hourly data)\");\nplt.show()"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#implementation-of-seasonalexponentialsmoothingoptimized-with-statsforecast",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#implementation-of-seasonalexponentialsmoothingoptimized-with-statsforecast",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Implementation of SeasonalExponentialSmoothingOptimized with StatsForecast ",
    "text": "Implementation of SeasonalExponentialSmoothingOptimized with StatsForecast \nTo also know more about the parameters of the functions of the SeasonalExponentialSmoothingOptimized Model, they are listed below. For more information, visit the documentation.\nalpha : float\n    Smoothing parameter.\nseason_length : int\n    Number of observations per unit of time. Ex: 24 Hourly data.\nalias : str\n    Custom name of the model.\nprediction_intervals : Optional[ConformalIntervals]\n    Information to compute conformal prediction intervals.\n    By default, the model will compute the native prediction\n    intervals.\n\nLoad libraries\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import SeasonalExponentialSmoothingOptimized\n\n\n\nBuilding Model\nImport and instantiate the models. Setting the argument is sometimes tricky. This article on Seasonal periods by the master, Rob Hyndmann, can be useful for season_length.\n\nseason_length = 24 # Hourly data \nhorizon = len(test) # number of predictions\n\nmodels = [SeasonalExponentialSmoothingOptimized(season_length=season_length)]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\nmodels: a list of models. Select the models you want from models and import them.\n\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(df=df,\n                   models=models,\n                   freq='H', \n                   n_jobs=-1)\n\n\n\nFit the Model\n\nsf.fit()\n\nStatsForecast(models=[SeasESOpt])\n\n\nLet‚Äôs see the results of our Seasonal Exponential Smoothing Optimized Model. We can observe it with the following instruction:\n\nresult=sf.fitted_[0,0].model_\nresult\n\n{'mean': array([ 80116.2  ,  79812.98 ,  84372.74 ,  99043.75 , 121514.94 ,\n        116611.32 , 108388.39 , 103424.516, 108289.664, 116023.664,\n        121505.336, 139466.77 , 146281.33 , 149037.22 , 149215.86 ,\n        146367.08 , 148183.7  , 150684.8  , 157380.17 , 156142.53 ,\n        127853.48 , 103063.23 ,  94190.15 ,  82520.19 ], dtype=float32),\n 'fitted': array([       nan,        nan,        nan,        nan,        nan,\n               nan,        nan,        nan,        nan,        nan,\n               nan,        nan,        nan,        nan,        nan,\n               nan,        nan,        nan,        nan,        nan,\n               nan,        nan,        nan,        nan,  80115.   ,\n         79885.   ,  89325.   , 101930.   , 121630.   , 116475.   ,\n        106495.   , 102795.   , 108055.   , 116125.   , 131030.   ,\n        149020.   , 157590.   , 150715.   , 149295.   , 150100.   ,\n        144780.   , 150690.   , 163840.   , 166235.   , 139520.   ,\n        105895.   ,  96780.   ,  82520.   ,  80115.1  ,  79839.5  ,\n         87987.92 , 101954.1  , 121665.05 , 116490.55 , 106601.53 ,\n        102791.8  , 107996.85 , 116122.15 , 130866.836, 147694.23 ,\n        154525.55 , 150612.58 , 149282.45 , 149723.33 , 145698.03 ,\n        150688.8  , 160717.08 , 162397.19 , 135501.27 , 103835.8  ,\n         95715.3  ,  82522.45 ,  80077.1  ,  79792.41 ,  86293.46 ,\n         99839.95 , 121632.7  , 116477.55 , 106770.836, 102752.484,\n        107958.734, 116047.58 , 129459.34 , 145644.39 , 153794.8  ,\n        150328.69 , 149269.83 , 149142.73 , 145707.48 , 150674.77 ,\n        160501.92 , 162076.73 , 135508.52 , 112853.91 ,  96752.19 ,\n         82573.375,  80154.68 ,  79882.93 ,  88212.44 , 100583.016,\n        121575.77 , 116602.266, 108121.59 , 103169.36 , 108311.64 ,\n        116219.   , 130052.28 , 144750.84 , 155067.58 , 150470.8  ,\n        149314.48 , 149742.   , 146605.06 , 150642.36 , 158771.97 ,\n        158364.27 , 131538.7  , 117874.29 ,  96740.12 ,  82683.74 ,\n         80243.734,  79977.555,  88961.   , 100214.62 , 121485.71 ,\n        116730.945, 109420.42 , 103663.266, 108754.33 , 116468.516,\n        135878.83 , 149370.3  , 159073.2  , 151538.19 , 149452.73 ,\n        151950.38 , 148868.33 , 150736.14 , 160848.06 , 161181.45 ,\n        135859.64 , 113004.195,  96879.98 ,  82673.66 ,  80236.39 ,\n         79961.27 ,  88670.77 , 100146.4  , 121508.66 , 116676.89 ,\n        109030.95 , 103603.18 , 108643.336, 116329.48 , 130568.05 ,\n        145525.64 , 152335.23 , 150896.27 , 149380.4  , 150026.   ,\n        148226.   , 150732.88 , 160993.8  , 159284.78 , 135418.84 ,\n        107124.39 ,  96455.72 ,  82642.07 ,  80217.38 ,  79908.37 ,\n         86554.016,  99793.52 , 121487.02 , 116641.266, 108634.83 ,\n        103507.15 , 108493.5  , 116208.03 , 126965.76 , 142833.   ,\n        150244.78 , 150128.48 , 149358.84 , 148539.44 , 148797.55 ,\n        150786.34 , 161078.64 , 160682.95 , 134904.86 , 105600.39 ,\n         95623.21 ,  82608.34 ,  80215.01 ,  79890.38 ,  86310.35 ,\n         99828.305, 121510.95 , 116638.2  , 108465.28 , 103486.48 ,\n        108384.914, 116128.6  , 125062.48 , 142273.05 , 146089.02 ,\n        149530.4  , 149280.52 , 146510.22 , 147309.14 , 150673.64 ,\n        157855.16 , 156224.12 , 130665.7  , 101402.41 ,  93899.984,\n         82542.766], dtype=float32)}\n\n\nLet us now visualize the fitted values of our models.\nAs we can see, the result obtained above has an output in a dictionary, to extract each element from the dictionary we are going to use the .get() function to extract the element and then we are going to save it in a pd.DataFrame().\n\nfitted=pd.DataFrame(result.get(\"fitted\"), columns=[\"fitted\"])\nfitted[\"ds\"]=df[\"ds\"]\nfitted\n\n\n\n\n\n\n\n\nfitted\nds\n\n\n\n\n0\nNaN\n2017-09-13 00:00:00\n\n\n1\nNaN\n2017-09-13 01:00:00\n\n\n2\nNaN\n2017-09-13 02:00:00\n\n\n...\n...\n...\n\n\n213\n101402.406250\n2017-09-21 21:00:00\n\n\n214\n93899.984375\n2017-09-21 22:00:00\n\n\n215\n82542.765625\n2017-09-21 23:00:00\n\n\n\n\n216 rows √ó 2 columns\n\n\n\n\nsns.lineplot(df, x=\"ds\", y=\"y\", label=\"Actual\", linewidth=2)\nsns.lineplot(fitted,x=\"ds\", y=\"fitted\", label=\"Fitted\", linestyle=\"--\" )\n\nplt.title(\"Ads watched (hourly data)\");\nplt.show()\n\n\n\n\n\n\nForecast Method\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min.\n\n# Prediction\nY_hat = sf.forecast(horizon, fitted=True)\nY_hat\n\n\n\n\n\n\n\n\nds\nSeasESOpt\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80116.203125\n\n\n1\n2017-09-22 01:00:00\n79812.976562\n\n\n1\n2017-09-22 02:00:00\n84372.742188\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n99043.750000\n\n\n1\n2017-09-23 04:00:00\n121514.937500\n\n\n1\n2017-09-23 05:00:00\n116611.320312\n\n\n\n\n30 rows √ó 2 columns\n\n\n\n\nvalues=sf.forecast_fitted_values()\nvalues.head()\n\n\n\n\n\n\n\n\nds\ny\nSeasESOpt\n\n\nunique_id\n\n\n\n\n\n\n\n1\n2017-09-13 00:00:00\n80115.0\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\nNaN\n\n\n1\n2017-09-13 02:00:00\n89325.0\nNaN\n\n\n1\n2017-09-13 03:00:00\n101930.0\nNaN\n\n\n1\n2017-09-13 04:00:00\n121630.0\nNaN\n\n\n\n\n\n\n\n\nStatsForecast.plot(values)\n\n\n\n\n\nY_hat=Y_hat.reset_index()\nY_hat\n\n\n\n\n\n\n\n\nunique_id\nds\nSeasESOpt\n\n\n\n\n0\n1\n2017-09-22 00:00:00\n80116.203125\n\n\n1\n1\n2017-09-22 01:00:00\n79812.976562\n\n\n2\n1\n2017-09-22 02:00:00\n84372.742188\n\n\n...\n...\n...\n...\n\n\n27\n1\n2017-09-23 03:00:00\n99043.750000\n\n\n28\n1\n2017-09-23 04:00:00\n121514.937500\n\n\n29\n1\n2017-09-23 05:00:00\n116611.320312\n\n\n\n\n30 rows √ó 3 columns\n\n\n\n\nY_hat1= pd.concat([df,Y_hat])\nY_hat1\n\n\n\n\n\n\n\n\nds\ny\nunique_id\nSeasESOpt\n\n\n\n\n0\n2017-09-13 00:00:00\n80115.0\n1\nNaN\n\n\n1\n2017-09-13 01:00:00\n79885.0\n1\nNaN\n\n\n2\n2017-09-13 02:00:00\n89325.0\n1\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n27\n2017-09-23 03:00:00\nNaN\n1\n99043.750000\n\n\n28\n2017-09-23 04:00:00\nNaN\n1\n121514.937500\n\n\n29\n2017-09-23 05:00:00\nNaN\n1\n116611.320312\n\n\n\n\n246 rows √ó 4 columns\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_df = pd.concat([df,Y_hat]).set_index('ds')\nplot_df[['y', \"SeasESOpt\"]].plot(ax=ax, linewidth=2)\nax.set_title(' Forecast', fontsize=22)\nax.set_ylabel('Ads watched (hourly data)', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid(True)\n\n\n\n\n\n\nPredict method with confidence interval\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 30 hours ahead.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecast_df = sf.predict(h=horizon) \nforecast_df\n\n\n\n\n\n\n\n\nds\nSeasESOpt\n\n\nunique_id\n\n\n\n\n\n\n1\n2017-09-22 00:00:00\n80116.203125\n\n\n1\n2017-09-22 01:00:00\n79812.976562\n\n\n1\n2017-09-22 02:00:00\n84372.742188\n\n\n...\n...\n...\n\n\n1\n2017-09-23 03:00:00\n99043.750000\n\n\n1\n2017-09-23 04:00:00\n121514.937500\n\n\n1\n2017-09-23 05:00:00\n116611.320312\n\n\n\n\n30 rows √ó 2 columns\n\n\n\nWe can join the forecast result with the historical data using the pandas function pd.concat(), and then be able to use this result for graphing.\n\ndf_plot= pd.concat([df, forecast_df]).set_index('ds')\ndf_plot\n\n\n\n\n\n\n\n\ny\nunique_id\nSeasESOpt\n\n\nds\n\n\n\n\n\n\n\n2017-09-13 00:00:00\n80115.0\n1\nNaN\n\n\n2017-09-13 01:00:00\n79885.0\n1\nNaN\n\n\n2017-09-13 02:00:00\n89325.0\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n2017-09-23 03:00:00\nNaN\nNaN\n99043.750000\n\n\n2017-09-23 04:00:00\nNaN\nNaN\n121514.937500\n\n\n2017-09-23 05:00:00\nNaN\nNaN\n116611.320312\n\n\n\n\n246 rows √ó 3 columns\n\n\n\nNow let‚Äôs visualize the result of our forecast and the historical data of our time series.\n\nsns.lineplot(df_plot,x=\"ds\", y=\"y\", label=\"Actual\")\nsns.lineplot(df_plot, x=\"ds\", y=\"SeasESOpt\", label=\"SeasESOpt\")\nplt.show()\n\n\n\n\nLet‚Äôs plot the same graph using the plot function that comes in Statsforecast, as shown below.\n\nsf.plot(df, forecast_df)"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#cross-validation",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#cross-validation",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Cross-validation ",
    "text": "Cross-validation \nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\nPerform time series cross-validation\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 5 months (n_windows=), forecasting every second months (step_size=12). Depending on your computer, this step should take around 1 min.\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 12 months ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvalidation_df = sf.cross_validation(df=df,\n                                         h=horizon,\n                                         step_size=30,\n                                         n_windows=3)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex().\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\nmodel: columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nSeasESOpt\n\n\nunique_id\n\n\n\n\n\n\n\n\n1\n2017-09-18 06:00:00\n2017-09-18 05:00:00\n99440.0\n141401.750000\n\n\n1\n2017-09-18 07:00:00\n2017-09-18 05:00:00\n97655.0\n152474.250000\n\n\n1\n2017-09-18 08:00:00\n2017-09-18 05:00:00\n97655.0\n152482.796875\n\n\n...\n...\n...\n...\n...\n\n\n1\n2017-09-21 21:00:00\n2017-09-20 17:00:00\n103080.0\n105600.390625\n\n\n1\n2017-09-21 22:00:00\n2017-09-20 17:00:00\n95155.0\n96717.390625\n\n\n1\n2017-09-21 23:00:00\n2017-09-20 17:00:00\n80285.0\n82608.343750\n\n\n\n\n90 rows √ó 4 columns"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#evaluate-model",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#evaluate-model",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Evaluate Model ",
    "text": "Evaluate Model \nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\nThe forecasts, in this case, Seasonal Exponential Smoothing Optimized Model.\n\n\nrmse = rmse(crossvalidation_df['y'], crossvalidation_df[\"SeasESOpt\"])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  16560.705"
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#acknowledgements",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#acknowledgements",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Naren Castellon for writing this tutorial."
  },
  {
    "objectID": "docs/models/seasonalexponentialsmoothingoptimized.html#references",
    "href": "docs/models/seasonalexponentialsmoothingoptimized.html#references",
    "title": "Seasonal Exponential Smoothing Optimized Model",
    "section": "References ",
    "text": "References \n\nChangquan Huang ‚Ä¢ Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python.\nIvan Svetunkov. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\nNixtla Parameters.\nPandas available frequencies.\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù..\nSeasonal periods- Rob J Hyndman."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "",
    "text": "By Fugue and Nixtla. Originally posted on TDS.\nTime-series modeling, analysis, and prediction of trends and seasonalities for data collected over time is a rapidly growing category of software applications.\nBusinesses, from electricity and economics to healthcare analytics, collect time-series data daily to predict patterns and build better data-driven product experiences. For example, temperature and humidity prediction is used in manufacturing to prevent defects, streaming metrics predictions help identify music‚Äôs popular artists, and sales forecasting for thousands of SKUs across different locations in the supply chain is used to optimize inventory costs. As data generation increases, the forecasting necessities have evolved from modeling a few time series to predicting millions.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#motivation",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#motivation",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Motivation",
    "text": "Motivation\nNixtla is an open-source project focused on state-of-the-art time series forecasting. They have a couple of libraries such as StatsForecast for statistical models, NeuralForecast for deep learning, and HierarchicalForecast for forecast aggregations across different levels of hierarchies. These are production-ready time series libraries focused on different modeling techniques.\nThis article looks at StatsForecast, a lightning-fast forecasting library with statistical and econometrics models. The AutoARIMA model of Nixtla is 20x faster than pmdarima, and the ETS (error, trend, seasonal) models performed 4x faster than statsmodels and are more robust. The benchmarks and code to reproduce can be found here. A huge part of the performance increase is due to using a JIT compiler called numba to achieve high speeds.\nThe faster iteration time means that data scientists can run more experiments and converge to more accurate models faster. It also means that running benchmarks at scale becomes easier.\nIn this article, we are interested in the scalability of the StatsForecast library in fitting models over Spark or Dask using the Fugue library. This combination will allow us to train a huge number of models distributedly over a temporary cluster quickly."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#experiment-setup",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#experiment-setup",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Experiment Setup",
    "text": "Experiment Setup\nWhen dealing with large time series data, users normally have to deal with thousands of logically independent time series (think of telemetry of different users or different product sales). In this case, we can train one big model over all of the series, or we can create one model for each series. Both are valid approaches since the bigger model will pick up trends across the population, while training thousands of models may fit individual series data better.\n\n\n\n\n\n\nNote\n\n\n\nNote: to pick up both the micro and macro trends of the time series population in one model, check the Nixtla HierarchicalForecast library, but this is also more computationally expensive and trickier to scale.\n\n\nThis article will deal with the scenario where we train a couple of models (AutoARIMA or ETS) per univariate time series. For this setup, we group the full data by time series, and then train each model for each group. The image below illustrates this. The distributed DataFrame can either be a Spark or Dask DataFrame.\n\n\n\nAutoARIMA per partition\n\n\nNixtla previously released benchmarks with Anyscale on distributing this model training on Ray. The setup and results can be found in this blog. The results are also shown below. It took 2000 cpus to run one million AutoARIMA models in 35 minutes. We‚Äôll compare this against running on Spark.\n\n\n\nStatsForecast on Ray results"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#statsforecast-code",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#statsforecast-code",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "StatsForecast code",
    "text": "StatsForecast code\nFirst, we‚Äôll look at the StatsForecast code used to run the AutoARIMA distributedly on Ray. This is a simplified version to run the scenario with a one million time series. It is also updated for the recent StatsForecast v1.0.0 release, so it may look a bit different from the code in the previous benchmarks.\nfrom time import time\n\nimport pandas as pd\nfrom statsforecast.utils import generate_series\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\nseries = generate_series(n_series=1000000, seed=1)\n\nmodel = StatsForecast(df=series,\n                      models=[AutoARIMA()], \n                      freq='D', \n                      n_jobs=-1,\n              ray_address=ray_address)\n\ninit = time()\nforecasts = model.forecast(7)\nprint(f'n_series: 1000000 total time: {(time() - init) / 60}')\nThe interface of StatsForecast is very minimal. It is already designed to perform the AutoARIMA on each group of data. Just supplying the ray_address will make this code snippet run distributedly. Without it, n_jobswill indicate the number of parallel processes for forecasting. model.forecast() will do the fit and predict in one step, and the input to this method in the time horizon to forecast."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#using-fugue-to-run-on-spark-and-dask",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#using-fugue-to-run-on-spark-and-dask",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Using Fugue to run on Spark and Dask",
    "text": "Using Fugue to run on Spark and Dask\nFugue is an abstraction layer that ports Python, Pandas, and SQL code to Spark and Dask. The most minimal interface is the transform() function. This function takes in a function and DataFrame, and brings it to Spark or Dask. We can use the transform() function to bring StatsForecast execution to Spark.\nThere are two parts to the code below. First, we have the forecast logic defined in the forecast_series function. Some parameters are hardcoded for simplicity. The most important one is that n_jobs=1. This is because Spark or Dask will already serve as the parallelization layer, and having two stages of parallelism can cause resource deadlocks.\nfrom fugue import transform\n\ndef forecast_series(df: pd.DataFrame, models) -&gt; pd.DataFrame:\n    tdf = df.set_index(\"unique_id\")\n    model = StatsForecast(df=tdf, models=models, freq='D', n_jobs=1)\n    return model.forecast(7).reset_index()\n\ntransform(series.reset_index(),\n          forecast_series,\n          params=dict(models=[AutoARIMA()]),\n          schema=\"unique_id:int, ds:date, AutoARIMA:float\",\n          partition={\"by\": \"unique_id\"},\n          engine=\"spark\"\n          ).show()\nSecond, the transform() function is used to apply the forecast_series() function on Spark. The first two arguments are the DataFrame and function to be applied. Output schema is a requirement for Spark, so we need to pass it in, and the partition argument will take care of splitting the time series modelling by unique_id.\nThis code already works and returns a Spark DataFrame output."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#nixtlas-fuguebackend",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#nixtlas-fuguebackend",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Nixtla‚Äôs FugueBackend",
    "text": "Nixtla‚Äôs FugueBackend\nThe transform() above is a general look at what Fugue can do. In practice, the Fugue and Nixtla teams collaborated to add a more native FugueBackend to the StatsForecast library. Along with it is a utility forecast() function to simplify the forecasting interface. Below is an end-to-end example of running StatsForecast on one million time series.\nfrom statsforecast.distributed.utils import forecast\nfrom statsforecast.distributed.fugue import FugueBackend\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nbackend = FugueBackend(spark, {\"fugue.spark.use_pandas_udf\":True})\n\nforecast(spark.read.parquet(\"/tmp/1m.parquet\"), \n         [AutoARIMA()], \n         freq=\"D\", \n         h=7, \n         parallel=backend).toPandas()\nWe just need to create the FugueBackend, which takes in a SparkSession and passes it to forecast(). This function can take either a DataFrame or file path to the data. If a file path is provided, it will be loaded with the parallel backend. In this example above, we replaced the file each time we ran the experiment to generate benchmarks.\n\n\n\n\n\n\nCaution\n\n\n\nIt‚Äôs also important to note that we can test locally before running the forecast() on full data. All we have to do is not supply anything for the parallel argument; everything will run on Pandas sequentially."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#benchmark-results",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#benchmark-results",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Benchmark Results",
    "text": "Benchmark Results\nThe benchmark results can be seen below. As of the time of this writing, Dask and Ray made recent releases, so only the Spark metrics are up to date. We will make a follow-up article after running these experiments with the updates.\n\n\n\nSpark and Dask benchmarks for StatsForecast at scale\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: The attempt was to use 2000 cpus but we were limited by available compute instances on AWS.\n\n\nThe important part here is that AutoARIMA trained one million time series models in less than 15 minutes. The cluster configuration is attached in the appendix. With very few lines of code, we were able to orchestrate the training of these time series models distributedly."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#conclusion",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#conclusion",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Conclusion",
    "text": "Conclusion\nTraining thousands of time series models distributedly normally takes a lot of coding with Spark and Dask, but we were able to run these experiments with very few lines of code. Nixtla‚Äôs StatsForecast offers the ability to quickly utilize all of the compute resources available to find the best model for each time series. All users need to do is supply a relevant parallel backend (Ray or Fugue) to run on a cluster.\nOn the scale of one million timeseries, our total training time took 12 minutes for AutoARIMA. This is the equivalent of close to 400 cpu-hours that we ran immediately, allowing data scientists to quickly iterate at scale without having to write the explicit code for parallelization. Because we used an ephemeral cluster, the cost is effectively the same as running this sequentially on an EC2 instance (parallelized over all cores)."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#resources",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#resources",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Resources",
    "text": "Resources\n\nNixtla StatsForecast repo\nStatsForecast docs\nFugue repo\nFugue tutorials\n\nTo chat with us:\n\nFugue Slack\nNixtla Slack"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#appendix",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#appendix",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Appendix",
    "text": "Appendix\nFor anyone. interested in the cluster configuration, it can be seen below. This will spin up a Databricks cluster. The important thing is the node_type_id that has the machines used.\n{\n    \"num_workers\": 20,\n    \"cluster_name\": \"fugue-nixtla-2\",\n    \"spark_version\": \"10.4.x-scala2.12\",\n    \"spark_conf\": {\n        \"spark.speculation\": \"true\",\n        \"spark.sql.shuffle.partitions\": \"8000\",\n        \"spark.sql.adaptive.enabled\": \"false\",\n        \"spark.task.cpus\": \"1\"\n    },\n    \"aws_attributes\": {\n        \"first_on_demand\": 1,\n        \"availability\": \"SPOT_WITH_FALLBACK\",\n        \"zone_id\": \"us-west-2c\",\n        \"spot_bid_price_percent\": 100,\n        \"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n        \"ebs_volume_count\": 1,\n        \"ebs_volume_size\": 32\n    },\n    \"node_type_id\": \"m5.24xlarge\",\n    \"driver_node_type_id\": \"m5.2xlarge\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"MKL_NUM_THREADS\": \"1\",\n        \"OPENBLAS_NUM_THREADS\": \"1\",\n        \"VECLIB_MAXIMUM_THREADS\": \"1\",\n        \"OMP_NUM_THREADS\": \"1\",\n        \"NUMEXPR_NUM_THREADS\": \"1\"\n    },\n    \"autotermination_minutes\": 20,\n    \"enable_elastic_disk\": false,\n    \"cluster_source\": \"UI\",\n    \"init_scripts\": [],\n    \"runtime_engine\": \"STANDARD\",\n    \"cluster_id\": \"0728-004950-oefym0ss\"\n}"
  },
  {
    "objectID": "src/distributed.multiprocess.html",
    "href": "src/distributed.multiprocess.html",
    "title": "MultiprocessBackend",
    "section": "",
    "text": "source\n\nMultiprocessBackend\n\n MultiprocessBackend (n_jobs:int)\n\nMultiprocessBackend Parent Class for Distributed Computation.\nParameters: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nNotes:\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/arima.html",
    "href": "src/arima.html",
    "title": "ARIMA",
    "section": "",
    "text": "source\n\npredict_arima\n\n predict_arima (model, n_ahead, newxreg=None, se_fit=True)\n\n\nmyarima(ap, order=(2, 1, 1), seasonal={'order': (0, 1, 0), 'period': 12}, \n        constant=False, ic='aicc', method='CSS-ML')['aic']\n\n\nsource\n\n\narima_string\n\n arima_string (model, padding=False)\n\n\nsource\n\n\nforecast_arima\n\n forecast_arima (model, h=None, level=None, fan=False, xreg=None,\n                 blambda=None, bootstrap=False, npaths=5000, biasadj=None)\n\n\nsource\n\n\nfitted_arima\n\n fitted_arima (model, h=1)\n\nReturns h-step forecasts for the data used in fitting the model.\n\nsource\n\n\nauto_arima_f\n\n auto_arima_f (x, d=None, D=None, max_p=5, max_q=5, max_P=2, max_Q=2,\n               max_order=5, max_d=2, max_D=1, start_p=2, start_q=2,\n               start_P=1, start_Q=1, stationary=False, seasonal=True,\n               ic='aicc', stepwise=True, nmodels=94, trace=False,\n               approximation=None, method=None, truncate=None, xreg=None,\n               test='kpss', test_kwargs=None, seasonal_test='seas',\n               seasonal_test_kwargs=None, allowdrift=True, allowmean=True,\n               blambda=None, biasadj=False, period=1)\n\n\nsource\n\n\nprint_statsforecast_ARIMA\n\n print_statsforecast_ARIMA (model, digits=3, se=True)\n\n\nsource\n\n\nARIMASummary\n\n ARIMASummary (model)\n\nARIMA Summary.\n\nsource\n\n\nAutoARIMA\n\n AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=None, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=True, allowmean:bool=True,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            period:int=1)\n\nAn AutoARIMA estimator.\nReturns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided.\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/ces.html",
    "href": "src/ces.html",
    "title": "CES Model",
    "section": "",
    "text": "source\n\n\n\n ces_target_fn (optimal_param, init_alpha_0, init_alpha_1, init_beta_0,\n                init_beta_1, opt_alpha_0, opt_alpha_1, opt_beta_0,\n                opt_beta_1, y, m, init_states, n_components, seasontype,\n                nmse)\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/ces.html#cescalc",
    "href": "src/ces.html#cescalc",
    "title": "CES Model",
    "section": "",
    "text": "source\n\n\n\n ces_target_fn (optimal_param, init_alpha_0, init_alpha_1, init_beta_0,\n                init_beta_1, opt_alpha_0, opt_alpha_1, opt_beta_0,\n                opt_beta_1, y, m, init_states, n_components, seasontype,\n                nmse)"
  },
  {
    "objectID": "src/core/models_intro.html",
    "href": "src/core/models_intro.html",
    "title": "StatsForecast‚Äôs Models",
    "section": "",
    "text": "Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoETS\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoCES\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/core/models_intro.html#automatic-forecasting",
    "href": "src/core/models_intro.html#automatic-forecasting",
    "title": "StatsForecast‚Äôs Models",
    "section": "",
    "text": "Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoETS\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoCES\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "src/core/models_intro.html#arima-family",
    "href": "src/core/models_intro.html#arima-family",
    "title": "StatsForecast‚Äôs Models",
    "section": "ARIMA Family",
    "text": "ARIMA Family\nThese models exploit the existing autocorrelations in the time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nARIMA\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoRegressive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "src/core/models_intro.html#theta-family",
    "href": "src/core/models_intro.html#theta-family",
    "title": "StatsForecast‚Äôs Models",
    "section": "Theta Family",
    "text": "Theta Family\nFit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nOptimizedTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nDynamicTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nDynamicOptimizedTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "src/core/models_intro.html#multiple-seasonalities",
    "href": "src/core/models_intro.html#multiple-seasonalities",
    "title": "StatsForecast‚Äôs Models",
    "section": "Multiple Seasonalities",
    "text": "Multiple Seasonalities\nSuited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nMSTL\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "src/core/models_intro.html#garch-and-arch-models",
    "href": "src/core/models_intro.html#garch-and-arch-models",
    "title": "StatsForecast‚Äôs Models",
    "section": "GARCH and ARCH Models",
    "text": "GARCH and ARCH Models\nSuited for modeling time series that exhibit non-constant volatility over time. The ARCH model is a particular case of GARCH.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nGARCH\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nARCH\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "src/core/models_intro.html#baseline-models",
    "href": "src/core/models_intro.html#baseline-models",
    "title": "StatsForecast‚Äôs Models",
    "section": "Baseline Models",
    "text": "Baseline Models\nClassical models for establishing baseline.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nHistoricAverage\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nNaive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nRandomWalkWithDrift\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nSeasonalNaive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nWindowAverage\n‚úÖ\n\n\n\n\n\nSeasonalWindowAverage\n‚úÖ"
  },
  {
    "objectID": "src/core/models_intro.html#exponential-smoothing",
    "href": "src/core/models_intro.html#exponential-smoothing",
    "title": "StatsForecast‚Äôs Models",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with clear trend and/or seasonality. Use the SimpleExponential family for data with no clear trend or seasonality.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nSimpleExponentialSmoothing\n‚úÖ\n\n\n\n\n\nSimpleExponentialSmoothingOptimized\n‚úÖ\n\n\n\n\n\nSeasonalExponentialSmoothing\n‚úÖ\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n‚úÖ\n\n\n\n\n\nHolt\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nHoltWinters\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "src/core/models_intro.html#sparse-or-intermittent",
    "href": "src/core/models_intro.html#sparse-or-intermittent",
    "title": "StatsForecast‚Äôs Models",
    "section": "Sparse or Intermittent",
    "text": "Sparse or Intermittent\nSuited for series with very few non-zero observations\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nADIDA\n‚úÖ\n\n\n\n\n\nCrostonClassic\n‚úÖ\n\n\n\n\n\nCrostonOptimized\n‚úÖ\n\n\n\n\n\nCrostonSBA\n‚úÖ\n\n\n\n\n\nIMAPA\n‚úÖ\n\n\n\n\n\nTSB\n‚úÖ"
  },
  {
    "objectID": "src/core/core.html",
    "href": "src/core/core.html",
    "title": "Core Methods",
    "section": "",
    "text": "The core methods of StatsForecast are:\ndef test_gp_df(df, sort_df):\n    df = df.set_index(\"ds\", append=True)\n    if not df.index.is_monotonic_increasing and sort_df:\n        df = df.sort_index()\n    data = df.values.astype(np.float32)\n    indices_sizes = df.index.get_level_values(\"unique_id\").value_counts(sort=False)\n    indices = indices_sizes.index\n    sizes = indices_sizes.values\n    cum_sizes = sizes.cumsum()\n    dates = df.index.get_level_values(\"ds\")[cum_sizes - 1]\n    indptr = np.append(0, cum_sizes).astype(np.int32)\n    return GroupedArray(data, indptr), indices, dates, df.index\nsource\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/core/core.html#statsforecast",
    "href": "src/core/core.html#statsforecast",
    "title": "Core Methods",
    "section": "StatsForecast",
    "text": "StatsForecast\n\n StatsForecast (models:List[Any], freq:str, n_jobs:int=1, df:Union[pandas.\n                core.frame.DataFrame,polars.dataframe.frame.DataFrame,None\n                Type]=None, sort_df:bool=True,\n                fallback_model:Optional[Any]=None, verbose:bool=False)\n\nTrain statistical models.\nThe StatsForecast class allows you to efficiently fit multiple StatsForecast models for large sets of time series. It operates with pandas DataFrame df that identifies series and datestamps with the unique_id and ds columns. The y column denotes the target time series variable.\nThe class has memory-efficient StatsForecast.forecast method that avoids storing partial model outputs. While the StatsForecast.fit and StatsForecast.predict methods with Scikit-learn interface store the fitted models.\nThe StatsForecast class offers parallelization utilities with Dask, Spark and Ray back-ends. See distributed computing example here.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodels\ntyping.List[typing.Any]\n\nList of instantiated objects models.StatsForecast.\n\n\nfreq\nstr\n\nFrequency of the data.See panda‚Äôs available frequencies.\n\n\nn_jobs\nint\n1\nNumber of jobs used in the parallel processing, use -1 for all cores.\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nfallback_model\ntyping.Optional[typing.Any]\nNone\nModel to be used if a model fails. Only works with the forecast and cross_validation methods.\n\n\nverbose\nbool\nFalse\nPrints TQDM progress bar when n_jobs=1.\n\n\n\n\n# StatsForecast's class usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    ADIDA,\n    AutoARIMA,\n    CrostonClassic,\n    CrostonOptimized,\n    CrostonSBA,\n    HistoricAverage,\n    IMAPA,\n    Naive,\n    RandomWalkWithDrift,\n    SeasonalExponentialSmoothing,\n    SeasonalNaive,\n    SeasonalWindowAverage,\n    SimpleExponentialSmoothing,\n    TSB,\n    WindowAverage,\n    DynamicOptimizedTheta,\n    AutoETS,\n    AutoCES\n)\n\n# Generate synthetic panel DataFrame for example\npanel_df = generate_series(n_series=9, equal_ends=False, engine='pandas')\npanel_df.groupby('unique_id').tail(4)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n218\n0\n2000-08-06\n1.212726\n\n\n219\n0\n2000-08-07\n2.442669\n\n\n220\n0\n2000-08-08\n3.339940\n\n\n221\n0\n2000-08-09\n4.228065\n\n\n315\n1\n2000-04-03\n0.048275\n\n\n316\n1\n2000-04-04\n1.128070\n\n\n317\n1\n2000-04-05\n2.295968\n\n\n318\n1\n2000-04-06\n3.238239\n\n\n482\n2\n2000-06-12\n6.480128\n\n\n483\n2\n2000-06-13\n0.036217\n\n\n484\n2\n2000-06-14\n1.009650\n\n\n485\n2\n2000-06-15\n2.489787\n\n\n724\n3\n2000-08-26\n3.289840\n\n\n725\n3\n2000-08-27\n4.227949\n\n\n726\n3\n2000-08-28\n5.321176\n\n\n727\n3\n2000-08-29\n6.127013\n\n\n1097\n4\n2001-01-04\n5.403709\n\n\n1098\n4\n2001-01-05\n6.081779\n\n\n1099\n4\n2001-01-06\n0.438420\n\n\n1100\n4\n2001-01-07\n1.386855\n\n\n1398\n5\n2000-10-24\n5.011166\n\n\n1399\n5\n2000-10-25\n6.397153\n\n\n1400\n5\n2000-10-26\n0.462146\n\n\n1401\n5\n2000-10-27\n1.253125\n\n\n1643\n6\n2000-08-29\n5.407805\n\n\n1644\n6\n2000-08-30\n6.340789\n\n\n1645\n6\n2000-08-31\n0.202894\n\n\n1646\n6\n2000-09-01\n1.491204\n\n\n2052\n7\n2001-02-09\n1.068102\n\n\n2053\n7\n2001-02-10\n2.233974\n\n\n2054\n7\n2001-02-11\n3.484143\n\n\n2055\n7\n2001-02-12\n4.176505\n\n\n2111\n8\n2000-02-25\n4.110373\n\n\n2112\n8\n2000-02-26\n5.483879\n\n\n2113\n8\n2000-02-27\n6.068916\n\n\n2114\n8\n2000-02-28\n0.040499\n\n\n\n\n\n\n\n\n# Declare list of instantiated StatsForecast estimators to be fitted\n# You can try other estimator's hyperparameters\n# You can try other methods from the `models.StatsForecast` collection\n# Check them here: https://nixtla.github.io/statsforecast/models.html\nmodels=[AutoARIMA(), Naive(), \n        AutoETS(), AutoARIMA(allowmean=True, alias='MeanAutoARIMA')] \n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=models,\n                     freq='D', \n                     n_jobs=1, \n                     verbose=True)\n\n# Efficiently predict\nfcsts_df = fcst.forecast(h=4, fitted=True).reset_index()\nfcsts_df.groupby('unique_id').tail(4)\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.fit",
    "href": "src/core/core.html#statsforecast.fit",
    "title": "Core Methods",
    "section": "StatsForecast.fit",
    "text": "StatsForecast.fit\n\n StatsForecast.fit (df:Union[pandas.core.frame.DataFrame,polars.dataframe.\n                    frame.DataFrame,NoneType]=None, sort_df:bool=True, pre\n                    diction_intervals:Optional[statsforecast.utils.Conform\n                    alIntervals]=None)\n\nFit statistical models.\nFit models to a large set of time series from DataFrame df and store fitted models for later inspection.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nReturns\nStatsForecast\n\nReturns with stored StatsForecast fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#satstforecast.predict",
    "href": "src/core/core.html#satstforecast.predict",
    "title": "Core Methods",
    "section": "SatstForecast.predict",
    "text": "SatstForecast.predict\n\n SatstForecast.predict (h:int, X_df:Union[pandas.core.frame.DataFrame,pola\n                        rs.dataframe.frame.DataFrame,NoneType]=None,\n                        level:Optional[List[int]]=None)\n\nPredict statistical models.\nUse stored fitted models to predict large set of time series from DataFrame df.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with [unique_id, ds] columns and df‚Äôs future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nReturns\npandas.DataFrame | polars.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.fit_predict",
    "href": "src/core/core.html#statsforecast.fit_predict",
    "title": "Core Methods",
    "section": "StatsForecast.fit_predict",
    "text": "StatsForecast.fit_predict\n\n StatsForecast.fit_predict (h:int, df:Union[pandas.core.frame.DataFrame,po\n                            lars.dataframe.frame.DataFrame,NoneType]=None,\n                            X_df:Union[pandas.core.frame.DataFrame,polars.\n                            dataframe.frame.DataFrame,NoneType]=None,\n                            level:Optional[List[int]]=None,\n                            sort_df:bool=True, prediction_intervals:Option\n                            al[statsforecast.utils.ConformalIntervals]=Non\n                            e)\n\nFit and Predict with statistical models.\nThis method avoids memory burden due from object storage. It is analogous to Scikit-Learn fit_predict without storing information. It requires the forecast horizon h in advance.\nIn contrast to StatsForecast.forecast this method stores partial models outputs.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous variables.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nX_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with [unique_id, ds] columns and df‚Äôs future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nReturns\npandas.DataFrame | polars.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.forecast",
    "href": "src/core/core.html#statsforecast.forecast",
    "title": "Core Methods",
    "section": "StatsForecast.forecast",
    "text": "StatsForecast.forecast\n\n StatsForecast.forecast (h:int, df:Union[pandas.core.frame.DataFrame,polar\n                         s.dataframe.frame.DataFrame,NoneType]=None, X_df:\n                         Union[pandas.core.frame.DataFrame,polars.datafram\n                         e.frame.DataFrame,NoneType]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False, sort_df:bool=True, prediction_\n                         intervals:Optional[statsforecast.utils.ConformalI\n                         ntervals]=None)\n\nMemory Efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to Scikit-Learn fit_predict without storing information. It requires the forecast horizon h in advance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nX_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with [unique_id, ds] columns and df‚Äôs future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nfitted\nbool\nFalse\nWether or not return insample predictions.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nReturns\npandas.DataFrame | polars.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\n# StatsForecast.forecast method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import AutoARIMA, Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[AutoARIMA(), Naive()],\n                     freq='D', n_jobs=1)\n\n# Efficiently predict without storing memory\nfcsts_df = fcst.forecast(h=4, fitted=True)\nfcsts_df.groupby('unique_id').tail(4)\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nNaive\n\n\nunique_id\n\n\n\n\n\n\n\n1.0\n1961-01-01\n476.006500\n432.0\n\n\n1.0\n1961-01-02\n482.846222\n432.0\n\n\n1.0\n1961-01-03\n512.423523\n432.0\n\n\n1.0\n1961-01-04\n502.038269\n432.0\n\n\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.forecast_fitted_values",
    "href": "src/core/core.html#statsforecast.forecast_fitted_values",
    "title": "Core Methods",
    "section": "StatsForecast.forecast_fitted_values",
    "text": "StatsForecast.forecast_fitted_values\n\n StatsForecast.forecast_fitted_values ()\n\nAccess insample predictions.\nAfter executing StatsForecast.forecast, you can access the insample prediction values for each model. To get them, you need to pass fitted=True to the StatsForecast.forecast method and then use the StatsForecast.forecast_fitted_values method.\n\n# StatsForecast.forecast_fitted_values method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[AutoARIMA()],\n                     freq='D', n_jobs=1)\n\n# Access insample predictions\nfcsts_df = fcst.forecast(h=12, fitted=True, level=(90, 10))\ninsample_fcsts_df = fcst.forecast_fitted_values()\ninsample_fcsts_df.tail(4)\n\n\n\n\n\n\n\n\nds\ny\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-lo-10\nAutoARIMA-hi-10\nAutoARIMA-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n1.0\n1960-09-30\n508.0\n572.654175\n525.092163\n569.020630\n576.287781\n620.216187\n\n\n1.0\n1960-10-31\n461.0\n451.528259\n403.966248\n447.894684\n455.161835\n499.090271\n\n\n1.0\n1960-11-30\n390.0\n437.915375\n390.353363\n434.281799\n441.548981\n485.477386\n\n\n1.0\n1960-12-31\n432.0\n369.718781\n322.156769\n366.085205\n373.352356\n417.280792\n\n\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.cross_validation",
    "href": "src/core/core.html#statsforecast.cross_validation",
    "title": "Core Methods",
    "section": "StatsForecast.cross_validation",
    "text": "StatsForecast.cross_validation\n\n StatsForecast.cross_validation (h:int, df:Union[pandas.core.frame.DataFra\n                                 me,polars.dataframe.frame.DataFrame,NoneT\n                                 ype]=None, n_windows:int=1,\n                                 step_size:int=1,\n                                 test_size:Optional[int]=None,\n                                 input_size:Optional[int]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False, refit:bool=True,\n                                 sort_df:bool=True, prediction_intervals:O\n                                 ptional[statsforecast.utils.ConformalInte\n                                 rvals]=None)\n\nTemporal Cross-Validation.\nEfficiently fits a list of StatsForecast models through multiple training windows, in either chained or rolled manner.\nStatsForecast.models‚Äô speed allows to overcome this evaluation technique high computational costs. Temporal cross-validation provides better model‚Äôs generalization measurements by increasing the test‚Äôs length and diversity.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nn_windows\nint\n1\nNumber of windows used for cross validation.\n\n\nstep_size\nint\n1\nStep size between each window.\n\n\ntest_size\ntyping.Optional[int]\nNone\nLength of test size. If passed, set n_windows=None.\n\n\ninput_size\ntyping.Optional[int]\nNone\nInput size for each window, if not none rolled windows.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nfitted\nbool\nFalse\nWether or not returns insample predictions.\n\n\nrefit\nbool\nTrue\nWether or not refit the model for each window.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by unique_id and ds.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nReturns\npandas.DataFrame\n\nDataFrame with insample models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\n# StatsForecast.crossvalidation method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[Naive()],\n                     freq='D', n_jobs=1, verbose=True)\n\n# Access insample predictions\nrolled_fcsts_df = fcst.cross_validation(14, n_windows=2)\nrolled_fcsts_df.head(4)\n\nCross Validation Time Series 1:   0%|          | 0/2 [00:00&lt;?, ?it/s]Cross Validation Time Series 1: 100%|##########| 2/2 [00:00&lt;00:00, 8208.03it/s]\n\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nNaive\n\n\nunique_id\n\n\n\n\n\n\n\n\n1.0\n1960-12-17\n1960-12-16\n407.0\n463.0\n\n\n1.0\n1960-12-18\n1960-12-16\n362.0\n463.0\n\n\n1.0\n1960-12-19\n1960-12-16\n405.0\n463.0\n\n\n1.0\n1960-12-20\n1960-12-16\n417.0\n463.0\n\n\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.cross_validation_fitted_values",
    "href": "src/core/core.html#statsforecast.cross_validation_fitted_values",
    "title": "Core Methods",
    "section": "StatsForecast.cross_validation_fitted_values",
    "text": "StatsForecast.cross_validation_fitted_values\n\n StatsForecast.cross_validation_fitted_values ()\n\nAccess insample cross validated predictions.\nAfter executing StatsForecast.cross_validation, you can access the insample prediction values for each model and window. To get them, you need to pass fitted=True to the StatsForecast.cross_validation method and then use the StatsForecast.cross_validation_fitted_values method.\n\n# StatsForecast.cross_validation_fitted_values method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[Naive()],\n                     freq='D', n_jobs=1)\n\n# Access insample predictions\nrolled_fcsts_df = fcst.cross_validation(h=12, n_windows=2, fitted=True)\ninsample_rolled_fcsts_df = fcst.cross_validation_fitted_values()\ninsample_rolled_fcsts_df.tail(4)\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nNaive\n\n\nunique_id\n\n\n\n\n\n\n\n\n1.0\n1959-09-30\n1959-12-31\n463.0\n559.0\n\n\n1.0\n1959-10-31\n1959-12-31\n407.0\n463.0\n\n\n1.0\n1959-11-30\n1959-12-31\n362.0\n407.0\n\n\n1.0\n1959-12-31\n1959-12-31\n405.0\n362.0\n\n\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.plot",
    "href": "src/core/core.html#statsforecast.plot",
    "title": "Core Methods",
    "section": "StatsForecast.plot",
    "text": "StatsForecast.plot\n\n StatsForecast.plot\n                     (df:Union[pandas.core.frame.DataFrame,polars.datafram\n                     e.frame.DataFrame], forecasts_df:Union[pandas.core.fr\n                     ame.DataFrame,polars.dataframe.frame.DataFrame,NoneTy\n                     pe]=None, unique_ids:Union[List[str],NoneType,numpy.n\n                     darray]=None, plot_random:bool=True,\n                     models:Optional[List[str]]=None,\n                     level:Optional[List[float]]=None,\n                     max_insample_length:Optional[int]=None,\n                     plot_anomalies:bool=False, engine:str='matplotlib',\n                     resampler_kwargs:Optional[Dict]=None)\n\nPlot forecasts and insample values.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nDataFrame with columns [unique_id, ds, y].\n\n\nforecasts_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds] and models.\n\n\nunique_ids\ntyping.Union[typing.List[str], NoneType, numpy.ndarray]\nNone\nTime Series to plot.If None, time series are selected randomly.\n\n\nplot_random\nbool\nTrue\nSelect time series to plot randomly.\n\n\nmodels\ntyping.Optional[typing.List[str]]\nNone\nList of models to plot.\n\n\nlevel\ntyping.Optional[typing.List[float]]\nNone\nList of prediction intervals to plot if paseed.\n\n\nmax_insample_length\ntyping.Optional[int]\nNone\nMax number of train/insample observations to be plotted.\n\n\nplot_anomalies\nbool\nFalse\nPlot anomalies for each prediction interval.\n\n\nengine\nstr\nmatplotlib\nLibrary used to plot. ‚Äòplotly‚Äô, ‚Äòplotly-resampler‚Äô or ‚Äòmatplotlib‚Äô.\n\n\nresampler_kwargs\ntyping.Optional[typing.Dict]\nNone\nKwargs to be passed to plotly-resampler constructor. For further custumization (‚Äúshow_dash‚Äù) call the method,store the plotting object and add the extra arguments toits show_dash method.\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.save",
    "href": "src/core/core.html#statsforecast.save",
    "title": "Core Methods",
    "section": "StatsForecast.save",
    "text": "StatsForecast.save\n\n StatsForecast.save (path:Union[pathlib.Path,str,NoneType]=None,\n                     max_size:Optional[str]=None, trim:bool=False)\n\nFunction that will save StatsForecast class with certain settings to make it reproducible.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\ntyping.Union[pathlib.Path, str, NoneType]\nNone\nPath of the file to be saved. If None will create one in the current directory using the current UTC timestamp.\n\n\nmax_size\ntyping.Optional[str]\nNone\nStatsForecast object should not exceed this size.Available byte naming: [‚ÄòB‚Äô, ‚ÄòKB‚Äô, ‚ÄòMB‚Äô, ‚ÄòGB‚Äô]\n\n\ntrim\nbool\nFalse\nDelete any attributes not needed for inference.\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.load",
    "href": "src/core/core.html#statsforecast.load",
    "title": "Core Methods",
    "section": "StatsForecast.load",
    "text": "StatsForecast.load\n\n StatsForecast.load (path:Union[pathlib.Path,str])\n\nAutomatically loads the model into ready StatsForecast.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\ntyping.Union[pathlib.Path, str]\nPath to saved StatsForecast file.\n\n\nReturns\nsf: StatsForecast\nPreviously saved StatsForecast"
  },
  {
    "objectID": "src/core/core.html#integer-datestamp",
    "href": "src/core/core.html#integer-datestamp",
    "title": "Core Methods",
    "section": "Integer datestamp",
    "text": "Integer datestamp\nThe StatsForecast class can also receive integers as datestamp, the following example shows how to do it.\n\n# from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengers as ap\nfrom statsforecast.models import HistoricAverage\n\n\nint_ds_df = pd.DataFrame({'ds': np.arange(1, len(ap) + 1), 'y': ap})\nint_ds_df.insert(0, 'unique_id', 'AirPassengers')\nint_ds_df.set_index('unique_id', inplace=True)\nint_ds_df.head()\n\n\nint_ds_df.tail()\n\n\nint_ds_df\n\n\nfcst = StatsForecast(df=int_ds_df, models=[HistoricAverage()], freq='D')\nhorizon = 7\nforecast = fcst.forecast(horizon)\nforecast.head()\n\n\nlast_date = int_ds_df['ds'].max()\ntest_eq(forecast['ds'].values, np.arange(last_date + 1, last_date + 1 + horizon))\n\n\nint_ds_cv = fcst.cross_validation(h=7, test_size=8, n_windows=None)\nint_ds_cv"
  },
  {
    "objectID": "src/core/core.html#external-regressors",
    "href": "src/core/core.html#external-regressors",
    "title": "Core Methods",
    "section": "External regressors",
    "text": "External regressors\nEvery column after y is considered an external regressor and will be passed to the models that allow them. If you use them you must supply the future values to the StatsForecast.forecast method.\n\nclass LinearRegression:\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, y, X):\n        self.coefs_, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return self\n    \n    def predict(self, h, X):\n        mean = X @ coefs\n        return mean\n    \n    def __repr__(self):\n        return 'LinearRegression()'\n    \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        coefs, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return {'mean': X_future @ coefs}\n    \n    def new(self):\n        b = type(self).__new__(type(self))\n        b.__dict__.update(self.__dict__)\n        return b\n\n\nseries_xreg = series = generate_series(10_000, equal_ends=True)\nseries_xreg['intercept'] = 1\nseries_xreg['dayofweek'] = series_xreg['ds'].dt.dayofweek\nseries_xreg = pd.get_dummies(series_xreg, columns=['dayofweek'], drop_first=True)\nseries_xreg\n\n\ndates = sorted(series_xreg['ds'].unique())\nvalid_start = dates[-14]\ntrain_mask = series_xreg['ds'] &lt; valid_start\nseries_train = series_xreg[train_mask]\nseries_valid = series_xreg[~train_mask]\nX_valid = series_valid.drop(columns=['y'])\nfcst = StatsForecast(\n    df=series_train,\n    models=[LinearRegression()],\n    freq='D',\n)\nxreg_res = fcst.forecast(14, X_df=X_valid)\nxreg_res['y'] = series_valid['y'].values\n\n\nxreg_res.groupby('ds').mean().plot()\n\n\nxreg_res_cv = fcst.cross_validation(h=3, test_size=5, n_windows=None)"
  },
  {
    "objectID": "src/core/core.html#prediction-intervals",
    "href": "src/core/core.html#prediction-intervals",
    "title": "Core Methods",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nYou can pass the argument level to the StatsForecast.forecast method to calculate prediction intervals. Not all models can calculate them at the moment, so we will only obtain the intervals of those models that have it implemented.\n\nap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap})\nap_df['unique_id'] = 0\nsf = StatsForecast(\n    models=[\n        SeasonalNaive(season_length=12), \n        AutoARIMA(season_length=12)\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = sf.forecast(df=ap_df, h=12, level=(80, 95)).reset_index()\nfcst.plot(ap_df, ap_ci, level=[80], engine=\"matplotlib\")"
  },
  {
    "objectID": "src/core/core.html#conformal-prediction-intervals",
    "href": "src/core/core.html#conformal-prediction-intervals",
    "title": "Core Methods",
    "section": "Conformal Prediction intervals",
    "text": "Conformal Prediction intervals\nYou can also add conformal intervals using the following code.\n\nfrom statsforecast.utils import ConformalIntervals\n\n\nsf = StatsForecast(\n    models=[\n        AutoARIMA(season_length=12),\n        AutoARIMA(\n            season_length=12, \n            prediction_intervals=ConformalIntervals(n_windows=2, h=12),\n            alias='ConformalAutoARIMA'\n        ),\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = sf.forecast(df=ap_df, h=12, level=(80, 95)).reset_index()\nfcst.plot(ap_df, ap_ci, level=[80], engine=\"plotly\")\n\nYou can also compute conformal intervals for all the models that support them, using the following,\n\nsf = StatsForecast(\n    models=[\n        AutoARIMA(season_length=12),\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = sf.forecast(\n    df=ap_df, \n    h=12, \n    level=(50, 80, 95), \n    prediction_intervals=ConformalIntervals(h=12),\n).reset_index()\nfcst.plot(ap_df, ap_ci, level=[80], engine=\"matplotlib\")"
  },
  {
    "objectID": "src/garch.html",
    "href": "src/garch.html",
    "title": "GARCH",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/garch.html#generate-garchpq-model",
    "href": "src/garch.html#generate-garchpq-model",
    "title": "GARCH",
    "section": "Generate GARCH(p,q) model",
    "text": "Generate GARCH(p,q) model\n\nsource\n\ngarch_model\n\n garch_model (x, p, q)\n\n\nsource\n\n\ngarch_forecast\n\n garch_forecast (mod, h)\n\n\n\nComparison with arch library\nThis section compares the coefficients generated by the previous functions with the coefficients generated by the arch library for \\(p=q\\), \\(p&gt;q\\), \\(p&lt;q\\), and \\(q=0\\)."
  },
  {
    "objectID": "src/distributed.utils.html",
    "href": "src/distributed.utils.html",
    "title": "Distributed utils",
    "section": "",
    "text": "source\n\nforecast\n\n forecast (df, models, freq, h, fallback_model=None, X_df=None,\n           level=None,\n           parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\nsource\n\n\ncross_validation\n\n cross_validation (df, models, freq, h, n_windows=1, step_size=1,\n                   test_size=None, input_size=None,\n                   parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "src/mstl.html",
    "href": "src/mstl.html",
    "title": "MSTL model",
    "section": "",
    "text": "source\n\nmstl\n\n mstl (x:numpy.ndarray, period:Union[int,List[int]],\n       blambda:Optional[float]=None, iterate:int=1,\n       s_window:Optional[numpy.ndarray]=None,\n       stl_kwargs:Optional[Dict]={})\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\ntime series\n\n\nperiod\ntyping.Union[int, typing.List[int]]\n\nseason length\n\n\nblambda\ntyping.Optional[float]\nNone\nbox-cox transform\n\n\niterate\nint\n1\nnumber of iterations\n\n\ns_window\ntyping.Optional[numpy.ndarray]\nNone\nseasonal window\n\n\nstl_kwargs\ntyping.Optional[typing.Dict]\n{}\n\n\n\n\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  }
]